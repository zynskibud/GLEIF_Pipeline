{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import bigjson\n",
    "import json\n",
    "import sys\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "from D_Infastructure import System_Helpers\n",
    "\n",
    "class GLEIF_Backill_Helpers:\n",
    "    def __init__(self, bool_Level_1 = False, bool_Level_2_Trees = False, bool_Level_2_Reporting_Exceptions = False):\n",
    "        self.bool_Level_1 = bool_Level_1\n",
    "        self.bool_Level_2_Trees = bool_Level_2_Trees\n",
    "        self.bool_Level_2_Reporting_Exceptions = bool_Level_2_Reporting_Exceptions\n",
    "\n",
    "    def get_level_download_links(self):\n",
    "        \"\"\"\n",
    "        This function uses selenium to webscrape the download link for all Level 1 Data in the GLEIF database.\n",
    "        \n",
    "        @return: str_download_link - the link which is used to download the entire GLEIF level 1\n",
    "        \"\"\"\n",
    "        #Maybe new function\n",
    "\n",
    "        driver_path = (r\"C:\\Drivers\\Google\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\")\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(url = \"https://www.gleif.org/en/lei-data/gleif-golden-copy/download-the-golden-copy#/\")\n",
    "\n",
    "        cookie_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, 'CybotCookiebotDialogBodyButton'))\n",
    "        )\n",
    "\n",
    "        cookie_button.click()\n",
    "\n",
    "        download_buttons = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'gc-download-button'))\n",
    "        )\n",
    "        \n",
    "        if self.bool_Level_1 == True:\n",
    "            download_buttons[0].click()\n",
    "        if self.bool_Level_2_Trees == True:\n",
    "            download_buttons[1].click()\n",
    "        if self.bool_Level_2_Reporting_Exceptions == True:\n",
    "            download_buttons[2].click()\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        str_download_link = ((soup.find_all(\"a\" , class_ = \"gc-icon gc-icon--json\"))[0])[\"href\"]\n",
    "        \n",
    "        return str_download_link        \n",
    "    \n",
    "    def create_sql_instance(self, str_db_name, str_table_name):\n",
    "        # Connect to the SQLite database with WAL mode enabled\n",
    "        conn = sqlite3.connect(f'{str_db_name}.db', timeout=10)  # Set a timeout for waiting on locks\n",
    "        conn.execute('PRAGMA journal_mode=WAL;')  # Enable WAL mode for concurrency\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create the table with an id and JSON field (storing JSON as TEXT)\n",
    "        cursor.execute(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {str_table_name} (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        data TEXT\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        return conn, cursor\n",
    "    \n",
    "    def unpacking_GLEIF_zip_files(self , str_download_link , str_zip_file_path , str_unpacked_zip_file_path):\n",
    "        session = requests.Session()\n",
    "        zip_file = session.get(url = str_download_link)\n",
    "\n",
    "        with open(str_zip_file_path, 'wb') as f:\n",
    "            f.write(zip_file.content)\n",
    "\n",
    "        with zipfile.ZipFile(str_zip_file_path, 'r') as zip_ref:\n",
    "            os.makedirs(str_unpacked_zip_file_path, exist_ok=True)\n",
    "            zip_ref.extractall(str_unpacked_zip_file_path)\n",
    "        \n",
    "        str_unpacked_zip_file_name = os.listdir(str_unpacked_zip_file_path)[0]\n",
    "        str_json_file_path = str_unpacked_zip_file_path + \"\\\\\" + str_unpacked_zip_file_name\n",
    "        \n",
    "        return str_json_file_path\n",
    "    \n",
    "    def company_id_dictionary_generator(self):\n",
    "        db_path = \"GLEIF_Data.db\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        table_name = \"Level_1_Data\"  # Replace with your table name\n",
    "        query = f\"SELECT * FROM {table_name};\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        dict_company_names_leis = {}\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            dict_company_data = json.loads(row.loc[\"data\"])\n",
    "            dict_company_names_leis[dict_company_data[\"Entity\"][\"LegalName\"][\"$\"]] = dict_company_data[\"LEI\"][\"$\"]\n",
    "        \n",
    "        with open(r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\B_Notebooks\\GLIEF_company_data_pipeline\\pickled_objs\\dict_company_names_leis.pickle\" , \"wb\") as file:\n",
    "            pickle.dump(dict_company_names_leis , file)\n",
    "    \n",
    "    def get_all_level_1_data(self):\n",
    "        db_path = \"GLEIF_Data.db\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        table_name = \"Level_1_Data\"  # Replace with your table name\n",
    "        query = f\"SELECT * FROM {table_name};\"\n",
    "        df_level_1_data = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        with open(r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\B_Notebooks\\GLIEF_company_data_pipeline\\pickled_objs\\df_level_1_data.pickle\" , \"wb\") as file:\n",
    "            pickle.dump(df_level_1_data , file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel1Data:\n",
    "    def __init__(self):\n",
    "        self.str_level_1_unpacked_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_1_Data\\Unpacked_Zip\"\n",
    "        self.str_level_1_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_1_Data\\Level_1.zip\"\n",
    "        self.obj_backfill_helpers = GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "\n",
    "    def insert_json_data(self, json_data , conn , cursor , str_table_name):\n",
    "        cursor.execute(f'''\n",
    "        INSERT INTO {str_table_name}  (data)\n",
    "        VALUES (?)\n",
    "        ''', (json.dumps(json_data),))\n",
    "        conn.commit()\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        str_level_1_download_link = self.obj_data_helpers.get_level_download_links()\n",
    "        str_json_file_path = self.obj_data_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link , str_zip_file_path = self.str_level_1_zip_file_path , str_unpacked_zip_file_path = self.str_level_1_unpacked_zip_file_path)\n",
    "        conn, cursor = self.obj_data_helpers.create_sql_instance(str_table_name = \"Level_1_Data\" , str_db_name = \"GLEIF_Data\")\n",
    "        \n",
    "        with open(str_json_file_path, 'r' , encoding='utf-8') as file:\n",
    "            dict_leis = bigjson.load(file)\n",
    "            #counter = 1\n",
    "            for dict_lei in dict_leis[\"records\"]:\n",
    "                #if counter != 15000:\n",
    "                self.insert_json_data(json_data = dict_lei.to_python() , conn = conn , cursor = cursor , str_table_name = \"Level_1_Data\")\n",
    "                    #counter += 1\n",
    "                #else:\n",
    "                    #break\n",
    "        conn.close()            \n",
    "        \n",
    "        obj_system_helpers = System_Helpers.SystemHelpers()\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_1_unpacked_zip_file_path , bool_directory = True)\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_1_zip_file_path , bool_file = True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel2Data:\n",
    "    def __init__(self):\n",
    "        self.str_level_2_unpacked_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_2_Data\\RR_CDF_Data\\Unpacked_Zip\"\n",
    "        self.str_level_2_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_2_Data\\RR_CDF_Data\\Level_2_RR_CDF.zip\"\n",
    "        self.obj_backfill_helpers = GLEIF_Backill_Helpers(bool_Level_2_Trees = True)\n",
    "\n",
    "    def insert_json_data(self, json_data , conn , cursor , str_table_name):\n",
    "        cursor.execute(f'''\n",
    "        INSERT INTO {str_table_name}  (data)\n",
    "        VALUES (?)\n",
    "        ''', (json.dumps(json_data),))\n",
    "        conn.commit()\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        str_level_2_download_link = self.obj_data_helpers.get_level_download_links()\n",
    "        str_json_file_path = self.obj_data_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_2_download_link , str_zip_file_path = self.str_level_2_zip_file_path , str_unpacked_zip_file_path = self.str_level_2_unpacked_zip_file_path)\n",
    "        conn, cursor = self.obj_data_helpers.create_sql_instance(str_table_name = \"Level_2_Tree_Data\" , str_db_name = \"GLEIF_Data\")\n",
    "        \n",
    "        with open(str_json_file_path, 'r' , encoding='utf-8') as file:\n",
    "            test = bigjson.load(file)\n",
    "            for dict_lei in test[\"relations\"]:\n",
    "                self.insert_json_data(json_data = dict_lei.to_python() , conn = conn , cursor = cursor , str_table_name = \"Level_2_Tree_Data\")\n",
    "                    \n",
    "            \n",
    "        conn.close()\n",
    "\n",
    "        obj_system_helpers = System_Helpers.SystemHelpers()\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_2_unpacked_zip_file_path , bool_directory = True)\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_2_zip_file_path , bool_file = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_table(old_table_name, new_table_name):\n",
    "    \"\"\"\n",
    "    Renames a table in the SQLite database.\n",
    "\n",
    "    Parameters:\n",
    "        db_name (str): The SQLite database name.\n",
    "        old_table_name (str): The current name of the table.\n",
    "        new_table_name (str): The new name for the table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\B_Notebooks\\GLIEF_company_data_pipeline\\GLEIF_Data.db\", check_same_thread=False)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Rename the table\n",
    "        cursor.execute(f\"ALTER TABLE {old_table_name} RENAME TO {new_table_name}\")\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Table '{old_table_name}' has been renamed to '{new_table_name}'.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error renaming table: {e}\")\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
