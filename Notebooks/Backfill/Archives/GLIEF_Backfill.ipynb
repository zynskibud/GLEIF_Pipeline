{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import bigjson\n",
    "import json\n",
    "import sys\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "from D_Infastructure import System_Helpers\n",
    "\n",
    "class GLEIF_Backill_Helpers:\n",
    "    def __init__(self, bool_Level_1 = False, bool_Level_2_Trees = False, bool_Level_2_Reporting_Exceptions = False):\n",
    "        self.bool_Level_1 = bool_Level_1\n",
    "        self.bool_Level_2_Trees = bool_Level_2_Trees\n",
    "        self.bool_Level_2_Reporting_Exceptions = bool_Level_2_Reporting_Exceptions\n",
    "\n",
    "    def get_level_download_links(self):\n",
    "        \"\"\"\n",
    "        This function uses selenium to webscrape the download link for all Level 1 Data in the GLEIF database.\n",
    "        \n",
    "        @return: str_download_link - the link which is used to download the entire GLEIF level 1\n",
    "        \"\"\"\n",
    "        #Maybe new function\n",
    "\n",
    "        driver_path = (r\"C:\\Drivers\\Google\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\")\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(url = \"https://www.gleif.org/en/lei-data/gleif-golden-copy/download-the-golden-copy#/\")\n",
    "\n",
    "        cookie_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, 'CybotCookiebotDialogBodyButton'))\n",
    "        )\n",
    "\n",
    "        cookie_button.click()\n",
    "\n",
    "        download_buttons = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'gc-download-button'))\n",
    "        )\n",
    "        \n",
    "        if self.bool_Level_1 == True:\n",
    "            download_buttons[0].click()\n",
    "        if self.bool_Level_2_Trees == True:\n",
    "            download_buttons[1].click()\n",
    "        if self.bool_Level_2_Reporting_Exceptions == True:\n",
    "            download_buttons[2].click()\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        str_download_link = ((soup.find_all(\"a\" , class_ = \"gc-icon gc-icon--json\"))[0])[\"href\"]\n",
    "        \n",
    "        return str_download_link        \n",
    "    \n",
    "    def create_sql_instance(self, str_db_name, str_table_name):\n",
    "        # Connect to the SQLite database with WAL mode enabled\n",
    "        conn = sqlite3.connect(f'{str_db_name}.db', timeout=10)  # Set a timeout for waiting on locks\n",
    "        conn.execute('PRAGMA journal_mode=WAL;')  # Enable WAL mode for concurrency\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create the table with an id and JSON field (storing JSON as TEXT)\n",
    "        cursor.execute(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {str_table_name} (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        data TEXT\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        return conn, cursor\n",
    "    \n",
    "    def unpacking_GLEIF_zip_files(self , str_download_link , str_zip_file_path , str_unpacked_zip_file_path):\n",
    "        session = requests.Session()\n",
    "        zip_file = session.get(url = str_download_link)\n",
    "\n",
    "        with open(str_zip_file_path, 'wb') as f:\n",
    "            f.write(zip_file.content)\n",
    "\n",
    "        with zipfile.ZipFile(str_zip_file_path, 'r') as zip_ref:\n",
    "            os.makedirs(str_unpacked_zip_file_path, exist_ok=True)\n",
    "            zip_ref.extractall(str_unpacked_zip_file_path)\n",
    "        \n",
    "        str_unpacked_zip_file_name = os.listdir(str_unpacked_zip_file_path)[0]\n",
    "        str_json_file_path = str_unpacked_zip_file_path + \"\\\\\" + str_unpacked_zip_file_name\n",
    "        \n",
    "        return str_json_file_path\n",
    "    \n",
    "    def company_id_dictionary_generator(self):\n",
    "        db_path = \"GLEIF_Data.db\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        table_name = \"Level_1_Data\"  # Replace with your table name\n",
    "        query = f\"SELECT * FROM {table_name};\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        dict_company_names_leis = {}\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            dict_company_data = json.loads(row.loc[\"data\"])\n",
    "            dict_company_names_leis[dict_company_data[\"Entity\"][\"LegalName\"][\"$\"]] = dict_company_data[\"LEI\"][\"$\"]\n",
    "        \n",
    "        with open(r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\B_Notebooks\\GLIEF_company_data_pipeline\\pickled_objs\\dict_company_names_leis.pickle\" , \"wb\") as file:\n",
    "            pickle.dump(dict_company_names_leis , file)\n",
    "    \n",
    "    def get_all_level_1_data(self):\n",
    "        db_path = \"GLEIF_Data.db\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        table_name = \"Level_1_Data\"  # Replace with your table name\n",
    "        query = f\"SELECT * FROM {table_name};\"\n",
    "        df_level_1_data = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        with open(r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\B_Notebooks\\GLIEF_company_data_pipeline\\pickled_objs\\df_level_1_data.pickle\" , \"wb\") as file:\n",
    "            pickle.dump(df_level_1_data , file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_relationship_meta_data(self, list_relationship_data):\n",
    "        \n",
    "        self.cursor.executemany(\"\"\"\n",
    "            INSERT INTO GLEIF_relationship_data (\n",
    "            StartNode, EndNode, RelationshipType, \n",
    "            RelationshipStatus, RegistrationStatus, InitialRegistrationDate, LastUpdateDate, NextRenewalDate, \n",
    "                ManagingLOU,\n",
    "                ValidationSources,\n",
    "                ValidationDocuments,\n",
    "                ValidationRegistration\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s);\n",
    "        \"\"\", (list_relationship_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_event_data(self , dict_data, base_keyword, target_keys):\n",
    "        \"\"\"\n",
    "        Extracts and organizes data for repeated keys in a dictionary based on a base keyword and target keys.\n",
    "\n",
    "        :param dict_data: Dictionary containing the raw data.\n",
    "        :param base_keyword: Common substring to identify relevant keys (e.g., \"LegalEntityEvents\").\n",
    "        :param target_keys: List of substrings to match keys that should be included in the tuple.\n",
    "        :return: A list of tuples, one for each numeric suffix group, containing values for the target keys.\n",
    "        \"\"\"\n",
    "        grouped_data = {}\n",
    "\n",
    "\n",
    "        # Group keys by numeric suffix\n",
    "        for key, value in dict_data.items():\n",
    "            if base_keyword in key:\n",
    "                # Extract the numeric suffix using regex\n",
    "                match = re.search(r\"_(\\d+)_\", key)\n",
    "                if not match:\n",
    "                    continue  # Skip keys without a numeric suffix\n",
    "                index = int(match.group(1))\n",
    "\n",
    "                # Extract the part of the key after the numeric suffix\n",
    "                key_suffix = key.split(f\"_{index}_\")[-1]\n",
    "\n",
    "                if index not in grouped_data:\n",
    "                    grouped_data[index] = {}\n",
    "                \n",
    "                # Check if this key matches any of the target keys as a substring\n",
    "                for target in target_keys:\n",
    "                    if target in key_suffix:\n",
    "                        grouped_data[index][target] = value\n",
    "                        break\n",
    "\n",
    "        # Create tuples for each group of keys\n",
    "        result = []\n",
    "        for index in sorted(grouped_data.keys()):\n",
    "            # Create a tuple of values for the target keys, using None if a key is missing\n",
    "            tuple_values = tuple(grouped_data[index].get(target, None) for target in target_keys)\n",
    "            result.append(tuple_values)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel1Data:\n",
    "    def __init__(self):\n",
    "        self.str_level_1_unpacked_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_1_Data\\Unpacked_Zip\"\n",
    "        self.str_level_1_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_1_Data\\Level_1.zip\"\n",
    "        self.obj_backfill_helpers = GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "\n",
    "    def insert_json_data(self, json_data , conn , cursor , str_table_name):\n",
    "        cursor.execute(f'''\n",
    "        INSERT INTO {str_table_name}  (data)\n",
    "        VALUES (?)\n",
    "        ''', (json.dumps(json_data),))\n",
    "        conn.commit()\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        str_level_1_download_link = self.obj_data_helpers.get_level_download_links()\n",
    "        str_json_file_path = self.obj_data_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link , str_zip_file_path = self.str_level_1_zip_file_path , str_unpacked_zip_file_path = self.str_level_1_unpacked_zip_file_path)\n",
    "        conn, cursor = self.obj_data_helpers.create_sql_instance(str_table_name = \"Level_1_Data\" , str_db_name = \"GLEIF_Data\")\n",
    "        \n",
    "        with open(str_json_file_path, 'r' , encoding='utf-8') as file:\n",
    "            dict_leis = bigjson.load(file)\n",
    "            #counter = 1\n",
    "            for dict_lei in dict_leis[\"records\"]:\n",
    "                #if counter != 15000:\n",
    "                self.insert_json_data(json_data = dict_lei.to_python() , conn = conn , cursor = cursor , str_table_name = \"Level_1_Data\")\n",
    "                    #counter += 1\n",
    "                #else:\n",
    "                    #break\n",
    "        conn.close()            \n",
    "        \n",
    "        obj_system_helpers = System_Helpers.SystemHelpers()\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_1_unpacked_zip_file_path , bool_directory = True)\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_1_zip_file_path , bool_file = True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel2Data:\n",
    "    def __init__(self):\n",
    "        self.str_level_2_unpacked_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_2_Data\\RR_CDF_Data\\Unpacked_Zip\"\n",
    "        self.str_level_2_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_2_Data\\RR_CDF_Data\\Level_2_RR_CDF.zip\"\n",
    "        self.obj_backfill_helpers = GLEIF_Backill_Helpers(bool_Level_2_Trees = True)\n",
    "\n",
    "    def insert_json_data(self, json_data , conn , cursor , str_table_name):\n",
    "        cursor.execute(f'''\n",
    "        INSERT INTO {str_table_name}  (data)\n",
    "        VALUES (?)\n",
    "        ''', (json.dumps(json_data),))\n",
    "        conn.commit()\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        str_level_2_download_link = self.obj_data_helpers.get_level_download_links()\n",
    "        str_json_file_path = self.obj_data_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_2_download_link , str_zip_file_path = self.str_level_2_zip_file_path , str_unpacked_zip_file_path = self.str_level_2_unpacked_zip_file_path)\n",
    "        conn, cursor = self.obj_data_helpers.create_sql_instance(str_table_name = \"Level_2_Tree_Data\" , str_db_name = \"GLEIF_Data\")\n",
    "        \n",
    "        with open(str_json_file_path, 'r' , encoding='utf-8') as file:\n",
    "            test = bigjson.load(file)\n",
    "            for dict_lei in test[\"relations\"]:\n",
    "                self.insert_json_data(json_data = dict_lei.to_python() , conn = conn , cursor = cursor , str_table_name = \"Level_2_Tree_Data\")\n",
    "                    \n",
    "            \n",
    "        conn.close()\n",
    "\n",
    "        obj_system_helpers = System_Helpers.SystemHelpers()\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_2_unpacked_zip_file_path , bool_directory = True)\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_2_zip_file_path , bool_file = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_table(old_table_name, new_table_name):\n",
    "    \"\"\"\n",
    "    Renames a table in the SQLite database.\n",
    "\n",
    "    Parameters:\n",
    "        db_name (str): The SQLite database name.\n",
    "        old_table_name (str): The current name of the table.\n",
    "        new_table_name (str): The new name for the table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\B_Notebooks\\GLIEF_company_data_pipeline\\GLEIF_Data.db\", check_same_thread=False)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Rename the table\n",
    "        cursor.execute(f\"ALTER TABLE {old_table_name} RENAME TO {new_table_name}\")\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Table '{old_table_name}' has been renamed to '{new_table_name}'.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error renaming table: {e}\")\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(dict_input):\n",
    "        \"\"\" \n",
    "        This function flattens a dictionary by changing the keys for nested dictionaries to be that of their nested key path from the root of the dictionary\n",
    "        it is using basic DFS on a tree (the dictionary).\n",
    "\n",
    "        @param: dict_input - json response input, nested dictionary as input basically \n",
    "        \n",
    "        @return: the flattened dictionary. \n",
    "        \"\"\"\n",
    "        dict_flattened = {}\n",
    "        def flatten(current_dict , parent_key = ''):\n",
    "            for key , value in current_dict.items():\n",
    "                new_key = f\"{parent_key}_{key}\" if parent_key else key\n",
    "                if isinstance(value , dict):\n",
    "                    flatten(value , new_key )\n",
    "                else:\n",
    "                    dict_flattened[new_key] = value\n",
    "\n",
    "        flatten(dict_input)\n",
    "        return dict_flattened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_table(old_table_name, new_table_name):\n",
    "    \"\"\"\n",
    "    Renames a table in the SQLite database.\n",
    "\n",
    "    Parameters:\n",
    "        db_name (str): The SQLite database name.\n",
    "        old_table_name (str): The current name of the table.\n",
    "        new_table_name (str): The new name for the table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\B_Notebooks\\GLIEF_company_data_pipeline\\GLEIF_Data.db\", check_same_thread=False)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Rename the table\n",
    "        cursor.execute(f\"ALTER TABLE {old_table_name} RENAME TO {new_table_name}\")\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Table '{old_table_name}' has been renamed to '{new_table_name}'.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error renaming table: {e}\")\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel2Data:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        self.obj_backfill_helpers = GLEIF_Backill_Helpers(bool_Level_2_Trees = True)\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Backfill_level_2.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            str_level_2_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "            self.str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_2_download_link , str_unpacked_zip_file_path_name = \"Level_2_unpacked\" , str_zip_file_path_name = \"Level_2.zip\")\n",
    "    \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/Level_2_unpacked\")[0]\n",
    "        self.str_json_file_path = rf\"../file_lib/Level_2_unpacked\" + \"//\" + str_unpacked_zip_file_name\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def create_table(self):\n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_relationship_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                StartNode TEXT,                \n",
    "                EndNode TEXT,\n",
    "                RelationshipType TEXT,\n",
    "                RelationshipStatus TEXT,\n",
    "                RegistrationStatus TEXT,\n",
    "                NextRenewalDate TEXT\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "        self.conn.commit()\n",
    "    \n",
    "    def insert_relationship(self, list_relationship_data):\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "            INSERT INTO GLEIF_relationship_data (\n",
    "            StartNode, EndNode, RelationshipType, \n",
    "            RelationshipStatus, RegistrationStatus, NextRenewalDate\n",
    "            ) VALUES (%s, %s, %s, %s, %s, %s);\n",
    "        \"\"\", (list_relationship_data[0], list_relationship_data[1], list_relationship_data[2], list_relationship_data[3], list_relationship_data[4], list_relationship_data[5]))\n",
    "    \n",
    "    def process_relationships(self , dict_relationship):\n",
    "        dict_relationship_flattened = self.obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "        list_relationship_data = self.obj_backfill_helpers.get_target_values(dict_data = dict_relationship_flattened, subset_string = True, target_keys = [\"StartNode\" , \"EndNode\" , \"RelationshipType\" , \"RelationshipStatus\" , \"RegistrationStatus\" , \"NextRenewalDate\"])\n",
    "        self.insert_relationship(list_relationship_data = list_relationship_data)\n",
    "        \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        self.create_table()\n",
    "        \n",
    "        with open(self.str_json_file_path, 'r' , encoding='utf-8') as file:\n",
    "            \n",
    "            dict_relationship_data = bigjson.load(file)\n",
    "            for dict_relationship in dict_relationship_data[\"relations\"]:\n",
    "                dict_record = dict_relationship.to_python()\n",
    "                self.process_relationships(dict_relationship = dict_record)               \n",
    "        \n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_insert_using_copy(self , table_name , columns, data):\n",
    "        \"\"\"Perform a bulk insert using PostgreSQL COPY with an in-memory buffer\n",
    "\n",
    "        Args:\n",
    "            table_name (_type_): Name of the table to insert into\n",
    "            columns (_type_): List of column names for the table\n",
    "            data (_type_): List of tuples with the data to be inserted\n",
    "        \"\"\"\n",
    "        \n",
    "        buffer = io.StringIO()\n",
    "        \n",
    "        #write data to the buffer\n",
    "        \n",
    "        for row in data:\n",
    "            '''row_converted = [\n",
    "            x.replace('\\\\', '\\\\\\\\') if isinstance(x, str) else x \n",
    "            for x in row]'''\n",
    "        # Replace None with \\N for PostgreSQL NULL representation\n",
    "        #row_converted = [str(x) if x is not None else '\\\\N' for x in row_converted]\n",
    "            #buffer.write('\\t'.join(row_converted) + \"\\n\")\n",
    "            #buffer.write('\\t'.join(map(str , row_converted)) + \"\\n\")\n",
    "            #buffer.write('\\t'.join(row_converted) + \"\\n\")\n",
    "            buffer.write('\\t'.join(map(str , row)) + \"\\n\")\n",
    "        buffer.seek(0) #reset buffer position to the beginning\n",
    "        \n",
    "        #Construct the copy query\n",
    "        #copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH DELIMITER '\\t', NULL '\\\\N'\"\n",
    "        \n",
    "        #copy_query = f\"\"\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH (FORMAT text, DELIMITER E'\\\\t', NULL '\\\\N')\"\"\"\n",
    "        copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH DELIMITER '\\t'\"\n",
    "        self.cursor.copy_expert(copy_query , buffer)\n",
    "        self.conn.commit()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
