{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this function will be in a seperate class which are parametrized functions used to obtain the data from the GLEIF. That way each script can take care of and process the data invidually, but the same function logic for grabbing a link would not have to be repeated in each script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "class GLEIF_Data_Helpers:\n",
    "    def __init__(self, bool_Level_1 = False, bool_Level_2_Trees = False, bool_Level_2_Reporting_Exceptions = False):\n",
    "        self.bool_Level_1 = bool_Level_1\n",
    "        self.bool_Level_2_Trees = bool_Level_2_Trees\n",
    "        self.bool_Level_2_Reporting_Exceptions = bool_Level_2_Reporting_Exceptions\n",
    "\n",
    "    def get_level_download_links(self):\n",
    "        \"\"\"\n",
    "        This function uses selenium to webscrape the download link for all Level 1 Data in the GLEIF database.\n",
    "        \n",
    "        @return: str_download_link - the link which is used to download the entire GLEIF level 1\n",
    "        \"\"\"\n",
    "        #Maybe new function\n",
    "\n",
    "        driver_path = (r\"C:\\Drivers\\Google\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\")\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(url = \"https://www.gleif.org/en/lei-data/gleif-golden-copy/download-the-golden-copy#/\")\n",
    "\n",
    "        cookie_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, 'CybotCookiebotDialogBodyButton'))\n",
    "        )\n",
    "\n",
    "        cookie_button.click()\n",
    "\n",
    "        download_buttons = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'gc-download-button'))\n",
    "        )\n",
    "        \n",
    "        if self.bool_Level_1 == True:\n",
    "            download_buttons[0].click()\n",
    "        if self.bool_Level_2_Trees == True:\n",
    "            download_buttons[1].click()\n",
    "        if self.bool_Level_2_Reporting_Exceptions == True:\n",
    "            download_buttons[2].click()\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        str_download_link = ((soup.find_all(\"a\" , class_ = \"gc-icon gc-icon--json\"))[0])[\"href\"]\n",
    "        \n",
    "        return str_download_link        \n",
    "    \n",
    "    def create_sql_instance(self, str_db_name, str_table_name):\n",
    "        # Connect to the SQLite database with WAL mode enabled\n",
    "        conn = sqlite3.connect(f'{str_db_name}.db', timeout=10)  # Set a timeout for waiting on locks\n",
    "        conn.execute('PRAGMA journal_mode=WAL;')  # Enable WAL mode for concurrency\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Create the table with an id and JSON field (storing JSON as TEXT)\n",
    "        cursor.execute(f'''\n",
    "        CREATE TABLE IF NOT EXISTS {str_table_name} (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        data TEXT\n",
    "        )\n",
    "        ''')\n",
    "        \n",
    "        return conn, cursor\n",
    "    \n",
    "    def unpacking_GLEIF_zip_files(self , str_download_link , str_zip_file_path , str_unpacked_zip_file_path):\n",
    "        session = requests.Session()\n",
    "        zip_file = session.get(url = str_download_link)\n",
    "\n",
    "        with open(str_zip_file_path, 'wb') as f:\n",
    "            f.write(zip_file.content)\n",
    "\n",
    "        with zipfile.ZipFile(str_zip_file_path, 'r') as zip_ref:\n",
    "            os.makedirs(str_unpacked_zip_file_path, exist_ok=True)\n",
    "            zip_ref.extractall(str_unpacked_zip_file_path)\n",
    "        \n",
    "        str_unpacked_zip_file_name = os.listdir(str_unpacked_zip_file_path)[0]\n",
    "        str_json_file_path = str_unpacked_zip_file_path + \"\\\\\" + str_unpacked_zip_file_name\n",
    "        \n",
    "        return str_json_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bigjson\n",
    "import json\n",
    "\n",
    "class GLEIFLevel1Data:\n",
    "    def __init__(self):\n",
    "        self.str_level_1_unpacked_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_1_Data\\Unpacked_Zip\"\n",
    "        self.str_level_1_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_1_Data.Level_1.zip\"\n",
    "        self.obj_data_helpers = GLEIF_Data_Helpers(bool_Level_1 = True)\n",
    "\n",
    "    def insert_json_data(self, json_data , conn , cursor , str_table_name):\n",
    "        cursor.execute(f'''\n",
    "        INSERT INTO {str_table_name}  (data)\n",
    "        VALUES (?)\n",
    "        ''', (json.dumps(json_data),))\n",
    "        conn.commit()\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        str_level_1_download_link = self.obj_data_helpers.get_level_download_links()\n",
    "        str_json_file_path = self.obj_data_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link , str_zip_file_path = self.str_level_1_zip_file_path , str_unpacked_zip_file_path = self.str_level_1_unpacked_zip_file_path)\n",
    "        conn, cursor = self.obj_data_helpers.create_sql_instance(str_table_name = \"Level_1_Data\" , str_db_name = \"GLEIF_Data\")\n",
    "        \n",
    "        with open(str_json_file_path, 'r' , encoding='utf-8') as file:\n",
    "            test = bigjson.load(file)\n",
    "            #counter = 1\n",
    "            for dict_lei in test[\"records\"]:\n",
    "                #if counter != 15000:\n",
    "                self.insert_json_data(json_data = dict_lei.to_python() , conn = conn , cursor = cursor , str_table_name = \"Level_1_Data\")\n",
    "                    #counter += 1\n",
    "                #else:\n",
    "                    #break\n",
    "                    \n",
    "    def company_id_dictionary_generator(self):\n",
    "        db_path = \"GLEIF_Data.db\"\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        table_name = \"Level_1_Data\"  # Replace with your table name\n",
    "        query = f\"SELECT * FROM {table_name};\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        dict_company_names_leis = {}\n",
    "\n",
    "        for _, row in df.iterrows():\n",
    "            dict_company_data = json.loads(row.loc[\"data\"])\n",
    "            dict_company_names_leis[dict_company_data[\"Entity\"][\"LegalName\"][\"$\"]] = dict_company_data[\"LEI\"][\"$\"]\n",
    "        \n",
    "        return dict_company_names_leis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_GLEIFLevel1Dara = GLEIFLevel1Data()\n",
    "obj_GLEIFLevel1Dara.storing_GLEIF_data_in_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_level_1_data():\n",
    "    db_path = \"GLEIF_Data.db\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    table_name = \"Level_1_Data\"  # Replace with your table name\n",
    "    query = f\"SELECT * FROM {table_name};\"\n",
    "    df_level_1_data = pd.read_sql_query(query, conn)\n",
    "    conn.close()\n",
    "    \n",
    "    \n",
    "    return df_level_1_data\n",
    "\n",
    "#this will maybe be somewhere else\n",
    "def company_name_id_dictionary_generator(df_level_1_data):\n",
    "    dict_company_names_leis = {}\n",
    "\n",
    "    for _, row in df_level_1_data.iterrows():\n",
    "        dict_company_data = json.loads(row.loc[\"data\"])\n",
    "        dict_company_names_leis[dict_company_data[\"Entity\"][\"LegalName\"][\"$\"]] = dict_company_data[\"LEI\"][\"$\"]\n",
    "    \n",
    "    return dict_company_names_leis\n",
    "\n",
    "df_level_1_data = get_all_level_1_data()\n",
    "dict_company_names_leis = company_name_id_dictionary_generator(df_level_1_data = df_level_1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\B_Notebooks\\GLIEF_company_data_pipeline\\pickled_objs\\df_level_1_data.pickle\" , \"wb\") as file:\n",
    "    pickle.dump(df_level_1_data , file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\B_Notebooks\\GLIEF_company_data_pipeline\\pickled_objs\\dict_company_names_leis.pickle\" , \"wb\") as file:\n",
    "    pickle.dump(dict_company_names_leis , file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
