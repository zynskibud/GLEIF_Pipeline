{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the jupyter notebook pertaining to the GLEIF_Backfill.py file.\n",
    "\n",
    "This code is responsible for obtaining all Level 1 and Level 2 Relationship data from the GLIEF, and backfilling the data and adding whatever features are needed to make the data accessible for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import psycopg2\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "import ijson\n",
    "\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel1Data_Rewritten:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        \n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Backfill_level_1.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            str_level_1_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "            self.str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link , str_unpacked_zip_file_path_name = \"Level_1_unpacked\" , str_zip_file_path_name = \"Level_1.zip\")\n",
    "        else:\n",
    "            str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/Level_1_unpacked\")[-1]\n",
    "            self.str_json_file_path = rf\"../file_lib/Level_1_unpacked\" + \"//\" + str_unpacked_zip_file_name\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    \n",
    "    def drop_table(self , lst_table_names):\n",
    "            \"\"\"\n",
    "            Drops a specific table from the database securely.\n",
    "            \n",
    "            Parameters:\n",
    "                table_name (list of string): The names of the tables to drop.\n",
    "            \"\"\"\n",
    "\n",
    "            for table_name in lst_table_names:\n",
    "                self.cursor.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n",
    "                \n",
    "            self.conn.commit()\n",
    "    \n",
    "    def load_batches_as_list(self , file_path , batch_size=100000):\n",
    "            \"\"\"Yield records in batches as lists of dictionaries.\"\"\"\n",
    "            with open(file_path, 'rb') as file:\n",
    "                # Create an iterator for the 'records' key\n",
    "                records = ijson.items(file, \"records.item\")\n",
    "                \n",
    "                batch = []\n",
    "                for index, record in enumerate(records, start=1):\n",
    "                    batch.append(record)  # Add record to the batch\n",
    "                    \n",
    "                    if index % batch_size == 0:  # Yield the batch when size is reached\n",
    "                        yield batch  # Yield the batch as a list\n",
    "                        batch = []  # Reset for the next batch\n",
    "                \n",
    "                # Yield any remaining records\n",
    "                if batch:\n",
    "                    yield batch\n",
    "    \n",
    "    def create_tables(self):\n",
    "        # GLEIF_entity_data\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_entity_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT UNIQUE,\n",
    "                LegalName TEXT,\n",
    "                LegalJurisdiction TEXT,\n",
    "                EntityCategory TEXT,\n",
    "                EntitySubCategory TEXT,\n",
    "                LegalForm_EntityLegalFormCode TEXT,\n",
    "                LegalForm_OtherLegalForm TEXT,\n",
    "                EntityStatus TEXT,\n",
    "                EntityCreationDate TEXT,\n",
    "                RegistrationAuthority_RegistrationAuthorityID TEXT,\n",
    "                RegistrationAuthority_RegistrationAuthorityEntityID TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_other_legal_names with unique constraint\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_other_legal_names (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                Type TEXT,\n",
    "                OtherEntityNames TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei),\n",
    "                UNIQUE (lei, OtherEntityNames, Type)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_LegalAddress\n",
    "        # If each LEI has only one legal address, keep the UNIQUE on lei\n",
    "        # Otherwise, define a composite unique constraint as needed\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_LegalAddress (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT UNIQUE,\n",
    "                LegalAddress_FirstAddressLine TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_1 TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_2 TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_3 TEXT,\n",
    "                LegalAddress_City TEXT,\n",
    "                LegalAddress_Region TEXT,\n",
    "                LegalAddress_Country TEXT,\n",
    "                LegalAddress_PostalCode TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_HeadquartersAddress\n",
    "        # Similar to GLEIF_LegalAddress\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_HeadquartersAddress (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT UNIQUE,\n",
    "                HeadquartersAddress_FirstAddressLine TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_1 TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_2 TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_3 TEXT,\n",
    "                HeadquartersAddress_City TEXT,\n",
    "                HeadquartersAddress_Region TEXT,\n",
    "                HeadquartersAddress_Country TEXT,\n",
    "                HeadquartersAddress_PostalCode TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_LegalEntityEvents with unique constraint\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_LegalEntityEvents (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                group_type TEXT,\n",
    "                event_status TEXT,\n",
    "                LegalEntityEventType TEXT,\n",
    "                LegalEntityEventEffectiveDate TEXT,\n",
    "                LegalEntityEventRecordedDate TEXT,\n",
    "                ValidationDocuments TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei),\n",
    "                UNIQUE (lei, group_type, event_status, LegalEntityEventType, LegalEntityEventEffectiveDate , LegalEntityEventRecordedDate, ValidationDocuments)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_registration_data\n",
    "        # If each LEI has only one registration record, keep the UNIQUE on lei\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_registration_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT UNIQUE,\n",
    "                InitialRegistrationDate TEXT,\n",
    "                LastUpdateDate TEXT,\n",
    "                RegistrationStatus TEXT,\n",
    "                NextRenewalDate TEXT,\n",
    "                ManagingLOU TEXT,\n",
    "                ValidationSources TEXT,\n",
    "                ValidationAuthorityID TEXT,\n",
    "                ValidationAuthorityEntityID TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_geocoding with unique constraint\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_geocoding (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                relevance TEXT,\n",
    "                match_type TEXT,\n",
    "                lat TEXT,\n",
    "                lng TEXT,\n",
    "                geocoding_date TEXT,\n",
    "                TopLeft_Latitude TEXT,\n",
    "                TopLeft_Longitude TEXT,\n",
    "                BottomRight_Latitude TEXT,\n",
    "                BottomRight_longitude TEXT,\n",
    "                match_level TEXT,\n",
    "                mapped_street TEXT,\n",
    "                mapped_housenumber TEXT,\n",
    "                mapped_postalcode TEXT,\n",
    "                mapped_city TEXT,\n",
    "                mapped_district TEXT,\n",
    "                mapped_state TEXT,\n",
    "                mapped_country TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei),\n",
    "                UNIQUE (lei, relevance, match_type, lat, geocoding_date, TopLeft_Latitude, TopLeft_Longitude, BottomRight_Latitude, BottomRight_longitude, match_level, mapped_street, mapped_housenumber, mapped_postalcode, mapped_city, mapped_district, mapped_state, mapped_country)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def bulk_insert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"Perform a bulk insert using PostgreSQL COPY with an in-memory buffer\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to insert into\n",
    "            columns (list): List of column names for the table\n",
    "            data (list): List of tuples with the data to be inserted\n",
    "        \"\"\"\n",
    "\n",
    "        buffer = io.StringIO()\n",
    "\n",
    "        for row in data:\n",
    "            # Escape backslashes and replace None with \\N for NULL\n",
    "            row_converted = []\n",
    "            for x in row:\n",
    "                if x is None:\n",
    "                    row_converted.append('\\\\N')  # NULL representation\n",
    "                elif isinstance(x, str):\n",
    "                    x = x.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "                    row_converted.append(x)\n",
    "                else:\n",
    "                    row_converted.append(str(x))\n",
    "            buffer.write('\\t'.join(row_converted) + '\\n')\n",
    "        \n",
    "        buffer.seek(0)  # Reset buffer position\n",
    "\n",
    "        # Construct the COPY query\n",
    "        copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH (FORMAT text, DELIMITER E'\\t', NULL '\\\\N')\"\n",
    "        self.cursor.copy_expert(copy_query, buffer)\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def remove_duplicates_keep_order(self , input_list):\n",
    "        seen = set()\n",
    "        output_list = []\n",
    "        for item in input_list:\n",
    "            if item not in seen:\n",
    "                output_list.append(item)\n",
    "                seen.add(item)\n",
    "        return output_list\n",
    "    \n",
    "    def process_entity_data(self , list_dict_records):\n",
    "        list_entity_meta_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , subset_string = \"Entity\" , target_keys = [\"LegalName\", \"LegalJurisdiction\", \"EntityCategory\", \"EntitySubCategory\", \"LegalForm_EntityLegalFormCode\", \"LegalForm_OtherLegalForm\", \"EntityStatus\", \"EntityCreationDate\", \"RegistrationAuthority_RegistrationAuthorityID\", \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_entity_meta_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_insert_using_copy(data = list_entity_meta_data_tuples , table_name = \"GLEIF_entity_data\" , \n",
    "                            columns = \n",
    "                            [\"lei\",\n",
    "                                \"LegalName\",\n",
    "                                \"LegalJurisdiction\",\n",
    "                                \"EntityCategory\",\n",
    "                                \"EntitySubCategory\",\n",
    "                                \"LegalForm_EntityLegalFormCode\",\n",
    "                                \"LegalForm_OtherLegalForm\",\n",
    "                                \"EntityStatus\",\n",
    "                                \"EntityCreationDate\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "    \n",
    "    def process_other_legal_names(self , list_dict_records):\n",
    "        list_other_names_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            #ist_output = self.obj_backfill_helpers.extract_other_entity_names(data_dict = dict_entity, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "            #xtract_both_entity_names\n",
    "            list_output = (self.obj_backfill_helpers.extract_both_entity_names(data_dict = dict_entity))[0]\n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup         \n",
    "            list_other_names_tuples.extend(list_output)\n",
    "                \n",
    "        list_clean_other_names_tuples = self.remove_duplicates_keep_order(list_other_names_tuples)\n",
    "                \n",
    "        self.bulk_insert_using_copy(data = list_clean_other_names_tuples , table_name = \"GLEIF_other_legal_names\" , columns = [\"lei\", \"Type\" , \"OtherEntityNames\"])        \n",
    "    \n",
    "    def process_legal_address(self , list_dict_records):\n",
    "        list_legal_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , target_keys = [\"Entity_LegalAddress_FirstAddressLine\" , \"Entity_LegalAddress_AdditionalAddressLine_1\" , \"Entity_LegalAddress_AdditionalAddressLine_2\" , \"Entity_LegalAddress_AdditionalAddressLine_3\" , \"Entity_LegalAddress_City\" , \"Entity_LegalAddress_Region\" , \"Entity_LegalAddress_Country\" , \"Entity_LegalAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_legal_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        \n",
    "        self.bulk_insert_using_copy(data = list_legal_address_tuples , table_name = \"GLEIF_LegalAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"LegalAddress_FirstAddressLine\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                                            \"LegalAddress_City\",\n",
    "                                            \"LegalAddress_Region\",\n",
    "                                            \"LegalAddress_Country\",\n",
    "                                            \"LegalAddress_PostalCode\"])  \n",
    "    \n",
    "    def process_headquarters_address(self , list_dict_records):\n",
    "        list_headquarters_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity , target_keys = [\"HeadquartersAddress_FirstAddressLine\" , \"HeadquartersAddress_AdditionalAddressLine_1\" , \"HeadquartersAddress_AdditionalAddressLine_2\" , \"HeadquartersAddress_AdditionalAddressLine_3\" , \"HeadquartersAddress_City\" , \"HeadquartersAddress_Region\" , \"HeadquartersAddress_Country\" , \"HeadquartersAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_headquarters_address_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_insert_using_copy(data = list_headquarters_address_tuples , table_name = \"GLEIF_HeadquartersAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"HeadquartersAddress_FirstAddressLine\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                                            \"HeadquartersAddress_City\",\n",
    "                                            \"HeadquartersAddress_Region\",\n",
    "                                            \"HeadquartersAddress_Country\",\n",
    "                                            \"HeadquartersAddress_PostalCode\"]) \n",
    "    \n",
    "    def process_legal_entity_events(self , list_dict_records):\n",
    "        list_legal_entity_events_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_event_data(dict_data = dict_entity , base_keyword=\"LegalEntityEvents\" , target_keys=[\"group_type\", \"event_status\", \"LegalEntityEventType\", \"LegalEntityEventEffectiveDate\", \"LegalEntityEventRecordedDate\", \"ValidationDocuments\"])\n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup \n",
    "            list_legal_entity_events_tuples.extend(list_output)\n",
    "\n",
    "        list_clean_legal_entity_events_tuples = self.remove_duplicates_keep_order(list_legal_entity_events_tuples)\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_clean_legal_entity_events_tuples , table_name = \"GLEIF_LegalEntityEvents\" , \n",
    "                                columns = [\"lei\",\n",
    "                                        \"group_type\",\n",
    "                                        \"event_status\",\n",
    "                                        \"LegalEntityEventType\",\n",
    "                                        \"LegalEntityEventEffectiveDate\",\n",
    "                                        \"LegalEntityEventRecordedDate\",\n",
    "                                        \"ValidationDocuments\"])\n",
    "        \n",
    "    def process_registration_data(self , list_dict_records):\n",
    "        list_registration_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_registration = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Registration\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_registration , target_keys = [\"InitialRegistrationDate\" , \"LastUpdateDate\" , \"RegistrationStatus\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationAuthority_ValidationAuthorityID\" , \"ValidationAuthority_ValidationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_registration_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_registration_tuples , table_name = \"GLEIF_registration_data\" , \n",
    "                                columns = [\n",
    "                                            \"lei\",\n",
    "                                            \"InitialRegistrationDate\",\n",
    "                                            \"LastUpdateDate\",\n",
    "                                            \"RegistrationStatus\",\n",
    "                                            \"NextRenewalDate\",\n",
    "                                            \"ManagingLOU\",\n",
    "                                            \"ValidationSources\",\n",
    "                                            \"ValidationAuthorityID\",\n",
    "                                            \"ValidationAuthorityEntityID\"]) \n",
    "    \n",
    "    def process_geoencoding_data(self , list_dict_records):\n",
    "        list_extension_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_extension = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Extension\"]\n",
    "            dict_mega_flat = self.obj_backfill_helpers.further_flatten_geocoding(dict_data = dict_extension)\n",
    "            if any(re.search(r\"_\\d+_\", key) for key in dict_mega_flat.keys()):\n",
    "                list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data = dict_mega_flat)\n",
    "                for dict_extension in list_dicts:\n",
    "                    list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_extension , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                    list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                    list_extension_data_tuples.append(tuple(list_output))\n",
    "            else:\n",
    "                list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_mega_flat , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                list_extension_data_tuples.append(tuple(list_output))\n",
    "\n",
    "        list_clean_extension_data_tuples = self.remove_duplicates_keep_order(list_extension_data_tuples)\n",
    "                    \n",
    "        self.bulk_insert_using_copy(data = list_clean_extension_data_tuples , table_name = \"GLEIF_geocoding\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"relevance\",\n",
    "                                            \"match_type\",\n",
    "                                            \"lat\",\n",
    "                                            \"lng\",\n",
    "                                            \"geocoding_date\",\n",
    "                                            \"TopLeft_Latitude\",\n",
    "                                            \"TopLeft_Longitude\",\n",
    "                                            \"BottomRight_Latitude\",\n",
    "                                            \"BottomRight_longitude\",\n",
    "                                            \"match_level\",\n",
    "                                            \"mapped_street\",\n",
    "                                            \"mapped_housenumber\",\n",
    "                                            \"mapped_postalcode\",\n",
    "                                            \"mapped_city\",\n",
    "                                            \"mapped_district\",\n",
    "                                            \"mapped_state\",\n",
    "                                            \"mapped_country\"]) \n",
    "    \n",
    "    def process_all_data(self , list_dict_records):\n",
    "        self.process_entity_data(list_dict_records = list_dict_records)\n",
    "        self.process_other_legal_names(list_dict_records = list_dict_records)\n",
    "        self.process_legal_address(list_dict_records = list_dict_records)\n",
    "        self.process_headquarters_address(list_dict_records = list_dict_records)\n",
    "        self.process_legal_entity_events(list_dict_records = list_dict_records)\n",
    "        self.process_registration_data(list_dict_records = list_dict_records)\n",
    "        self.process_geoencoding_data(list_dict_records = list_dict_records)\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        self.create_tables()\n",
    "        \n",
    "        generator_batched_json = self.load_batches_as_list(file_path = self.str_json_file_path , batch_size = 100000)\n",
    "        \n",
    "        for index , list_dict_records in enumerate(generator_batched_json):\n",
    "            self.process_all_data(list_dict_records = list_dict_records)\n",
    "        \n",
    "        self.obj_backfill_helpers.file_tracker(file_path = self.str_json_file_path , str_db_name = \"GLEIF_test_db\" , str_data_title = \"Level_1_meta_Data\")\n",
    "        \n",
    "        self.conn.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file_path(file_path):\n",
    "    filename = os.path.basename(file_path)\n",
    "    date_part, time_part = filename.split('-')[:2]\n",
    "    formatted_date = f\"{date_part[:4]}-{date_part[4:6]}-{date_part[6:]}\"\n",
    "    formatted_time = f\"{time_part[:2]}:{time_part[2:]} UTC\"\n",
    "    return filename, formatted_date, formatted_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test of new method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = GLEIFLevel1Data_Rewritten(bool_log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../file_lib/Level_1_unpacked//20241217-1600-gleif-goldencopy-lei2-golden-copy.json'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj.str_json_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers()\n",
    "obj_backfill_helpers.file_tracker(file_path = obj.str_json_file_path , str_db_name = \"GLEIF_test_db\" , str_data_title = \"Level_1_Meta_Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = GLEIFLevel1Data_Rewritten(bool_log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_table_names = [\"GLEIF_entity_data\" , \"GLEIF_other_legal_names\" , \"GLEIF_LegalAddress\" , \"GLEIF_HeadquartersAddress\" , \"GLEIF_LegalEntityEvents\" , \"GLEIF_registration_data\" , \"GLEIF_geocoding\"]\n",
    "obj.drop_table(lst_table_names = list_table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_table_names = [\"gleif_files_processed\"]\n",
    "obj.drop_table(lst_table_names = list_table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.storing_GLEIF_data_in_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_helper = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the JSON incrementally and store the first 10 dictionaries\n",
    "first_10_records = []\n",
    "\n",
    "with open(obj.str_json_file_path, 'r', encoding=\"utf-8\") as file:\n",
    "    records = ijson.items(file, \"records.item\")  # Iterate through 'records' list\n",
    "    for record in records:\n",
    "        first_10_records.append(record)  # Append each dictionary to the list\n",
    "        if len(first_10_records) == 100000:  # Stop after collecting 10 dictionaries\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display((obj_helper.flatten_dict(first_10_records[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_flat = [obj_helper.flatten_dict(dict) for dict in first_10_records]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lei_dict = {d['LEI_$']: d for d in list_flat}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(lei_dict[\"097900BEJX0000002337\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_both_entity_names(data_dict):\n",
    "    \"\"\"\n",
    "    Extracts and organizes both `OtherEntityNames` and `TransliteratedOtherEntityNames` keys.\n",
    "    Returns two lists of tuples:\n",
    "    - First list for OtherEntityNames: (type, value)\n",
    "    - Second list for TransliteratedOtherEntityNames: (type, value)\n",
    "    \"\"\"\n",
    "    other_grouped_data = {}\n",
    "    transliterated_grouped_data = {}\n",
    "\n",
    "    for key, value in data_dict.items():\n",
    "        if \"OtherEntityNames\" in key and \"TransliteratedOtherEntityNames\" not in key:\n",
    "            # This is a regular OtherEntityNames key\n",
    "            match = re.search(r\"_(\\d+)\", key)\n",
    "            if not match:\n",
    "                continue\n",
    "            index = int(match.group(1))\n",
    "\n",
    "            if index not in other_grouped_data:\n",
    "                other_grouped_data[index] = {\"type\": None, \"value\": None}\n",
    "\n",
    "            if \"@type\" in key:\n",
    "                other_grouped_data[index][\"type\"] = value\n",
    "            else:\n",
    "                other_grouped_data[index][\"value\"] = value\n",
    "\n",
    "        elif \"TransliteratedOtherEntityNames\" in key:\n",
    "            # This is a TransliteratedOtherEntityNames key\n",
    "            match = re.search(r\"_(\\d+)\", key)\n",
    "            if not match:\n",
    "                continue\n",
    "            index = int(match.group(1))\n",
    "\n",
    "            if index not in transliterated_grouped_data:\n",
    "                transliterated_grouped_data[index] = {\"type\": None, \"value\": None}\n",
    "\n",
    "            if \"@type\" in key:\n",
    "                transliterated_grouped_data[index][\"type\"] = value\n",
    "            else:\n",
    "                transliterated_grouped_data[index][\"value\"] = value\n",
    "\n",
    "    # Convert grouped data into lists of tuples\n",
    "    other_result = [\n",
    "        (other_grouped_data[idx][\"type\"], other_grouped_data[idx][\"value\"])\n",
    "        for idx in sorted(other_grouped_data.keys())\n",
    "    ]\n",
    "\n",
    "    transliterated_result = [\n",
    "        (transliterated_grouped_data[idx][\"type\"], transliterated_grouped_data[idx][\"value\"])\n",
    "        for idx in sorted(transliterated_grouped_data.keys())\n",
    "    ]\n",
    "\n",
    "    return other_result, transliterated_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(extract_both_entity_names(data_dict = lei_dict[\"097900BEJX0000002337\"])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_result = extract_both_entity_names(data_dict = lei_dict[\"097900BEJX0000002337\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lei_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepend the LEI value to each tuple in other_result\n",
    "for i, tup in enumerate(other_result):\n",
    "    other_result[i] = (lei_value,) + tup\n",
    "\n",
    "# Prepend the LEI value to each tuple in transliterated_result\n",
    "for i, tup in enumerate(transliterated_result):\n",
    "    transliterated_result[i] = (lei_value,) + tup\n",
    "\n",
    "# If you want to combine them into one list, you can do:\n",
    "combined_list = other_result + transliterated_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = obj_helper.flatten_dict(dict_input = first_10_records[0])\n",
    "clean = obj_helper.clean_keys(input_dict = flat)\n",
    "entity = obj_helper.organize_by_prefix(input_dict = clean)[\"Entity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_helper.extract_other_entity_names(data_dict = entity, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(entity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
