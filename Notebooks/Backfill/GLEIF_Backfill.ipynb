{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the jupyter notebook pertaining to the GLEIF_Backfill.py file.\n",
    "\n",
    "This code is responsible for obtaining all Level 1 and Level 2 Relationship data from the GLIEF, and backfilling the data and adding whatever features are needed to make the data accessible for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import psycopg2\n",
    "import sys\n",
    "import io\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "import ijson\n",
    "\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel1Data_Rewritten:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        \n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Backfill_level_1.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            str_level_1_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "            self.str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link , str_unpacked_zip_file_name = \"Level_1_unpacked\" , str_zip_file_path_name = \"Level_1.zip\")\n",
    "    \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/Level_1_unpacked\")[0]\n",
    "        self.str_json_file_path = rf\"../file_lib/Level_1_unpacked\" + \"//\" + str_unpacked_zip_file_name\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    \n",
    "    def drop_table(self , lst_table_names):\n",
    "            \"\"\"\n",
    "            Drops a specific table from the database securely.\n",
    "            \n",
    "            Parameters:\n",
    "                table_name (list of string): The names of the tables to drop.\n",
    "            \"\"\"\n",
    "\n",
    "            for table_name in lst_table_names:\n",
    "                self.cursor.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n",
    "                \n",
    "            self.conn.commit()\n",
    "    \n",
    "    def load_batches_as_list(self , file_path , batch_size=100000):\n",
    "            \"\"\"Yield records in batches as lists of dictionaries.\"\"\"\n",
    "            with open(file_path, 'rb') as file:\n",
    "                # Create an iterator for the 'records' key\n",
    "                records = ijson.items(file, \"records.item\")\n",
    "                \n",
    "                batch = []\n",
    "                for index, record in enumerate(records, start=1):\n",
    "                    batch.append(record)  # Add record to the batch\n",
    "                    \n",
    "                    if index % batch_size == 0:  # Yield the batch when size is reached\n",
    "                        yield batch  # Yield the batch as a list\n",
    "                        batch = []  # Reset for the next batch\n",
    "                \n",
    "                # Yield any remaining records\n",
    "                if batch:\n",
    "                    yield batch\n",
    "    \n",
    "    def create_tables(self):\n",
    "        # GLEIF_entity_data\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_entity_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT UNIQUE,\n",
    "                LegalName TEXT,\n",
    "                LegalJurisdiction TEXT,\n",
    "                EntityCategory TEXT,\n",
    "                EntitySubCategory TEXT,\n",
    "                LegalForm_EntityLegalFormCode TEXT,\n",
    "                LegalForm_OtherLegalForm TEXT,\n",
    "                EntityStatus TEXT,\n",
    "                EntityCreationDate TEXT,\n",
    "                RegistrationAuthority_RegistrationAuthorityID TEXT,\n",
    "                RegistrationAuthority_RegistrationAuthorityEntityID TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_other_legal_names with unique constraint\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_other_legal_names (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                Type TEXT,\n",
    "                OtherEntityNames TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei),\n",
    "                UNIQUE (lei, OtherEntityNames, Type)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_LegalAddress\n",
    "        # If each LEI has only one legal address, keep the UNIQUE on lei\n",
    "        # Otherwise, define a composite unique constraint as needed\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_LegalAddress (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT UNIQUE,\n",
    "                LegalAddress_FirstAddressLine TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_1 TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_2 TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_3 TEXT,\n",
    "                LegalAddress_City TEXT,\n",
    "                LegalAddress_Region TEXT,\n",
    "                LegalAddress_Country TEXT,\n",
    "                LegalAddress_PostalCode TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_HeadquartersAddress\n",
    "        # Similar to GLEIF_LegalAddress\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_HeadquartersAddress (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT UNIQUE,\n",
    "                HeadquartersAddress_FirstAddressLine TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_1 TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_2 TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_3 TEXT,\n",
    "                HeadquartersAddress_City TEXT,\n",
    "                HeadquartersAddress_Region TEXT,\n",
    "                HeadquartersAddress_Country TEXT,\n",
    "                HeadquartersAddress_PostalCode TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_LegalEntityEvents with unique constraint\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_LegalEntityEvents (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                group_type TEXT,\n",
    "                event_status TEXT,\n",
    "                LegalEntityEventType TEXT,\n",
    "                LegalEntityEventEffectiveDate TEXT,\n",
    "                LegalEntityEventRecordedDate TEXT,\n",
    "                ValidationDocuments TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei),\n",
    "                UNIQUE (lei, group_type, event_status, LegalEntityEventType, LegalEntityEventEffectiveDate , LegalEntityEventRecordedDate, ValidationDocuments)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_registration_data\n",
    "        # If each LEI has only one registration record, keep the UNIQUE on lei\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_registration_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT UNIQUE,\n",
    "                InitialRegistrationDate TEXT,\n",
    "                LastUpdateDate TEXT,\n",
    "                RegistrationStatus TEXT,\n",
    "                NextRenewalDate TEXT,\n",
    "                ManagingLOU TEXT,\n",
    "                ValidationSources TEXT,\n",
    "                ValidationAuthorityID TEXT,\n",
    "                ValidationAuthorityEntityID TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # GLEIF_geocoding with unique constraint\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_geocoding (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                relevance TEXT,\n",
    "                match_type TEXT,\n",
    "                lat TEXT,\n",
    "                lng TEXT,\n",
    "                geocoding_date TEXT,\n",
    "                TopLeft_Latitude TEXT,\n",
    "                TopLeft_Longitude TEXT,\n",
    "                BottomRight_Latitude TEXT,\n",
    "                BottomRight_longitude TEXT,\n",
    "                match_level TEXT,\n",
    "                mapped_street TEXT,\n",
    "                mapped_housenumber TEXT,\n",
    "                mapped_postalcode TEXT,\n",
    "                mapped_city TEXT,\n",
    "                mapped_district TEXT,\n",
    "                mapped_state TEXT,\n",
    "                mapped_country TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei),\n",
    "                UNIQUE (lei, relevance, match_type, lat, geocoding_date, TopLeft_Latitude, TopLeft_Longitude, BottomRight_Latitude, BottomRight_longitude, match_level, mapped_street, mapped_housenumber, mapped_postalcode, mapped_city, mapped_district, mapped_state, mapped_country)\n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def bulk_insert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"Perform a bulk insert using PostgreSQL COPY with an in-memory buffer\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to insert into\n",
    "            columns (list): List of column names for the table\n",
    "            data (list): List of tuples with the data to be inserted\n",
    "        \"\"\"\n",
    "\n",
    "        buffer = io.StringIO()\n",
    "\n",
    "        for row in data:\n",
    "            # Escape backslashes and replace None with \\N for NULL\n",
    "            row_converted = []\n",
    "            for x in row:\n",
    "                if x is None:\n",
    "                    row_converted.append('\\\\N')  # NULL representation\n",
    "                elif isinstance(x, str):\n",
    "                    x = x.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "                    row_converted.append(x)\n",
    "                else:\n",
    "                    row_converted.append(str(x))\n",
    "            buffer.write('\\t'.join(row_converted) + '\\n')\n",
    "        \n",
    "        buffer.seek(0)  # Reset buffer position\n",
    "\n",
    "        # Construct the COPY query\n",
    "        copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH (FORMAT text, DELIMITER E'\\t', NULL '\\\\N')\"\n",
    "        self.cursor.copy_expert(copy_query, buffer)\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def process_entity_data(self , list_dict_records):\n",
    "        list_entity_meta_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , subset_string = \"Entity\" , target_keys = [\"LegalName\", \"LegalJurisdiction\", \"EntityCategory\", \"EntitySubCategory\", \"LegalForm_EntityLegalFormCode\", \"LegalForm_OtherLegalForm\", \"EntityStatus\", \"EntityCreationDate\", \"RegistrationAuthority_RegistrationAuthorityID\", \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_entity_meta_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_insert_using_copy(data = list_entity_meta_data_tuples , table_name = \"GLEIF_entity_data\" , \n",
    "                            columns = \n",
    "                            [\"lei\",\n",
    "                                \"LegalName\",\n",
    "                                \"LegalJurisdiction\",\n",
    "                                \"EntityCategory\",\n",
    "                                \"EntitySubCategory\",\n",
    "                                \"LegalForm_EntityLegalFormCode\",\n",
    "                                \"LegalForm_OtherLegalForm\",\n",
    "                                \"EntityStatus\",\n",
    "                                \"EntityCreationDate\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "    \n",
    "    def process_other_legal_names(self , list_dict_records):\n",
    "        list_other_names_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_other_entity_names(data_dict = dict_entity, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup         \n",
    "            list_other_names_tuples.extend(list_output)\n",
    "        \n",
    "        list_clean_other_names_tuples = list(set(list_other_names_tuples))\n",
    "        \n",
    "        self.bulk_insert_using_copy(data = list_clean_other_names_tuples , table_name = \"GLEIF_other_legal_names\" , columns = [\"lei\", \"Type\" , \"OtherEntityNames\"])        \n",
    "    \n",
    "    def process_legal_address(self , list_dict_records):\n",
    "        list_legal_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , target_keys = [\"Entity_LegalAddress_FirstAddressLine\" , \"Entity_LegalAddress_AdditionalAddressLine_1\" , \"Entity_LegalAddress_AdditionalAddressLine_2\" , \"Entity_LegalAddress_AdditionalAddressLine_3\" , \"Entity_LegalAddress_City\" , \"Entity_LegalAddress_Region\" , \"Entity_LegalAddress_Country\" , \"Entity_LegalAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_legal_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        \n",
    "        self.bulk_insert_using_copy(data = list_legal_address_tuples , table_name = \"GLEIF_LegalAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"LegalAddress_FirstAddressLine\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                                            \"LegalAddress_City\",\n",
    "                                            \"LegalAddress_Region\",\n",
    "                                            \"LegalAddress_Country\",\n",
    "                                            \"LegalAddress_PostalCode\"])  \n",
    "    \n",
    "    def process_headquarters_address(self , list_dict_records):\n",
    "        list_headquarters_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity , target_keys = [\"HeadquartersAddress_FirstAddressLine\" , \"HeadquartersAddress_AdditionalAddressLine_1\" , \"HeadquartersAddress_AdditionalAddressLine_2\" , \"HeadquartersAddress_AdditionalAddressLine_3\" , \"HeadquartersAddress_City\" , \"HeadquartersAddress_Region\" , \"HeadquartersAddress_Country\" , \"HeadquartersAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_headquarters_address_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_insert_using_copy(data = list_headquarters_address_tuples , table_name = \"GLEIF_HeadquartersAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"HeadquartersAddress_FirstAddressLine\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                                            \"HeadquartersAddress_City\",\n",
    "                                            \"HeadquartersAddress_Region\",\n",
    "                                            \"HeadquartersAddress_Country\",\n",
    "                                            \"HeadquartersAddress_PostalCode\"]) \n",
    "    \n",
    "    def process_legal_entity_events(self , list_dict_records):\n",
    "        list_legal_entity_events_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_event_data(dict_data = dict_entity , base_keyword=\"LegalEntityEvents\" , target_keys=[\"group_type\", \"event_status\", \"LegalEntityEventType\", \"LegalEntityEventEffectiveDate\", \"LegalEntityEventRecordedDate\", \"ValidationDocuments\"])\n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup \n",
    "            list_legal_entity_events_tuples.extend(list_output)\n",
    "\n",
    "        list_clean_legal_entity_events_tuples = list(set(list_legal_entity_events_tuples))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_clean_legal_entity_events_tuples , table_name = \"GLEIF_LegalEntityEvents\" , \n",
    "                                columns = [\"lei\",\n",
    "                                        \"group_type\",\n",
    "                                        \"event_status\",\n",
    "                                        \"LegalEntityEventType\",\n",
    "                                        \"LegalEntityEventEffectiveDate\",\n",
    "                                        \"LegalEntityEventRecordedDate\",\n",
    "                                        \"ValidationDocuments\"])\n",
    "        \n",
    "    def process_registration_data(self , list_dict_records):\n",
    "        list_registration_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_registration = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Registration\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_registration , target_keys = [\"InitialRegistrationDate\" , \"LastUpdateDate\" , \"RegistrationStatus\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationAuthority_ValidationAuthorityID\" , \"ValidationAuthority_ValidationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_registration_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_registration_tuples , table_name = \"GLEIF_registration_data\" , \n",
    "                                columns = [\n",
    "                                            \"lei\",\n",
    "                                            \"InitialRegistrationDate\",\n",
    "                                            \"LastUpdateDate\",\n",
    "                                            \"RegistrationStatus\",\n",
    "                                            \"NextRenewalDate\",\n",
    "                                            \"ManagingLOU\",\n",
    "                                            \"ValidationSources\",\n",
    "                                            \"ValidationAuthorityID\",\n",
    "                                            \"ValidationAuthorityEntityID\"]) \n",
    "    \n",
    "    def process_geoencoding_data(self , list_dict_records):\n",
    "        list_extension_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_extension = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Extension\"]\n",
    "            dict_mega_flat = self.obj_backfill_helpers.further_flatten_geocoding(dict_data = dict_extension)\n",
    "            if any(re.search(r\"_\\d+_\", key) for key in dict_mega_flat.keys()):\n",
    "                list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data = dict_mega_flat)\n",
    "                for dict_extension in list_dicts:\n",
    "                    list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_extension , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                    list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                    list_extension_data_tuples.append(tuple(list_output))\n",
    "            else:\n",
    "                list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_mega_flat , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                list_extension_data_tuples.append(tuple(list_output))\n",
    "\n",
    "        list_clean_extension_data_tuples = list(set(list_extension_data_tuples))\n",
    "        \n",
    "        with open(\"output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            for item in list_clean_extension_data_tuples:\n",
    "                file.write(f\"{item}\\n\")    \n",
    "                    \n",
    "        self.bulk_insert_using_copy(data = list_clean_extension_data_tuples , table_name = \"GLEIF_geocoding\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"relevance\",\n",
    "                                            \"match_type\",\n",
    "                                            \"lat\",\n",
    "                                            \"lng\",\n",
    "                                            \"geocoding_date\",\n",
    "                                            \"TopLeft_Latitude\",\n",
    "                                            \"TopLeft_Longitude\",\n",
    "                                            \"BottomRight_Latitude\",\n",
    "                                            \"BottomRight_longitude\",\n",
    "                                            \"match_level\",\n",
    "                                            \"mapped_street\",\n",
    "                                            \"mapped_housenumber\",\n",
    "                                            \"mapped_postalcode\",\n",
    "                                            \"mapped_city\",\n",
    "                                            \"mapped_district\",\n",
    "                                            \"mapped_state\",\n",
    "                                            \"mapped_country\"]) \n",
    "    \n",
    "    def process_all_data(self , list_dict_records):\n",
    "        self.process_entity_data(list_dict_records = list_dict_records)\n",
    "        self.process_other_legal_names(list_dict_records = list_dict_records)\n",
    "        self.process_legal_address(list_dict_records = list_dict_records)\n",
    "        self.process_headquarters_address(list_dict_records = list_dict_records)\n",
    "        self.process_legal_entity_events(list_dict_records = list_dict_records)\n",
    "        self.process_registration_data(list_dict_records = list_dict_records)\n",
    "        self.process_geoencoding_data(list_dict_records = list_dict_records)\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        self.create_tables()\n",
    "        \n",
    "        generator_batched_json = self.load_batches_as_list(file_path = self.str_json_file_path , batch_size = 100000)\n",
    "        \n",
    "        for index , list_dict_records in enumerate(generator_batched_json):\n",
    "            self.process_all_data(list_dict_records = list_dict_records)\n",
    "        \n",
    "        self.conn.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = GLEIFLevel1Data_Rewritten(bool_log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_table_names = [\"GLEIF_entity_data\" , \"GLEIF_other_legal_names\" , \"GLEIF_LegalAddress\" , \"GLEIF_HeadquartersAddress\" , \"GLEIF_LegalEntityEvents\" , \"GLEIF_registration_data\" , \"GLEIF_geocoding\"]\n",
    "obj.drop_table(lst_table_names = list_table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstoring_GLEIF_data_in_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 415\u001b[0m, in \u001b[0;36mGLEIFLevel1Data_Rewritten.storing_GLEIF_data_in_database\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    412\u001b[0m generator_batched_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_batches_as_list(file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_json_file_path , batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m)\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index , list_dict_records \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generator_batched_json):\n\u001b[1;32m--> 415\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[2], line 405\u001b[0m, in \u001b[0;36mGLEIFLevel1Data_Rewritten.process_all_data\u001b[1;34m(self, list_dict_records)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_headquarters_address(list_dict_records \u001b[38;5;241m=\u001b[39m list_dict_records)\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_legal_entity_events(list_dict_records \u001b[38;5;241m=\u001b[39m list_dict_records)\n\u001b[1;32m--> 405\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_registration_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_geoencoding_data(list_dict_records \u001b[38;5;241m=\u001b[39m list_dict_records)\n",
      "Cell \u001b[1;32mIn[2], line 335\u001b[0m, in \u001b[0;36mGLEIFLevel1Data_Rewritten.process_registration_data\u001b[1;34m(self, list_dict_records)\u001b[0m\n\u001b[0;32m    332\u001b[0m list_registration_tuples \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dict_record \u001b[38;5;129;01min\u001b[39;00m list_dict_records:\n\u001b[1;32m--> 335\u001b[0m     dict_flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj_backfill_helpers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdict_record\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    336\u001b[0m     dict_clean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj_backfill_helpers\u001b[38;5;241m.\u001b[39mclean_keys(input_dict \u001b[38;5;241m=\u001b[39m dict_flat)\n\u001b[0;32m    337\u001b[0m     dict_registration \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj_backfill_helpers\u001b[38;5;241m.\u001b[39morganize_by_prefix(dict_clean))[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRegistration\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\mattp\\Work_Related\\Gradient_Trading\\GLEIF_Pipeline\\GLEIF_Pipeline\\Production\\Backfill\\GLEIF_Backfill_Helpers.py:89\u001b[0m, in \u001b[0;36mGLEIF_Backill_Helpers.flatten_dict\u001b[1;34m(self, dict_input)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m         dict_flattened[parent_key] \u001b[38;5;241m=\u001b[39m current_element\n\u001b[1;32m---> 89\u001b[0m \u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdict_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dict_flattened\n",
      "File \u001b[1;32mc:\\Users\\mattp\\Work_Related\\Gradient_Trading\\GLEIF_Pipeline\\GLEIF_Pipeline\\Production\\Backfill\\GLEIF_Backfill_Helpers.py:81\u001b[0m, in \u001b[0;36mGLEIF_Backill_Helpers.flatten_dict.<locals>.flatten\u001b[1;34m(current_element, parent_key)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m current_element\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     80\u001b[0m         new_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parent_key \u001b[38;5;28;01melse\u001b[39;00m key\n\u001b[1;32m---> 81\u001b[0m         \u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_element, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(current_element, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mattp\\Work_Related\\Gradient_Trading\\GLEIF_Pipeline\\GLEIF_Pipeline\\Production\\Backfill\\GLEIF_Backfill_Helpers.py:81\u001b[0m, in \u001b[0;36mGLEIF_Backill_Helpers.flatten_dict.<locals>.flatten\u001b[1;34m(current_element, parent_key)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m current_element\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     80\u001b[0m         new_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parent_key \u001b[38;5;28;01melse\u001b[39;00m key\n\u001b[1;32m---> 81\u001b[0m         \u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_element, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(current_element, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\mattp\\Work_Related\\Gradient_Trading\\GLEIF_Pipeline\\GLEIF_Pipeline\\Production\\Backfill\\GLEIF_Backfill_Helpers.py:81\u001b[0m, in \u001b[0;36mGLEIF_Backill_Helpers.flatten_dict.<locals>.flatten\u001b[1;34m(current_element, parent_key)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m current_element\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     80\u001b[0m         new_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m parent_key \u001b[38;5;28;01melse\u001b[39;00m key\n\u001b[1;32m---> 81\u001b[0m         flatten(value, new_key)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(current_element, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(current_element, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obj.storing_GLEIF_data_in_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break. Messing Around checking things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\"GLEIF_geocoding\" , \"GLEIF_registration_data\" , \"GLEIF_LegalEntityEvents\" , \"GLEIF_HeadquartersAddress\" , \"GLEIF_LegalAddress\" , \"GLEIF_other_legal_names\" , \"GLEIF_entity_data\"]\n",
    "\n",
    "for table in tables:\n",
    "    obj.delete_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.storing_GLEIF_data_in_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing all around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_records_ijson(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        count = sum(1 for _ in ijson.items(file, \"records.item\"))\n",
    "    return count\n",
    "\n",
    "def load_batches_as_list(file_path, batch_size=100000):\n",
    "    \"\"\"Yield records in batches as lists of dictionaries.\"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Create an iterator for the 'records' key\n",
    "        records = ijson.items(file, \"records.item\")\n",
    "        \n",
    "        batch = []\n",
    "        for index, record in enumerate(records, start=1):\n",
    "            batch.append(record)  # Add record to the batch\n",
    "            \n",
    "            if index % batch_size == 0:  # Yield the batch when size is reached\n",
    "                yield batch  # Yield the batch as a list\n",
    "                batch = []  # Reset for the next batch\n",
    "        \n",
    "        # Yield any remaining records\n",
    "        if batch:\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storing_GLEIF_data_in_database():\n",
    "        \n",
    "        obj.create_tables()\n",
    "        \n",
    "        generator_batched_json = load_batches_as_list(file_path = obj.str_json_file_path , batch_size = 100000)\n",
    "        \n",
    "        for list_dict_records in generator_batched_json:\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = load_batches_as_list(obj.str_json_file_path, batch_size=100000)\n",
    "\n",
    "for batch_index, batch in enumerate(pp, start=1):\n",
    "    print(f\"Batch {batch_index}: {len(batch)} records\")\n",
    "    #display(batch)  # Display the entire list of dictionaries\n",
    "    \n",
    "    \"\"\"# Break after displaying the first two batches\n",
    "    if batch_index == 2:\n",
    "        break\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_num_entries = 2781566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going in Deep with the processing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_first_100000_records(file_path, batch_size=400000):\n",
    "    \"\"\"Load the first 100,000 records from the 'records' key in the JSON file.\"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Create an iterator for the 'records' key\n",
    "        records = ijson.items(file, \"records.item\")\n",
    "        \n",
    "        # Collect the first batch of records\n",
    "        batch = []\n",
    "        for index, record in enumerate(records, start=1):\n",
    "            batch.append(record)\n",
    "            if index >= batch_size:  # Stop after batch_size records\n",
    "                break\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict_records = load_first_100000_records(file_path = obj.str_json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_flate = obj_helpers.flatten_dict(list_dict_records)\n",
    "list_clean = obj_helpers.clean_keys(list_flate)\n",
    "\n",
    "for record in list_clean:\n",
    "    # Check if the specific key exists in the current record\n",
    "    if \"254900MCCDCAJRMOD236\" in record.values():\n",
    "        display(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, dict in enumerate(list_dict_records):\n",
    "    pp_flat = obj_helpers.flatten_dict(pp)\n",
    "    pp_clean = obj_helpers.clean_keys(pp_flat)\n",
    "    \n",
    "    if \"Entity_LegalEntityEvents_LegalEntityEvent_1_@group_type\" in pp_clean.keys():\n",
    "        display(index)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(list_dict_records[8])\n",
    "pp = list_dict_records[0]\n",
    "pp_flat = obj_helpers.flatten_dict(pp)\n",
    "#display(pp_flat)\n",
    "display(obj_helpers.clean_keys(pp_flat))\n",
    "pp_clean = obj_helpers.clean_keys(pp_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_data(list_dict_records):\n",
    "    \n",
    "        \n",
    "    list_entity_process_entity_data = process_entity_data(list_dict_records = list_dict_records)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
