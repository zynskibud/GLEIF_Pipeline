{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the jupyter notebook pertaining to the GLEIF_Backfill.py file.\n",
    "\n",
    "This code is responsible for obtaining all Level 1 and Level 2 Relationship data from the GLIEF, and backfilling the data and adding whatever features are needed to make the data accessible for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import logging\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import bigjson\n",
    "import json\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import sys\n",
    "import math\n",
    "import io\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "import ijson\n",
    "\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel1Data:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        \n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Backfill_level_1.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            str_level_1_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "            self.str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link , str_unpacked_zip_file_name = \"Level_1_unpacked\" , str_zip_file_path_name = \"Level_1.zip\")\n",
    "    \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/Level_1_unpacked\")[0]\n",
    "        self.str_json_file_path = rf\"../file_lib/Level_1_unpacked\" + \"//\" + str_unpacked_zip_file_name\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def delete_table(self, table_name):\n",
    "        \"\"\"\n",
    "        Deletes a single table from the PostgreSQL database.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to delete.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Sanitize table name to prevent SQL injection\n",
    "            drop_query = sql.SQL(\"DROP TABLE IF EXISTS {table} CASCADE;\").format(\n",
    "                table=sql.Identifier(table_name)\n",
    "            )\n",
    "\n",
    "            # Execute the DROP TABLE command\n",
    "            self.cursor.execute(drop_query)\n",
    "\n",
    "            # Log and print success message\n",
    "            logging.info(f\"Successfully deleted table: {table_name}\")\n",
    "            print(f\"Successfully deleted table: {table_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log and print error message\n",
    "            logging.error(f\"Error deleting table {table_name}: {e}\")\n",
    "            print(f\"Error deleting table {table_name}: {e}\")\n",
    "\n",
    "    \n",
    "    def create_tables(self):\n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_entity_data (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT UNIQUE,\n",
    "                    LegalName TEXT,\n",
    "                    LegalJurisdiction TEXT,\n",
    "                    EntityCategory TEXT,\n",
    "                    EntitySubCategory TEXT,\n",
    "                    LegalForm_EntityLegalFormCode TEXT,\n",
    "                    LegalForm_OtherLegalForm TEXT,\n",
    "                    EntityStatus TEXT,\n",
    "                    EntityCreationDate TEXT,\n",
    "                    RegistrationAuthority_RegistrationAuthorityID TEXT,\n",
    "                    RegistrationAuthority_RegistrationAuthorityEntityID TEXT\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_other_legal_names (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT,\n",
    "                    OtherEntityNames TEXT,\n",
    "                    Type TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_LegalAddress (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT UNIQUE,\n",
    "                    LegalAddress_FirstAddressLine TEXT,\n",
    "                    LegalAddress_AdditionalAddressLine_1 TEXT,\n",
    "                    LegalAddress_AdditionalAddressLine_2 TEXT,\n",
    "                    LegalAddress_AdditionalAddressLine_3 TEXT,\n",
    "                    LegalAddress_City TEXT,\n",
    "                    LegalAddress_Region TEXT,\n",
    "                    LegalAddress_Country TEXT,\n",
    "                    LegalAddress_PostalCode TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_HeadquartersAddress (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT UNIQUE,\n",
    "                    HeadquartersAddress_FirstAddressLine TEXT,\n",
    "                    HeadquartersAddress_AdditionalAddressLine_1 TEXT,\n",
    "                    HeadquartersAddress_AdditionalAddressLine_2 TEXT,\n",
    "                    HeadquartersAddress_AdditionalAddressLine_3 TEXT,\n",
    "                    HeadquartersAddress_City TEXT,\n",
    "                    HeadquartersAddress_Region TEXT,\n",
    "                    HeadquartersAddress_Country TEXT,\n",
    "                    HeadquartersAddress_PostalCode TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "                        \n",
    "        # LegalEntityEvents\n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_LegalEntityEvents (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT,\n",
    "                    group_type TEXT,\n",
    "                    event_status TEXT,\n",
    "                    LegalEntityEventType TEXT,\n",
    "                    LegalEntityEventEffectiveDate TEXT,\n",
    "                    LegalEntityEventRecordedDate TEXT,\n",
    "                    ValidationDocuments TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        # Registration Data\n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_registration_data (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT UNIQUE,\n",
    "                    InitialRegistrationDate TEXT,\n",
    "                    LastUpdateDate TEXT,\n",
    "                    RegistrationStatus TEXT,\n",
    "                    NextRenewalDate TEXT,\n",
    "                    ManagingLOU TEXT,\n",
    "                    ValidationSources TEXT,\n",
    "                    ValidationAuthorityID TEXT,\n",
    "                    ValidationAuthorityEntityID TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        # Geoencoding\n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_geocoding (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT,\n",
    "                    relevance TEXT,\n",
    "                    match_type TEXT,\n",
    "                    lat TEXT,\n",
    "                    lng TEXT,\n",
    "                    geocoding_date TEXT,\n",
    "                    TopLeft_Latitude TEXT,\n",
    "                    TopLeft_Longitude TEXT,\n",
    "                    BottomRight_Latitude TEXT,\n",
    "                    BottomRight_longitude TEXT,\n",
    "                    match_level TEXT,\n",
    "                    mapped_street TEXT,\n",
    "                    mapped_housenumber TEXT,\n",
    "                    mapped_postalcode TEXT,\n",
    "                    mapped_city TEXT,\n",
    "                    mapped_district TEXT,\n",
    "                    mapped_state TEXT,\n",
    "                    mapped_country TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.conn.commit()\n",
    "\n",
    "        \n",
    "    #copy from other code\n",
    "\n",
    "    def load_batches_as_list(self , file_path , batch_size=100000):\n",
    "        \"\"\"Yield records in batches as lists of dictionaries.\"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Create an iterator for the 'records' key\n",
    "            records = ijson.items(file, \"records.item\")\n",
    "            \n",
    "            batch = []\n",
    "            for index, record in enumerate(records, start=1):\n",
    "                batch.append(record)  # Add record to the batch\n",
    "                \n",
    "                if index % batch_size == 0:  # Yield the batch when size is reached\n",
    "                    yield batch  # Yield the batch as a list\n",
    "                    batch = []  # Reset for the next batch\n",
    "            \n",
    "            # Yield any remaining records\n",
    "            if batch:\n",
    "                yield batch\n",
    "    \n",
    "    def bulk_insert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"Perform a bulk insert using PostgreSQL COPY with an in-memory buffer\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to insert into\n",
    "            columns (list): List of column names for the table\n",
    "            data (list): List of tuples with the data to be inserted\n",
    "        \"\"\"\n",
    "\n",
    "        buffer = io.StringIO()\n",
    "\n",
    "        for row in data:\n",
    "            # Escape backslashes and replace None with \\N for NULL\n",
    "            row_converted = []\n",
    "            for x in row:\n",
    "                if x is None:\n",
    "                    row_converted.append('\\\\N')  # NULL representation\n",
    "                elif isinstance(x, str):\n",
    "                    x = x.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "                    row_converted.append(x)\n",
    "                else:\n",
    "                    row_converted.append(str(x))\n",
    "            buffer.write('\\t'.join(row_converted) + '\\n')\n",
    "        \n",
    "        buffer.seek(0)  # Reset buffer position\n",
    "\n",
    "        # Construct the COPY query\n",
    "        copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH (FORMAT text, DELIMITER E'\\t', NULL '\\\\N')\"\n",
    "        self.cursor.copy_expert(copy_query, buffer)\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def process_entity_data(self , list_dict_records):\n",
    "        list_entity_meta_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , subset_string = \"Entity\" , target_keys = [\"LegalName\", \"LegalJurisdiction\", \"EntityCategory\", \"EntitySubCategory\", \"LegalForm_EntityLegalFormCode\", \"LegalForm_OtherLegalForm\", \"EntityStatus\", \"EntityCreationDate\", \"RegistrationAuthority_RegistrationAuthorityID\", \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_entity_meta_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_insert_using_copy(data = list_entity_meta_data_tuples , table_name = \"GLEIF_entity_data\" , \n",
    "                            columns = \n",
    "                            [\"lei\",\n",
    "                                \"LegalName\",\n",
    "                                \"LegalJurisdiction\",\n",
    "                                \"EntityCategory\",\n",
    "                                \"EntitySubCategory\",\n",
    "                                \"LegalForm_EntityLegalFormCode\",\n",
    "                                \"LegalForm_OtherLegalForm\",\n",
    "                                \"EntityStatus\",\n",
    "                                \"EntityCreationDate\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "    \n",
    "    def process_other_legal_names(self , list_dict_records):\n",
    "        list_other_names_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_other_entity_names(data_dict = dict_entity, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup         \n",
    "            list_other_names_tuples.extend(list_output)\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_other_names_tuples , table_name = \"GLEIF_other_legal_names\" , columns = [\"lei\", \"OtherEntityNames\", \"Type\"])        \n",
    "    \n",
    "    def process_legal_address(self , list_dict_records):\n",
    "        list_legal_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , target_keys = [\"Entity_LegalAddress_FirstAddressLine\" , \"Entity_LegalAddress_AdditionalAddressLine_1\" , \"Entity_LegalAddress_AdditionalAddressLine_2\" , \"Entity_LegalAddress_AdditionalAddressLine_3\" , \"Entity_LegalAddress_City\" , \"Entity_LegalAddress_Region\" , \"Entity_LegalAddress_Country\" , \"Entity_LegalAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_legal_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_legal_address_tuples , table_name = \"GLEIF_LegalAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"LegalAddress_FirstAddressLine\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                                            \"LegalAddress_City\",\n",
    "                                            \"LegalAddress_Region\",\n",
    "                                            \"LegalAddress_Country\",\n",
    "                                            \"LegalAddress_PostalCode\"])  \n",
    "    \n",
    "    def process_headquarters_address(self , list_dict_records):\n",
    "        list_headquarters_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity , target_keys = [\"HeadquartersAddress_FirstAddressLine\" , \"HeadquartersAddress_AdditionalAddressLine_1\" , \"HeadquartersAddress_AdditionalAddressLine_2\" , \"HeadquartersAddress_AdditionalAddressLine_3\" , \"HeadquartersAddress_City\" , \"HeadquartersAddress_Region\" , \"HeadquartersAddress_Country\" , \"HeadquartersAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_headquarters_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_headquarters_address_tuples , table_name = \"GLEIF_HeadquartersAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"HeadquartersAddress_FirstAddressLine\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                                            \"HeadquartersAddress_City\",\n",
    "                                            \"HeadquartersAddress_Region\",\n",
    "                                            \"HeadquartersAddress_Country\",\n",
    "                                            \"HeadquartersAddress_PostalCode\"]) \n",
    "    \n",
    "    def process_legal_entity_events(self , list_dict_records):\n",
    "        list_legal_entity_events_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_event_data(dict_data = dict_entity , base_keyword=\"LegalEntityEvents\" , target_keys=[\"group_type\", \"event_status\", \"LegalEntityEventType\", \"LegalEntityEventEffectiveDate\", \"LegalEntityEventRecordedDate\", \"ValidationDocuments\"])\n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup \n",
    "            list_legal_entity_events_tuples.extend(list_output)\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_legal_entity_events_tuples , table_name = \"GLEIF_LegalEntityEvents\" , \n",
    "                                columns = [\"lei\",\n",
    "                                        \"group_type\",\n",
    "                                        \"event_status\",\n",
    "                                        \"LegalEntityEventType\",\n",
    "                                        \"LegalEntityEventEffectiveDate\",\n",
    "                                        \"LegalEntityEventRecordedDate\",\n",
    "                                        \"ValidationDocuments\"])\n",
    "        \n",
    "    def process_registration_data(self , list_dict_records):\n",
    "        list_registration_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_registration = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Registration\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_registration , target_keys = [\"InitialRegistrationDate\" , \"LastUpdateDate\" , \"RegistrationStatus\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationAuthority_ValidationAuthorityID\" , \"ValidationAuthority_ValidationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_registration_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_registration_tuples , table_name = \"GLEIF_registration_data\" , \n",
    "                                columns = [\n",
    "                                            \"lei\",\n",
    "                                            \"InitialRegistrationDate\",\n",
    "                                            \"LastUpdateDate\",\n",
    "                                            \"RegistrationStatus\",\n",
    "                                            \"NextRenewalDate\",\n",
    "                                            \"ManagingLOU\",\n",
    "                                            \"ValidationSources\",\n",
    "                                            \"ValidationAuthorityID\",\n",
    "                                            \"ValidationAuthorityEntityID\"]) \n",
    "    \n",
    "    def process_geoencoding_data(self , list_dict_records):\n",
    "        list_extension_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_extension = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Extension\"]\n",
    "            dict_mega_flat = self.obj_backfill_helpers.further_flatten_geocoding(dict_data = dict_extension)\n",
    "            if any(re.search(r\"_\\d+_\", key) for key in dict_mega_flat.keys()):\n",
    "                list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data = dict_mega_flat)\n",
    "                for dict_extension in list_dicts:\n",
    "                    list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_extension , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                    list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                    list_extension_data_tuples.append(tuple(list_output))\n",
    "            else:\n",
    "                list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_mega_flat , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                list_extension_data_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_extension_data_tuples , table_name = \"GLEIF_geocoding\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"relevance\",\n",
    "                                            \"match_type\",\n",
    "                                            \"lat\",\n",
    "                                            \"lng\",\n",
    "                                            \"geocoding_date\",\n",
    "                                            \"TopLeft_Latitude\",\n",
    "                                            \"TopLeft_Longitude\",\n",
    "                                            \"BottomRight_Latitude\",\n",
    "                                            \"BottomRight_longitude\",\n",
    "                                            \"match_level\",\n",
    "                                            \"mapped_street\",\n",
    "                                            \"mapped_housenumber\",\n",
    "                                            \"mapped_postalcode\",\n",
    "                                            \"mapped_city\",\n",
    "                                            \"mapped_district\",\n",
    "                                            \"mapped_state\",\n",
    "                                            \"mapped_country\"]) \n",
    "    \n",
    "    def process_all_data(self , list_dict_records):\n",
    "        self.process_entity_data(list_dict_records = list_dict_records)\n",
    "        self.process_other_legal_names(list_dict_records = list_dict_records)\n",
    "        self.process_legal_address(list_dict_records = list_dict_records)\n",
    "        self.process_headquarters_address(list_dict_records = list_dict_records)\n",
    "        self.process_legal_entity_events(list_dict_records = list_dict_records)\n",
    "        self.process_registration_data(list_dict_records = list_dict_records)\n",
    "        self.process_geoencoding_data(list_dict_records = list_dict_records)\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        self.create_tables()\n",
    "        \n",
    "        generator_batched_json = self.load_batches_as_list(file_path = self.str_json_file_path , batch_size = 100000)\n",
    "        \n",
    "        for index , list_dict_records in enumerate(generator_batched_json):\n",
    "            self.process_all_data(list_dict_records = list_dict_records)\n",
    "        \n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = GLEIFLevel1Data(bool_log = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstoring_GLEIF_data_in_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 411\u001b[0m, in \u001b[0;36mGLEIFLevel1Data.storing_GLEIF_data_in_database\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    408\u001b[0m generator_batched_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_batches_as_list(file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_json_file_path , batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index , list_dict_records \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generator_batched_json):\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[6], line 402\u001b[0m, in \u001b[0;36mGLEIFLevel1Data.process_all_data\u001b[1;34m(self, list_dict_records)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_legal_entity_events(list_dict_records \u001b[38;5;241m=\u001b[39m list_dict_records)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_registration_data(list_dict_records \u001b[38;5;241m=\u001b[39m list_dict_records)\n\u001b[1;32m--> 402\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_geoencoding_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 375\u001b[0m, in \u001b[0;36mGLEIFLevel1Data.process_geoencoding_data\u001b[1;34m(self, list_dict_records)\u001b[0m\n\u001b[0;32m    372\u001b[0m         list_output\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m , dict_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEI\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    373\u001b[0m         list_extension_data_tuples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mtuple\u001b[39m(list_output))\n\u001b[1;32m--> 375\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbulk_insert_using_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlist_extension_data_tuples\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGLEIF_geocoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlei\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrelevance\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatch_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlng\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgeocoding_date\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTopLeft_Latitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTopLeft_Longitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBottomRight_Latitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBottomRight_longitude\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmatch_level\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmapped_street\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmapped_housenumber\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmapped_postalcode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmapped_city\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmapped_district\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmapped_state\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmapped_country\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 228\u001b[0m, in \u001b[0;36mGLEIFLevel1Data.bulk_insert_using_copy\u001b[1;34m(self, table_name, columns, data)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Construct the COPY query\u001b[39;00m\n\u001b[0;32m    227\u001b[0m copy_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOPY \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) FROM STDIN WITH (FORMAT text, DELIMITER E\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, NULL \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_expert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "obj.storing_GLEIF_data_in_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break. Messing Around checking things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\"GLEIF_geocoding\" , \"GLEIF_registration_data\" , \"GLEIF_LegalEntityEvents\" , \"GLEIF_HeadquartersAddress\" , \"GLEIF_LegalAddress\" , \"GLEIF_other_legal_names\" , \"GLEIF_entity_data\"]\n",
    "\n",
    "for table in tables:\n",
    "    obj.delete_table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "UniqueViolation",
     "evalue": "duplicate key value violates unique constraint \"gleif_other_legal_names_lei_key\"\nDETAIL:  Key (lei)=(01YA4M3ZQ45K0V190G80) already exists.\nCONTEXT:  COPY gleif_other_legal_names, line 8\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUniqueViolation\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstoring_GLEIF_data_in_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 411\u001b[0m, in \u001b[0;36mGLEIFLevel1Data.storing_GLEIF_data_in_database\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    408\u001b[0m generator_batched_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_batches_as_list(file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_json_file_path , batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m)\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index , list_dict_records \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(generator_batched_json):\n\u001b[1;32m--> 411\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[2], line 397\u001b[0m, in \u001b[0;36mGLEIFLevel1Data.process_all_data\u001b[1;34m(self, list_dict_records)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_all_data\u001b[39m(\u001b[38;5;28mself\u001b[39m , list_dict_records):\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_entity_data(list_dict_records \u001b[38;5;241m=\u001b[39m list_dict_records)\n\u001b[1;32m--> 397\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_other_legal_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_legal_address(list_dict_records \u001b[38;5;241m=\u001b[39m list_dict_records)\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_headquarters_address(list_dict_records \u001b[38;5;241m=\u001b[39m list_dict_records)\n",
      "Cell \u001b[1;32mIn[2], line 267\u001b[0m, in \u001b[0;36mGLEIFLevel1Data.process_other_legal_names\u001b[1;34m(self, list_dict_records)\u001b[0m\n\u001b[0;32m    264\u001b[0m         list_output[index] \u001b[38;5;241m=\u001b[39m (dict_clean[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEI\u001b[39m\u001b[38;5;124m\"\u001b[39m],) \u001b[38;5;241m+\u001b[39m tup         \n\u001b[0;32m    265\u001b[0m     list_other_names_tuples\u001b[38;5;241m.\u001b[39mextend(list_output)\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbulk_insert_using_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlist_other_names_tuples\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGLEIF_other_legal_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlei\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOtherEntityNames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mType\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 228\u001b[0m, in \u001b[0;36mGLEIFLevel1Data.bulk_insert_using_copy\u001b[1;34m(self, table_name, columns, data)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;66;03m# Construct the COPY query\u001b[39;00m\n\u001b[0;32m    227\u001b[0m copy_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOPY \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) FROM STDIN WITH (FORMAT text, DELIMITER E\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, NULL \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 228\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_expert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[1;31mUniqueViolation\u001b[0m: duplicate key value violates unique constraint \"gleif_other_legal_names_lei_key\"\nDETAIL:  Key (lei)=(01YA4M3ZQ45K0V190G80) already exists.\nCONTEXT:  COPY gleif_other_legal_names, line 8\n"
     ]
    }
   ],
   "source": [
    "obj.storing_GLEIF_data_in_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing all around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_records_ijson(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        count = sum(1 for _ in ijson.items(file, \"records.item\"))\n",
    "    return count\n",
    "\n",
    "def load_batches_as_list(file_path, batch_size=100000):\n",
    "    \"\"\"Yield records in batches as lists of dictionaries.\"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Create an iterator for the 'records' key\n",
    "        records = ijson.items(file, \"records.item\")\n",
    "        \n",
    "        batch = []\n",
    "        for index, record in enumerate(records, start=1):\n",
    "            batch.append(record)  # Add record to the batch\n",
    "            \n",
    "            if index % batch_size == 0:  # Yield the batch when size is reached\n",
    "                yield batch  # Yield the batch as a list\n",
    "                batch = []  # Reset for the next batch\n",
    "        \n",
    "        # Yield any remaining records\n",
    "        if batch:\n",
    "            yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storing_GLEIF_data_in_database():\n",
    "        \n",
    "        obj.create_tables()\n",
    "        \n",
    "        generator_batched_json = load_batches_as_list(file_path = obj.str_json_file_path , batch_size = 100000)\n",
    "        \n",
    "        for list_dict_records in generator_batched_json:\n",
    "                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = load_batches_as_list(obj.str_json_file_path, batch_size=100000)\n",
    "\n",
    "for batch_index, batch in enumerate(pp, start=1):\n",
    "    print(f\"Batch {batch_index}: {len(batch)} records\")\n",
    "    #display(batch)  # Display the entire list of dictionaries\n",
    "    \n",
    "    \"\"\"# Break after displaying the first two batches\n",
    "    if batch_index == 2:\n",
    "        break\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_num_entries = 2781566"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going in Deep with the processing logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_first_100000_records(file_path, batch_size=400000):\n",
    "    \"\"\"Load the first 100,000 records from the 'records' key in the JSON file.\"\"\"\n",
    "    with open(file_path, 'rb') as file:\n",
    "        # Create an iterator for the 'records' key\n",
    "        records = ijson.items(file, \"records.item\")\n",
    "        \n",
    "        # Collect the first batch of records\n",
    "        batch = []\n",
    "        for index, record in enumerate(records, start=1):\n",
    "            batch.append(record)\n",
    "            if index >= batch_size:  # Stop after batch_size records\n",
    "                break\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dict_records = load_first_100000_records(file_path = obj.str_json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_flate = obj_helpers.flatten_dict(list_dict_records)\n",
    "list_clean = obj_helpers.clean_keys(list_flate)\n",
    "\n",
    "for record in list_clean:\n",
    "    # Check if the specific key exists in the current record\n",
    "    if \"254900MCCDCAJRMOD236\" in record.values():\n",
    "        display(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, dict in enumerate(list_dict_records):\n",
    "    pp_flat = obj_helpers.flatten_dict(pp)\n",
    "    pp_clean = obj_helpers.clean_keys(pp_flat)\n",
    "    \n",
    "    if \"Entity_LegalEntityEvents_LegalEntityEvent_1_@group_type\" in pp_clean.keys():\n",
    "        display(index)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(list_dict_records[8])\n",
    "pp = list_dict_records[0]\n",
    "pp_flat = obj_helpers.flatten_dict(pp)\n",
    "#display(pp_flat)\n",
    "display(obj_helpers.clean_keys(pp_flat))\n",
    "pp_clean = obj_helpers.clean_keys(pp_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_data(list_dict_records):\n",
    "    \n",
    "        \n",
    "    list_entity_process_entity_data = process_entity_data(list_dict_records = list_dict_records)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
