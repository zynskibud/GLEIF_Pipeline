{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import logging\n",
    "import json\n",
    "import psycopg2\n",
    "import io\n",
    "import sys\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel2Data:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_2_Trees = True)\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Backfill_level_2.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            str_level_2_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "            self.str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_2_download_link , str_unpacked_zip_file_path_name = \"Level_2_unpacked\" , str_zip_file_path_name = \"Level_2.zip\")\n",
    "        else:\n",
    "            str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/Level_2_unpacked\")[-1]\n",
    "            self.str_json_file_path = rf\"../file_lib/Level_2_unpacked\" + \"//\" + str_unpacked_zip_file_name\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def create_table(self):\n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_relationship_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                StartNode TEXT NOT NULL,\n",
    "                EndNode TEXT NOT NULL,\n",
    "                RelationshipType TEXT NOT NULL,\n",
    "                RelationshipStatus TEXT,\n",
    "                RegistrationStatus TEXT,\n",
    "                InitialRegistrationDate TEXT,\n",
    "                LastUpdateDate TEXT,\n",
    "                NextRenewalDate TEXT,\n",
    "                ManagingLOU TEXT,\n",
    "                ValidationSources TEXT,\n",
    "                ValidationDocuments TEXT,\n",
    "                ValidationReference TEXT,\n",
    "                UNIQUE (StartNode, EndNode, RelationshipType)\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_relationship_date_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                StartNode TEXT NOT NULL,\n",
    "                EndNode TEXT NOT NULL,\n",
    "                RelationshipType TEXT NOT NULL,\n",
    "                StartDate TEXT,\n",
    "                EndDate TEXT,\n",
    "                PeriodType TEXT,\n",
    "                FOREIGN KEY (StartNode, EndNode, RelationshipType) \n",
    "                    REFERENCES GLEIF_relationship_data(StartNode, EndNode, RelationshipType),\n",
    "                UNIQUE (StartNode, EndNode, RelationshipType, StartDate, PeriodType)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_relationship_qualifiers (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                StartNode TEXT NOT NULL,\n",
    "                EndNode TEXT NOT NULL,\n",
    "                RelationshipType TEXT NOT NULL,                \n",
    "                QualifierDimension TEXT,                \n",
    "                QualifierCategory TEXT,\n",
    "                FOREIGN KEY (StartNode, EndNode, RelationshipType) \n",
    "                    REFERENCES GLEIF_relationship_data(StartNode, EndNode, RelationshipType),\n",
    "                UNIQUE (StartNode, EndNode, RelationshipType , QualifierDimension , QualifierCategory)                \n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_relationship_quantifiers (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                StartNode TEXT NOT NULL,\n",
    "                EndNode TEXT NOT NULL,\n",
    "                RelationshipType TEXT NOT NULL,               \n",
    "                MeasurementMethod TEXT,                \n",
    "                QuantifierAmount TEXT,\n",
    "                QuantifierUnits TEXT,\n",
    "                FOREIGN KEY (StartNode, EndNode, RelationshipType) \n",
    "                    REFERENCES GLEIF_relationship_data(StartNode, EndNode, RelationshipType),\n",
    "                UNIQUE (StartNode, EndNode, RelationshipType, MeasurementMethod, QuantifierAmount, QuantifierUnits)                \n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.conn.commit()\n",
    "    \n",
    "    def drop_table(self , lst_table_names):\n",
    "            \"\"\"\n",
    "            Drops a specific table from the database securely.\n",
    "            \n",
    "            Parameters:\n",
    "                table_name (list of string): The names of the tables to drop.\n",
    "            \"\"\"\n",
    "\n",
    "            for table_name in lst_table_names:\n",
    "                self.cursor.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n",
    "                \n",
    "            self.conn.commit()\n",
    "    \n",
    "    def bulk_insert_using_copy(self , table_name , columns, data):\n",
    "        \"\"\"Perform a bulk insert using PostgreSQL COPY with an in-memory buffer\n",
    "\n",
    "        Args:\n",
    "            table_name (_type_): Name of the table to insert into\n",
    "            columns (_type_): List of column names for the table\n",
    "            data (_type_): List of tuples with the data to be inserted\n",
    "        \"\"\"\n",
    "        \n",
    "        buffer = io.StringIO()\n",
    "        \n",
    "        #write data to the buffer\n",
    "        \n",
    "        for row in data:\n",
    "            buffer.write('\\t'.join(map(str , row)) + \"\\n\")\n",
    "        buffer.seek(0) #reset buffer position to the beginning\n",
    "        \n",
    "        #Construct the copy query\n",
    "        copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH DELIMITER '\\t'\"\n",
    "        self.cursor.copy_expert(copy_query , buffer)\n",
    "        self.conn.commit\n",
    "    \n",
    "    \n",
    "    def clean_string(self , value):\n",
    "        if value:\n",
    "            # Replace specific problematic sequences\n",
    "            value = value.replace('\\\\00', '').replace('\\\\09', '')\n",
    "            # Replace backslashes with forward slashes or keep as needed\n",
    "            value = value.replace('\\\\', '/')\n",
    "            return value.strip()\n",
    "        return None\n",
    "        \n",
    "    def clean_url(self, list_input):\n",
    "        \"\"\"Clean the ValidationReference field (index 11).\"\"\"\n",
    "        if list_input[11]:\n",
    "            original_value = list_input[11]\n",
    "            list_input[11] = self.clean_string(list_input[11])\n",
    "            logging.debug(f\"Original: {original_value}, Cleaned: {list_input[11]}\")\n",
    "        else:\n",
    "            list_input[11] = None\n",
    "        return list_input\n",
    "        \n",
    "    def remove_duplicates_keep_order(self , input_list):\n",
    "        seen = set()\n",
    "        output_list = []\n",
    "        for item in input_list:\n",
    "            if item not in seen:\n",
    "                output_list.append(item)\n",
    "                seen.add(item)\n",
    "        return output_list\n",
    "    \n",
    "    def process_meta_data(self , dict_relationships):\n",
    "        list_tuples_relationships = []\n",
    "\n",
    "        for dict_relationship in dict_relationships:\n",
    "            dict_relationship_flattened = self.obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_relationship_flattened, subset_string = True, target_keys = [\"StartNode\" , \"EndNode\" , \"RelationshipType\" , \"RelationshipStatus\" , \"RegistrationStatus\" , \"InitialRegistrationDate\" , \"LastUpdateDate\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationDocuments\" , \"ValidationReference\"])\n",
    "            list_clean_output = self.clean_url(list_input = list_output)\n",
    "            list_tuples_relationships.append(tuple(list_clean_output))\n",
    "        \n",
    "        list_clean_tuples_relationships = self.remove_duplicates_keep_order(list_tuples_relationships)\n",
    "        \n",
    "        self.bulk_insert_using_copy(table_name = \"GLEIF_relationship_data\" , \n",
    "                                            data = list_clean_tuples_relationships , \n",
    "                                            columns = [\n",
    "                                                        'StartNode',\n",
    "                                                        'EndNode',\n",
    "                                                        'RelationshipType',\n",
    "                                                        'RelationshipStatus',\n",
    "                                                        'RegistrationStatus',\n",
    "                                                        'InitialRegistrationDate',\n",
    "                                                        'LastUpdateDate',\n",
    "                                                        'NextRenewalDate',\n",
    "                                                        'ManagingLOU',\n",
    "                                                        'ValidationSources',\n",
    "                                                        'ValidationDocuments',\n",
    "                                                        'ValidationReference'\n",
    "                                                    ])      \n",
    "        \n",
    "    def process_relationships_date_data(self , dict_relationships):\n",
    "        list_relationship_date_data = []\n",
    "        \n",
    "        for dict_relationship in dict_relationships:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "            list_tuples_dates = self.obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"RelationshipPeriods_RelationshipPeriod\" , target_keys = [\"StartDate\" , \"EndDate\" , \"PeriodType\"])\n",
    "            \n",
    "            list_unique_keys = self.obj_backfill_helpers.get_target_values(dict_data = dict_flat, subset_string = True, target_keys = [\"StartNode\" , \"EndNode\" , \"RelationshipType\"])\n",
    "            \n",
    "            list_tuples_with_keys = [(*list_unique_keys, *tup) for tup in list_tuples_dates]\n",
    "            \n",
    "            # Append the result to the main list\n",
    "            list_relationship_date_data.extend(list_tuples_with_keys)\n",
    "        \n",
    "        list_clean_relationship_date_data = self.remove_duplicates_keep_order(list_relationship_date_data)\n",
    "\n",
    "        \n",
    "        self.bulk_insert_using_copy(table_name = \"GLEIF_relationship_date_data\" , \n",
    "                                    data = list_clean_relationship_date_data , \n",
    "                                    columns = [\n",
    "                                        'StartNode',\n",
    "                                        'EndNode',\n",
    "                                        'RelationshipType',                                        \n",
    "                                        \"StartDate\",\n",
    "                                        \"EndDate\",\n",
    "                                        \"PeriodType\"])\n",
    "                \n",
    "    def process_relationships_qualifiers(self , dict_relationships):\n",
    "        list_qualifier_data = []\n",
    "            \n",
    "        for dict_relationship in dict_relationships:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "            list_tuples_qualifiers = self.obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"RelationshipQualifiers_RelationshipQualifier\" , target_keys = [\"QualifierDimension\" , \"QualifierCategory\"])\n",
    "            \n",
    "            list_unique_keys = self.obj_backfill_helpers.get_target_values(dict_data = dict_flat, subset_string = True, target_keys = [\"StartNode\" , \"EndNode\" , \"RelationshipType\"])\n",
    "            \n",
    "            list_tuples_with_keys = [(*list_unique_keys, *tup) for tup in list_tuples_qualifiers]\n",
    "            \n",
    "            # Append the result to the main list\n",
    "            list_qualifier_data.extend(list_tuples_with_keys)\n",
    "        \n",
    "        list_clean_qualifier_data = self.remove_duplicates_keep_order(list_qualifier_data)\n",
    "        \n",
    "        self.bulk_insert_using_copy(table_name = \"GLEIF_relationship_qualifiers\" , data = list_clean_qualifier_data , \n",
    "                                    columns = [\n",
    "                                        'StartNode',\n",
    "                                        'EndNode',\n",
    "                                        'RelationshipType',\n",
    "                                        \"QualifierDimension\",                \n",
    "                                        \"QualifierCategory\"\n",
    "                                    ])\n",
    "    \n",
    "    def process_relationships_quantifiers(self , dict_relationships):\n",
    "        list_quantifier_data = []\n",
    "            \n",
    "        for dict_relationship in dict_relationships:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "            list_tuples_quantifiers = self.obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"Relationship_RelationshipQuantifiers\" , target_keys = [\"MeasurementMethod\" , \"QuantifierAmount\" , \"QuantifierUnits\"])\n",
    "            \n",
    "            list_unique_keys = self.obj_backfill_helpers.get_target_values(dict_data = dict_flat, subset_string = True, target_keys = [\"StartNode\" , \"EndNode\" , \"RelationshipType\"])\n",
    "            \n",
    "            list_tuples_with_keys = [(*list_unique_keys, *tup) for tup in list_tuples_quantifiers]\n",
    "            \n",
    "            # Append the result to the main list\n",
    "            list_quantifier_data.extend(list_tuples_with_keys)\n",
    "            \n",
    "        list_clean_quantifier_data = self.remove_duplicates_keep_order(list_quantifier_data)\n",
    "        \n",
    "        self.bulk_insert_using_copy(table_name = \"GLEIF_relationship_quantifiers\" , data = list_clean_quantifier_data , \n",
    "                                    columns = [\n",
    "                                        'StartNode',\n",
    "                                        'EndNode',\n",
    "                                        'RelationshipType',\n",
    "                                        \"MeasurementMethod\",                \n",
    "                                        \"QuantifierAmount\",\n",
    "                                        \"QuantifierUnits\",\n",
    "                                    ])    \n",
    "    \n",
    "    def process_relationships(self , dict_relationships):\n",
    "\n",
    "        self.process_meta_data(dict_relationships = dict_relationships)\n",
    "        \n",
    "        self.process_relationships_date_data(dict_relationships = dict_relationships)\n",
    "        \n",
    "        self.process_relationships_qualifiers(dict_relationships = dict_relationships)\n",
    "        \n",
    "        self.process_relationships_quantifiers(dict_relationships = dict_relationships)\n",
    "        \n",
    "            \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        self.create_table()\n",
    "        \n",
    "        with open(self.str_json_file_path, 'r', encoding='utf-8') as file:\n",
    "            dict_relationships = json.load(file)            \n",
    "            self.process_relationships(dict_relationships = dict_relationships[\"relations\"])               \n",
    "        \n",
    "        self.obj_backfill_helpers.file_tracker(file_path = self.str_json_file_path , str_db_name = \"GLEIF_test_db\")\n",
    "        \n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = GLEIFLevel2Data(bool_log = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../file_lib/Level_2_unpacked//20241224-0000-gleif-goldencopy-rr-golden-copy.json'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(obj.str_json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers()\n",
    "obj_backfill_helpers.file_tracker(file_path = obj.str_json_file_path , str_db_name = \"GLEIF_test_db\" , str_data_title = \"Level_2_Relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_table_names = [\"GLEIF_relationship_data\" , \"GLEIF_relationship_date_data\" , \"GLEIF_relationship_qualifiers\" , \"GLEIF_relationship_quantifiers\"]\n",
    "obj.drop_table(lst_table_names = list_table_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.storing_GLEIF_data_in_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.str_json_file_path\n",
    "obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_2_Trees = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(obj.str_json_file_path, 'r', encoding='utf-8') as file:\n",
    "    dict_relationships = json.load(file)  \n",
    "    \n",
    "def process_relationships(dict_relationships):\n",
    "    list_tuples_relationships = []\n",
    "\n",
    "    for dict_relationship in dict_relationships:\n",
    "        dict_relationship_flattened = obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "        list_output = obj_backfill_helpers.get_target_values(dict_data = dict_relationship_flattened, subset_string = True, target_keys = [\"StartNode\" , \"EndNode\" , \"RelationshipType\" , \"RelationshipStatus\" , \"RegistrationStatus\" , \"InitialRegistrationDate\" , \"LastUpdateDate\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationDocuments\" , \"ValidationRegistration\"])\n",
    "        list_tuples_relationships.append(tuple(list_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relationships_date_data(dict_relationships):\n",
    "    list_relationship_date_data = []\n",
    "    \n",
    "    for index, dict_relationship in enumerate(dict_relationships):\n",
    "        dict_flat = obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "        list_tuples_dates = obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"RelationshipPeriods_RelationshipPeriod\" , target_keys = [\"StartDate\" , \"EndDate\" , \"PeriodType\"])\n",
    "        \n",
    "        list_tuples_with_index = [(index + 1, *tup) for tup in list_tuples_dates]\n",
    "        \n",
    "        # Append the result to the main list\n",
    "        list_relationship_date_data.extend(list_tuples_with_index)\n",
    "        \n",
    "    return list_relationship_date_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relationships_qualifiers(dict_relationships):\n",
    "    list_qualifier_data = []\n",
    "        \n",
    "    for index, dict_relationship in enumerate(dict_relationships):\n",
    "        dict_flat = obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "        list_tuples_qualifiers = obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"RelationshipQualifiers_RelationshipQualifier\" , target_keys = [\"QualifierDimension\" , \"QualifierCategory\"])\n",
    "        \n",
    "        list_tuples_with_index = [(index + 1, *tup) for tup in list_tuples_qualifiers]\n",
    "        \n",
    "        # Append the result to the main list\n",
    "        list_qualifier_data.extend(list_tuples_with_index)\n",
    "        \n",
    "    return list_qualifier_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relationships_quatifiers(dict_relationships):\n",
    "    list_quantifier_data = []\n",
    "        \n",
    "    for index, dict_relationship in enumerate(dict_relationships):\n",
    "        dict_flat = obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "        list_tuples_quantifiers = obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"Relationship_RelationshipQuantifiers\" , target_keys = [\"MeasurementMethod\" , \"QuantifierAmount\" , \"QuantifierUnits\"])\n",
    "        \n",
    "        list_tuples_with_index = [(index + 1, *tup) for tup in list_tuples_quantifiers]\n",
    "        \n",
    "        # Append the result to the main list\n",
    "        list_quantifier_data.extend(list_tuples_with_index)\n",
    "        \n",
    "    return list_quantifier_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(obj_backfill_helpers.flatten_dict(dict_relationships[\"relations\"][101108]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hex(value):\n",
    "    if value:\n",
    "        print(\"Hexadecimal representation:\", \" \".join(f\"{ord(c):02x}\" for c in value))\n",
    "    else:\n",
    "        print(\"Value is None or empty.\")\n",
    "\n",
    "log_hex(\"C:\\\\Users\\\\A0006041\\\\OneDrive - Allianz\\\\Shared Documents - AZCZ-Investice_&_Treasury\\\\09_BACK_OFFICE\\\\00_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(value):\n",
    "    if value:\n",
    "        # Remove null bytes and any control characters\n",
    "        return ''.join(c for c in value if c.isprintable()).replace('\\x00', '').strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = clean_string(\"C:\\\\Users\\\\A0006041\\\\OneDrive - Allianz\\\\Shared Documents - AZCZ-Investice_&_Treasury\\\\09_BACK_OFFICE\\\\00_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(value):\n",
    "    if value:\n",
    "        # Replace specific problematic sequences\n",
    "        value = value.replace('\\\\00', '').replace('\\\\09', '')\n",
    "        # Replace backslashes with forward slashes or keep as needed\n",
    "        value = value.replace('\\\\', '/')\n",
    "        return value.strip()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(self, list_input):\n",
    "    \"\"\"Clean the ValidationReference field (index 11).\"\"\"\n",
    "    if list_input[11]:\n",
    "        original_value = list_input[11]\n",
    "        list_input[11] = clean_string(list_input[11])\n",
    "        logging.debug(f\"Original: {original_value}, Cleaned: {list_input[11]}\")\n",
    "    else:\n",
    "        list_input[11] = None\n",
    "    return list_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(clean_string(value = \"C:\\\\Users\\\\A0006041\\\\OneDrive - Allianz\\\\Shared Documents - AZCZ-Investice_&_Treasury\\\\09_BACK_OFFICE\\\\00_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
