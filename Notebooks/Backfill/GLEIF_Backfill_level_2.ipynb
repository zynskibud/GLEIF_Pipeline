{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import logging\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import zipfile\n",
    "import json\n",
    "import json\n",
    "import psycopg2\n",
    "import io\n",
    "\n",
    "class GLEIF_Backill_Helpers:\n",
    "    def __init__(self, bool_Level_1 = False, bool_Level_2_Trees = False, bool_Level_2_Reporting_Exceptions = False):\n",
    "        self.bool_Level_1 = bool_Level_1\n",
    "        self.bool_Level_2_Trees = bool_Level_2_Trees\n",
    "        self.bool_Level_2_Reporting_Exceptions = bool_Level_2_Reporting_Exceptions\n",
    "        \n",
    "\n",
    "    def get_level_download_links(self):\n",
    "        \"\"\"\n",
    "        This function uses selenium to webscrape the download link for all Level 1 Data in the GLEIF database.\n",
    "        \n",
    "        @return: str_download_link - the link which is used to download the entire GLEIF level 1\n",
    "        \"\"\"\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(url = \"https://www.gleif.org/en/lei-data/gleif-golden-copy/download-the-golden-copy#/\")\n",
    "\n",
    "        cookie_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, 'CybotCookiebotDialogBodyButton'))\n",
    "        )\n",
    "\n",
    "        cookie_button.click()\n",
    "\n",
    "        download_buttons = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'gc-download-button'))\n",
    "        )\n",
    "        \n",
    "        if self.bool_Level_1 == True:\n",
    "            download_buttons[0].click()\n",
    "        if self.bool_Level_2_Trees == True:\n",
    "            download_buttons[1].click()\n",
    "        if self.bool_Level_2_Reporting_Exceptions == True:\n",
    "            download_buttons[2].click()\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        str_download_link = ((soup.find_all(\"a\" , class_ = \"gc-icon gc-icon--json\"))[0])[\"href\"]\n",
    "        \n",
    "        return str_download_link        \n",
    "    \n",
    "    def unpacking_GLEIF_zip_files(self , str_download_link , str_zip_file_path_name , str_unpacked_zip_file_path_name):\n",
    "        session = requests.Session()\n",
    "        zip_file = session.get(url = str_download_link)\n",
    "\n",
    "        with open(rf\"../file_lib/{str_zip_file_path_name}\", 'wb') as f:\n",
    "            f.write(zip_file.content)\n",
    "\n",
    "        with zipfile.ZipFile(rf\"../file_lib/{str_zip_file_path_name}\", 'r') as zip_ref:\n",
    "            os.makedirs(rf\"../file_lib/{str_unpacked_zip_file_path_name}\", exist_ok=True)\n",
    "            zip_ref.extractall(rf\"../file_lib/{str_unpacked_zip_file_path_name}\")\n",
    "        \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/{str_unpacked_zip_file_path_name}\")[0]\n",
    "        str_json_file_path = rf\"../file_lib/{str_unpacked_zip_file_path_name}\" + \"//\" + str_unpacked_zip_file_name\n",
    "        \n",
    "        return str_json_file_path\n",
    "    \n",
    "    def flatten_dict(self , dict_input):\n",
    "        dict_flattened = {}\n",
    "\n",
    "        def flatten(current_element, parent_key=''):\n",
    "            if isinstance(current_element, dict):\n",
    "                for key, value in current_element.items():\n",
    "                    new_key = f\"{parent_key}_{key}\" if parent_key else key\n",
    "                    flatten(value, new_key)\n",
    "            elif isinstance(current_element, list):\n",
    "                for index, item in enumerate(current_element, start=1):\n",
    "                    indexed_key = f\"{parent_key}_{index}\"\n",
    "                    flatten(item, indexed_key)\n",
    "            else:\n",
    "                dict_flattened[parent_key] = current_element\n",
    "\n",
    "        flatten(dict_input)\n",
    "        return dict_flattened\n",
    "    \n",
    "    def clean_keys(self , input_dict):\n",
    "        cleaned_dict = {}\n",
    "        \n",
    "        for key, value in input_dict.items():\n",
    "            if not '@xml:lang' in key:\n",
    "                if key.endswith('_$'):\n",
    "                    new_key = key[:-2]  # Remove the last 2 characters ('_$')\n",
    "                else:\n",
    "                    new_key = key  # Keep the key as is\n",
    "                \n",
    "                cleaned_dict[new_key] = value\n",
    "        \n",
    "        return cleaned_dict\n",
    "    \n",
    "    def organize_by_prefix(self , input_dict):\n",
    "        dict_organized = {}\n",
    "        \n",
    "        for key, value in input_dict.items():\n",
    "            prefix, _, sub_key = key.partition('_')\n",
    "            \n",
    "            if prefix not in dict_organized:\n",
    "                dict_organized[prefix] = {}\n",
    "            \n",
    "            # Add the key-value pair to the corresponding sub-dictionary\n",
    "            dict_organized[prefix][sub_key] = value\n",
    "        \n",
    "        return dict_organized\n",
    "\n",
    "    def extract_other_entity_names(self , data_dict, base_keyword, exclude_keywords=None):\n",
    "        \"\"\"\n",
    "        Extracts and organizes `OtherEntityNames` keys into a list of tuples.\n",
    "        Each tuple contains the `@type` and the main value for a given numeric suffix.\n",
    "\n",
    "        :param data_dict: Dictionary containing the raw data.\n",
    "        :param base_keyword: Common substring to identify relevant keys (e.g., \"OtherEntityNames\").\n",
    "        :param exclude_keywords: List of keywords to exclude (e.g., [\"TranslatedOtherEntityNames\"]).\n",
    "        :return: A list of tuples: (type, value) for each numeric suffix group.\n",
    "        \"\"\"\n",
    "        grouped_data = {}\n",
    "\n",
    "        # Default empty list for exclude_keywords\n",
    "        if exclude_keywords is None:\n",
    "            exclude_keywords = []\n",
    "\n",
    "        # Group keys by numeric suffix\n",
    "        for key, value in data_dict.items():\n",
    "            # Skip keys with excluded keywords\n",
    "            if any(exclude in key for exclude in exclude_keywords):\n",
    "                continue\n",
    "\n",
    "            if base_keyword in key:\n",
    "                # Extract the numeric suffix using regex\n",
    "                match = re.search(r\"_(\\d+)\", key)\n",
    "                if not match:\n",
    "                    continue  # Skip keys without a numeric suffix\n",
    "                index = int(match.group(1))\n",
    "\n",
    "                if index not in grouped_data:\n",
    "                    grouped_data[index] = {\"type\": None, \"value\": None}\n",
    "\n",
    "                # Check if this key is `@type` or the main value\n",
    "                if \"@type\" in key:\n",
    "                    grouped_data[index][\"type\"] = value\n",
    "                else:\n",
    "                    grouped_data[index][\"value\"] = value\n",
    "\n",
    "        # Convert grouped data into a list of tuples\n",
    "        result = [(grouped_data[index][\"type\"], grouped_data[index][\"value\"]) for index in sorted(grouped_data.keys())]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def extract_event_data(self , dict_data, base_keyword, target_keys):\n",
    "        \"\"\"\n",
    "        Extracts and organizes data for repeated keys in a dictionary based on a base keyword and target keys.\n",
    "\n",
    "        :param dict_data: Dictionary containing the raw data.\n",
    "        :param base_keyword: Common substring to identify relevant keys (e.g., \"LegalEntityEvents\").\n",
    "        :param target_keys: List of substrings to match keys that should be included in the tuple.\n",
    "        :return: A list of tuples, one for each numeric suffix group, containing values for the target keys.\n",
    "        \"\"\"\n",
    "        import re\n",
    "\n",
    "        grouped_data = {}\n",
    "\n",
    "        # Group keys by numeric suffix\n",
    "        for key, value in dict_data.items():\n",
    "            if base_keyword in key:\n",
    "                # Extract the numeric suffix using regex\n",
    "                match = re.search(r\"_(\\d+)_\", key)\n",
    "                if match:\n",
    "                    index = int(match.group(1))\n",
    "\n",
    "                    # Extract the part of the key after the numeric suffix\n",
    "                    key_suffix = key.split(f\"_{index}_\")[-1]\n",
    "\n",
    "                    if index not in grouped_data:\n",
    "                        grouped_data[index] = {}\n",
    "                    \n",
    "                    # Check if this key matches any of the target keys as a substring\n",
    "                    for target in target_keys:\n",
    "                        if target in key_suffix:\n",
    "                            grouped_data[index][target] = value\n",
    "                            break\n",
    "                else:\n",
    "                    # Handle unnumbered keys (keys without numeric suffix)\n",
    "                    key_suffix = key.split(base_keyword + \"_\")[-1]  # Extract key after base_keyword\n",
    "                    for target in target_keys:\n",
    "                        if target in key_suffix:\n",
    "                            # Add to group 0 (special group for unnumbered keys)\n",
    "                            if 0 not in grouped_data:\n",
    "                                grouped_data[0] = {}\n",
    "                            grouped_data[0][target] = value\n",
    "                            break\n",
    "\n",
    "        # Create tuples for each group of keys\n",
    "        result = []\n",
    "        for index in sorted(grouped_data.keys()):\n",
    "            # Create a tuple of values for the target keys, using None if a key is missing\n",
    "            tuple_values = tuple(grouped_data[index].get(target, None) for target in target_keys)\n",
    "            result.append(tuple_values)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_target_values(self , dict_data, target_keys, subset_string=False):\n",
    "        \"\"\"\n",
    "        Retrieves values for a set of target keys from a dictionary.\n",
    "        If a key is not present, it returns None for that key.\n",
    "        Optionally allows searching for subsets of key strings.\n",
    "\n",
    "        :param dict_data: Dictionary containing the raw data.\n",
    "        :param target_keys: List of keys (or substrings) to retrieve values for.\n",
    "        :param subset_string: Boolean indicating whether to search for subsets of key strings.\n",
    "        :return: A tuple of values corresponding to the target keys (or None if a key is missing).\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for target in target_keys:\n",
    "            if subset_string:\n",
    "                # Search for a key containing the target substring\n",
    "                found_key = next((key for key in dict_data if target in key), None)\n",
    "                result.append(dict_data.get(found_key, None))\n",
    "            else:\n",
    "                # Direct key lookup\n",
    "                result.append(dict_data.get(target, None))\n",
    "        return list(result)\n",
    "\n",
    "    def split_into_list_of_dictionaries(self , dict_data):\n",
    "        \"\"\"\n",
    "        Splits a dictionary into a list of dictionaries, grouped by numeric suffixes.\n",
    "        Keys without numeric suffixes are excluded.\n",
    "\n",
    "        :param data_dict: Dictionary containing the keys and values.\n",
    "        :return: A list of dictionaries, one for each numeric suffix.\n",
    "        \"\"\"\n",
    "        grouped_dicts = {}\n",
    "\n",
    "        # Iterate through the dictionary and group by numeric suffix\n",
    "        for key, value in dict_data.items():\n",
    "            match = re.search(r\"_(\\d+)_\", key)\n",
    "            if match:\n",
    "                # Extract the numeric suffix\n",
    "                group_number = int(match.group(1))\n",
    "                if group_number not in grouped_dicts:\n",
    "                    grouped_dicts[group_number] = {}\n",
    "                grouped_dicts[group_number][key] = value\n",
    "\n",
    "        # Convert the grouped dictionaries to a list\n",
    "        return [grouped_dicts[group] for group in sorted(grouped_dicts.keys())]\n",
    "\n",
    "\n",
    "    def further_flatten_geocoding(self , dict_data):\n",
    "        \"\"\"\n",
    "        Flattens a dictionary by extracting bounding box values and adding them as separate keys.\n",
    "        \n",
    "        :param data_dict: Dictionary containing geocoding data with bounding box values.\n",
    "        :return: A new dictionary with flattened bounding box values.\n",
    "        \"\"\"\n",
    "        flattened_dict = {}\n",
    "\n",
    "        for key, value in dict_data.items():\n",
    "            # Check for bounding box keys and split their values\n",
    "            if \"bounding_box\" in key:\n",
    "                # Extract numeric suffix if present\n",
    "                match = re.search(r\"_(\\d+)_\", key)\n",
    "                group_number = f\"_{match.group(1)}_\" if match else \"_\"\n",
    "\n",
    "                # Parse the bounding box values\n",
    "                bounding_box_values = value.split(\", \")\n",
    "                for box_value in bounding_box_values:\n",
    "                    # Split key-value pair (e.g., \"TopLeft.Latitude: 39.7496542\")\n",
    "                    box_key, box_val = box_value.split(\": \")\n",
    "                    # Create a new key with group number\n",
    "                    new_key = f\"{key.split('bounding_box')[0]}{box_key.strip()}{group_number}\".strip(\"_\")\n",
    "                    flattened_dict[new_key] = box_val.strip()\n",
    "            else:\n",
    "                # If not bounding box, keep the original key-value pair\n",
    "                flattened_dict[key] = value\n",
    "\n",
    "        return flattened_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel2Data:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_db\" , bool_downloaded = True):\n",
    "        self.obj_backfill_helpers = GLEIF_Backill_Helpers(bool_Level_2_Trees = True)\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Backfill_level_2.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            str_level_2_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "            self.str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_2_download_link , str_unpacked_zip_file_path_name = \"Level_2_unpacked\" , str_zip_file_path_name = \"Level_2.zip\")\n",
    "    \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/Level_2_unpacked\")[0]\n",
    "        self.str_json_file_path = rf\"../file_lib/Level_2_unpacked\" + \"//\" + str_unpacked_zip_file_name\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def create_table(self):\n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_relationship_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                StartNode TEXT,                \n",
    "                EndNode TEXT,\n",
    "                RelationshipType TEXT,\n",
    "                RelationshipStatus TEXT,\n",
    "                RegistrationStatus TEXT,\n",
    "                InitialRegistrationDate TEXT,\n",
    "                LastUpdateDate TEXT,\n",
    "                NextRenewalDate TEXT,\n",
    "                ManagingLOU TEXT,\n",
    "                ValidationSources TEXT,\n",
    "                ValidationDocuments TEXT,\n",
    "                ValidationReference TEXT\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_relationship_date_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                relationship_id INTEGER NOT NULL,\n",
    "                StartDate TEXT,\n",
    "                EndDate TEXT,\n",
    "                PeriodType TEXT,\n",
    "                FOREIGN KEY (relationship_id) REFERENCES GLEIF_relationship_data(id)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_relationship_qualifiers (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                relationship_id INTEGER NOT NULL,\n",
    "                QualifierDimension TEXT,                \n",
    "                QualifierCategory TEXT,\n",
    "                FOREIGN KEY (relationship_id) REFERENCES GLEIF_relationship_data(id)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_relationship_quantifiers (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                relationship_id INTEGER NOT NULL,\n",
    "                MeasurementMethod TEXT,                \n",
    "                QuantifierAmount TEXT,\n",
    "                QuantifierUnits TEXT,\n",
    "                FOREIGN KEY (relationship_id) REFERENCES GLEIF_relationship_data(id)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.conn.commit()\n",
    "    \n",
    "    def bulk_insert_using_copy(self , table_name , columns, data):\n",
    "        \"\"\"Perform a bulk insert using PostgreSQL COPY with an in-memory buffer\n",
    "\n",
    "        Args:\n",
    "            table_name (_type_): Name of the table to insert into\n",
    "            columns (_type_): List of column names for the table\n",
    "            data (_type_): List of tuples with the data to be inserted\n",
    "        \"\"\"\n",
    "        \n",
    "        buffer = io.StringIO()\n",
    "        \n",
    "        #write data to the buffer\n",
    "        \n",
    "        for row in data:\n",
    "            buffer.write('\\t'.join(map(str , row)) + \"\\n\")\n",
    "        buffer.seek(0) #reset buffer position to the beginning\n",
    "        \n",
    "        #Construct the copy query\n",
    "        copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH DELIMITER '\\t'\"\n",
    "        self.cursor.copy_expert(copy_query , buffer)\n",
    "        self.conn.commit\n",
    "    \n",
    "    \n",
    "    def clean_string(self , value):\n",
    "        if value:\n",
    "            # Replace specific problematic sequences\n",
    "            value = value.replace('\\\\00', '').replace('\\\\09', '')\n",
    "            # Replace backslashes with forward slashes or keep as needed\n",
    "            value = value.replace('\\\\', '/')\n",
    "            return value.strip()\n",
    "        return None\n",
    "        \n",
    "    def clean_url(self, list_input):\n",
    "        \"\"\"Clean the ValidationReference field (index 11).\"\"\"\n",
    "        if list_input[11]:\n",
    "            original_value = list_input[11]\n",
    "            list_input[11] = self.clean_string(list_input[11])\n",
    "            logging.debug(f\"Original: {original_value}, Cleaned: {list_input[11]}\")\n",
    "        else:\n",
    "            list_input[11] = None\n",
    "        return list_input\n",
    "        \n",
    "    def process_meta_data(self , dict_relationships):\n",
    "        list_tuples_relationships = []\n",
    "\n",
    "        for dict_relationship in dict_relationships:\n",
    "            dict_relationship_flattened = self.obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_relationship_flattened, subset_string = True, target_keys = [\"StartNode\" , \"EndNode\" , \"RelationshipType\" , \"RelationshipStatus\" , \"RegistrationStatus\" , \"InitialRegistrationDate\" , \"LastUpdateDate\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationDocuments\" , \"ValidationReference\"])\n",
    "            list_clean_output = self.clean_url(list_input = list_output)\n",
    "            list_tuples_relationships.append(tuple(list_clean_output))\n",
    "        \n",
    "        return list_tuples_relationships\n",
    "    \n",
    "    def process_relationships_date_data(self , dict_relationships):\n",
    "        list_relationship_date_data = []\n",
    "        \n",
    "        for index, dict_relationship in enumerate(dict_relationships):\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "            list_tuples_dates = self.obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"RelationshipPeriods_RelationshipPeriod\" , target_keys = [\"StartDate\" , \"EndDate\" , \"PeriodType\"])\n",
    "            \n",
    "            list_tuples_with_index = [(index + 1, *tup) for tup in list_tuples_dates]\n",
    "            \n",
    "            # Append the result to the main list\n",
    "            list_relationship_date_data.extend(list_tuples_with_index)\n",
    "            \n",
    "        return list_relationship_date_data\n",
    "        \n",
    "    def process_relationships_qualifiers(self , dict_relationships):\n",
    "        list_qualifier_data = []\n",
    "            \n",
    "        for index, dict_relationship in enumerate(dict_relationships):\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "            list_tuples_qualifiers = self.obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"RelationshipQualifiers_RelationshipQualifier\" , target_keys = [\"QualifierDimension\" , \"QualifierCategory\"])\n",
    "            \n",
    "            list_tuples_with_index = [(index + 1, *tup) for tup in list_tuples_qualifiers]\n",
    "            \n",
    "            # Append the result to the main list\n",
    "            list_qualifier_data.extend(list_tuples_with_index)\n",
    "        \n",
    "        return list_qualifier_data\n",
    "    \n",
    "    def process_relationships_quantifiers(self , dict_relationships):\n",
    "        list_quantifier_data = []\n",
    "            \n",
    "        for index, dict_relationship in enumerate(dict_relationships):\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "            list_tuples_quantifiers = self.obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"Relationship_RelationshipQuantifiers\" , target_keys = [\"MeasurementMethod\" , \"QuantifierAmount\" , \"QuantifierUnits\"])\n",
    "            \n",
    "            list_tuples_with_index = [(index + 1, *tup) for tup in list_tuples_quantifiers]\n",
    "            \n",
    "            # Append the result to the main list\n",
    "            list_quantifier_data.extend(list_tuples_with_index)\n",
    "            \n",
    "        return list_quantifier_data\n",
    "    \n",
    "    \n",
    "    def process_relationships(self , dict_relationships):\n",
    "\n",
    "        list_tuples_relationship_meta_data = self.process_meta_data(dict_relationships = dict_relationships)\n",
    "        self.bulk_insert_using_copy(table_name = \"GLEIF_relationship_data\" , \n",
    "                                    data = list_tuples_relationship_meta_data , \n",
    "                                    columns = [\n",
    "                                                'StartNode',\n",
    "                                                'EndNode',\n",
    "                                                'RelationshipType',\n",
    "                                                'RelationshipStatus',\n",
    "                                                'RegistrationStatus',\n",
    "                                                'InitialRegistrationDate',\n",
    "                                                'LastUpdateDate',\n",
    "                                                'NextRenewalDate',\n",
    "                                                'ManagingLOU',\n",
    "                                                'ValidationSources',\n",
    "                                                'ValidationDocuments',\n",
    "                                                'ValidationReference'\n",
    "                                            ])  \n",
    "        list_relationship_date_data = self.process_relationships_date_data(dict_relationships = dict_relationships)\n",
    "        self.bulk_insert_using_copy(table_name = \"GLEIF_relationship_date_data\" , \n",
    "                                    data = list_relationship_date_data , \n",
    "                                    columns = [\n",
    "                                        \"relationship_id\", \n",
    "                                        \"StartDate\",\n",
    "                                        \"EndDate\",\n",
    "                                        \"PeriodType\"])\n",
    "        list_qualifier_data = self.process_relationships_qualifiers(dict_relationships = dict_relationships)\n",
    "        self.bulk_insert_using_copy(table_name = \"GLEIF_relationship_qualifiers\" , data = list_qualifier_data , \n",
    "                                    columns = [\n",
    "                                        \"relationship_id\",\n",
    "                                        \"QualifierDimension\",                \n",
    "                                        \"QualifierCategory\"\n",
    "                                    ])\n",
    "        list_quantifier_data = self.process_relationships_quantifiers(dict_relationships = dict_relationships)\n",
    "        self.bulk_insert_using_copy(table_name = \"GLEIF_relationship_quantifiers\" , data = list_quantifier_data , \n",
    "                                    columns = [\n",
    "                                        \"relationship_id\",\n",
    "                                        \"MeasurementMethod\",                \n",
    "                                        \"QuantifierAmount\",\n",
    "                                        \"QuantifierUnits\",\n",
    "                                    ])\n",
    "            \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        self.create_table()\n",
    "        \n",
    "        with open(self.str_json_file_path, 'r', encoding='utf-8') as file:\n",
    "            dict_relationships = json.load(file)            \n",
    "            self.process_relationships(dict_relationships = dict_relationships[\"relations\"])               \n",
    "        \n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = GLEIFLevel2Data(bool_log = True)\n",
    "obj.storing_GLEIF_data_in_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.str_json_file_path\n",
    "obj_backfill_helpers = GLEIF_Backill_Helpers(bool_Level_2_Trees = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(obj.str_json_file_path, 'r', encoding='utf-8') as file:\n",
    "    dict_relationships = json.load(file)  \n",
    "    \n",
    "def process_relationships(dict_relationships):\n",
    "    list_tuples_relationships = []\n",
    "\n",
    "    for dict_relationship in dict_relationships:\n",
    "        dict_relationship_flattened = obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "        list_output = obj_backfill_helpers.get_target_values(dict_data = dict_relationship_flattened, subset_string = True, target_keys = [\"StartNode\" , \"EndNode\" , \"RelationshipType\" , \"RelationshipStatus\" , \"RegistrationStatus\" , \"InitialRegistrationDate\" , \"LastUpdateDate\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationDocuments\" , \"ValidationRegistration\"])\n",
    "        list_tuples_relationships.append(tuple(list_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relationships_date_data(dict_relationships):\n",
    "    list_relationship_date_data = []\n",
    "    \n",
    "    for index, dict_relationship in enumerate(dict_relationships):\n",
    "        dict_flat = obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "        list_tuples_dates = obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"RelationshipPeriods_RelationshipPeriod\" , target_keys = [\"StartDate\" , \"EndDate\" , \"PeriodType\"])\n",
    "        \n",
    "        list_tuples_with_index = [(index + 1, *tup) for tup in list_tuples_dates]\n",
    "        \n",
    "        # Append the result to the main list\n",
    "        list_relationship_date_data.extend(list_tuples_with_index)\n",
    "        \n",
    "    return list_relationship_date_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relationships_qualifiers(dict_relationships):\n",
    "    list_qualifier_data = []\n",
    "        \n",
    "    for index, dict_relationship in enumerate(dict_relationships):\n",
    "        dict_flat = obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "        list_tuples_qualifiers = obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"RelationshipQualifiers_RelationshipQualifier\" , target_keys = [\"QualifierDimension\" , \"QualifierCategory\"])\n",
    "        \n",
    "        list_tuples_with_index = [(index + 1, *tup) for tup in list_tuples_qualifiers]\n",
    "        \n",
    "        # Append the result to the main list\n",
    "        list_qualifier_data.extend(list_tuples_with_index)\n",
    "        \n",
    "    return list_qualifier_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_relationships_quatifiers(dict_relationships):\n",
    "    list_quantifier_data = []\n",
    "        \n",
    "    for index, dict_relationship in enumerate(dict_relationships):\n",
    "        dict_flat = obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "        list_tuples_quantifiers = obj_backfill_helpers.extract_event_data(dict_data = dict_flat , base_keyword = \"Relationship_RelationshipQuantifiers\" , target_keys = [\"MeasurementMethod\" , \"QuantifierAmount\" , \"QuantifierUnits\"])\n",
    "        \n",
    "        list_tuples_with_index = [(index + 1, *tup) for tup in list_tuples_quantifiers]\n",
    "        \n",
    "        # Append the result to the main list\n",
    "        list_quantifier_data.extend(list_tuples_with_index)\n",
    "        \n",
    "    return list_quantifier_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(obj_backfill_helpers.flatten_dict(dict_relationships[\"relations\"][101108]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_hex(value):\n",
    "    if value:\n",
    "        print(\"Hexadecimal representation:\", \" \".join(f\"{ord(c):02x}\" for c in value))\n",
    "    else:\n",
    "        print(\"Value is None or empty.\")\n",
    "\n",
    "log_hex(\"C:\\\\Users\\\\A0006041\\\\OneDrive - Allianz\\\\Shared Documents - AZCZ-Investice_&_Treasury\\\\09_BACK_OFFICE\\\\00_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(value):\n",
    "    if value:\n",
    "        # Remove null bytes and any control characters\n",
    "        return ''.join(c for c in value if c.isprintable()).replace('\\x00', '').strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = clean_string(\"C:\\\\Users\\\\A0006041\\\\OneDrive - Allianz\\\\Shared Documents - AZCZ-Investice_&_Treasury\\\\09_BACK_OFFICE\\\\00_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_string(value):\n",
    "    if value:\n",
    "        # Replace specific problematic sequences\n",
    "        value = value.replace('\\\\00', '').replace('\\\\09', '')\n",
    "        # Replace backslashes with forward slashes or keep as needed\n",
    "        value = value.replace('\\\\', '/')\n",
    "        return value.strip()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_url(self, list_input):\n",
    "    \"\"\"Clean the ValidationReference field (index 11).\"\"\"\n",
    "    if list_input[11]:\n",
    "        original_value = list_input[11]\n",
    "        list_input[11] = clean_string(list_input[11])\n",
    "        logging.debug(f\"Original: {original_value}, Cleaned: {list_input[11]}\")\n",
    "    else:\n",
    "        list_input[11] = None\n",
    "    return list_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(clean_string(value = \"C:\\\\Users\\\\A0006041\\\\OneDrive - Allianz\\\\Shared Documents - AZCZ-Investice_&_Treasury\\\\09_BACK_OFFICE\\\\00_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
