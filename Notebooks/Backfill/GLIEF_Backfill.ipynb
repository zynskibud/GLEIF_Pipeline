{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the jupyter notebook pertaining to the GLEIF_Backfill.py file.\n",
    "\n",
    "This code is responsible for obtaining all Level 1 and Level 2 Relationship data from the GLIEF, and backfilling the data and adding whatever features are needed to make the data accessible for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mattp\\\\Work_Related\\\\Gradient_Trading\\\\GLEIF_Pipeline\\\\GLEIF_Pipeline\\\\Notebooks\\\\Baselines'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mattp\\\\Work_Related\\\\Gradient_Trading\\\\GLEIF_Pipeline\\\\GLEIF_Pipeline\\\\Notebooks\\\\Baselines'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'Infastructure'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m display(target_directory)\n\u001b[0;32m     22\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(target_directory)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mInfastructure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m System_Helpers\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'Infastructure'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import bigjson\n",
    "import json\n",
    "import sys\n",
    "current_directory = os.getcwd()\n",
    "display(current_directory)\n",
    "target_directory = os.path.abspath(os.path.join(current_directory))\n",
    "display(target_directory)\n",
    "sys.path.append(target_directory)\n",
    "from Infastructure import System_Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'D_Infastructure'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m target_directory \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_directory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     20\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(target_directory)\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mD_Infastructure\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m System_Helpers\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGLEIF_Backill_Helpers\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, bool_Level_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, bool_Level_2_Trees \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, bool_Level_2_Reporting_Exceptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'D_Infastructure'"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import logging\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import bigjson\n",
    "import json\n",
    "import sys\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "from D_Infastructure import System_Helpers\n",
    "\n",
    "class GLEIF_Backill_Helpers:\n",
    "    def __init__(self, bool_Level_1 = False, bool_Level_2_Trees = False, bool_Level_2_Reporting_Exceptions = False):\n",
    "        self.bool_Level_1 = bool_Level_1\n",
    "        self.bool_Level_2_Trees = bool_Level_2_Trees\n",
    "        self.bool_Level_2_Reporting_Exceptions = bool_Level_2_Reporting_Exceptions\n",
    "        \n",
    "\n",
    "    def get_level_download_links(self):\n",
    "        \"\"\"\n",
    "        This function uses selenium to webscrape the download link for all Level 1 Data in the GLEIF database.\n",
    "        \n",
    "        @return: str_download_link - the link which is used to download the entire GLEIF level 1\n",
    "        \"\"\"\n",
    "\n",
    "        driver_path = (r\"C:\\Drivers\\Google\\chromedriver-win64\\chromedriver-win64\\chromedriver.exe\")\n",
    "        service = Service(driver_path)\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(url = \"https://www.gleif.org/en/lei-data/gleif-golden-copy/download-the-golden-copy#/\")\n",
    "\n",
    "        cookie_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, 'CybotCookiebotDialogBodyButton'))\n",
    "        )\n",
    "\n",
    "        cookie_button.click()\n",
    "\n",
    "        download_buttons = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'gc-download-button'))\n",
    "        )\n",
    "        \n",
    "        if self.bool_Level_1 == True:\n",
    "            download_buttons[0].click()\n",
    "        if self.bool_Level_2_Trees == True:\n",
    "            download_buttons[1].click()\n",
    "        if self.bool_Level_2_Reporting_Exceptions == True:\n",
    "            download_buttons[2].click()\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        str_download_link = ((soup.find_all(\"a\" , class_ = \"gc-icon gc-icon--json\"))[0])[\"href\"]\n",
    "        \n",
    "        return str_download_link        \n",
    "    \n",
    "    def create_sql_instance(self, str_db_name):\n",
    "        # Connect to the SQLite database\n",
    "        conn = sqlite3.connect(f'{str_db_name}.db', timeout=10)\n",
    "        # Create the schema\n",
    "        self.create_database_schema(conn)\n",
    "        return conn\n",
    "        \n",
    "    def create_tables(self, conn):\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"PRAGMA foreign_keys = ON;\")\n",
    "\n",
    "        schema = \"\"\"\n",
    "        -- Table: entity\n",
    "        CREATE TABLE IF NOT EXISTS entity (\n",
    "            lei TEXT PRIMARY KEY,\n",
    "            legal_name TEXT NOT NULL,\n",
    "            legal_jurisdiction TEXT,\n",
    "            entity_category TEXT,\n",
    "            entity_status TEXT,\n",
    "            entity_creation_date TEXT,\n",
    "            successor_lei TEXT,\n",
    "            successor_entity_name TEXT,\n",
    "            entity_legal_form_code TEXT,\n",
    "            other_legal_form TEXT,\n",
    "            FOREIGN KEY (successor_lei) REFERENCES entity(lei)\n",
    "        );\n",
    "\n",
    "        -- Table: address\n",
    "        CREATE TABLE IF NOT EXISTS address (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            entity_lei TEXT NOT NULL,\n",
    "            address_type TEXT NOT NULL,\n",
    "            first_address_line TEXT,\n",
    "            additional_address_line TEXT,\n",
    "            city TEXT,\n",
    "            country TEXT,\n",
    "            postal_code TEXT,\n",
    "            FOREIGN KEY (entity_lei) REFERENCES entity(lei)\n",
    "        );\n",
    "\n",
    "        -- Table: registration_authority\n",
    "        CREATE TABLE IF NOT EXISTS registration_authority (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            entity_lei TEXT NOT NULL,\n",
    "            registration_authority_id TEXT,\n",
    "            registration_authority_entity_id TEXT,\n",
    "            FOREIGN KEY (entity_lei) REFERENCES entity(lei)\n",
    "        );\n",
    "\n",
    "        -- Table: registration\n",
    "        CREATE TABLE IF NOT EXISTS registration (\n",
    "            entity_lei TEXT PRIMARY KEY,\n",
    "            initial_registration_date TEXT,\n",
    "            last_update_date TEXT,\n",
    "            registration_status TEXT,\n",
    "            next_renewal_date TEXT,\n",
    "            managing_lou TEXT,\n",
    "            validation_sources TEXT,\n",
    "            FOREIGN KEY (entity_lei) REFERENCES entity(lei)\n",
    "        );\n",
    "\n",
    "        -- Table: validation_authority\n",
    "        CREATE TABLE IF NOT EXISTS validation_authority (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            entity_lei TEXT NOT NULL,\n",
    "            is_other_authority INTEGER NOT NULL DEFAULT 0,\n",
    "            validation_authority_id TEXT,\n",
    "            other_validation_authority_id TEXT,\n",
    "            validation_authority_entity_id TEXT,\n",
    "            FOREIGN KEY (entity_lei) REFERENCES entity(lei)\n",
    "        );\n",
    "\n",
    "        -- Table: legal_entity_events\n",
    "        CREATE TABLE IF NOT EXISTS legal_entity_events (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            entity_lei TEXT NOT NULL,\n",
    "            legal_entity_event_type TEXT,\n",
    "            legal_entity_event_effective_date TEXT,\n",
    "            legal_entity_event_recorded_date TEXT,\n",
    "            validation_documents TEXT,\n",
    "            validation_reference TEXT,\n",
    "            affected_fields TEXT,\n",
    "            group_type TEXT,\n",
    "            event_status TEXT,\n",
    "            group_id TEXT,\n",
    "            group_sequence_no INTEGER,\n",
    "            FOREIGN KEY (entity_lei) REFERENCES entity(lei)\n",
    "        );\n",
    "\n",
    "        -- Table: geocoding\n",
    "        CREATE TABLE IF NOT EXISTS geocoding (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            entity_lei TEXT NOT NULL,\n",
    "            original_address TEXT,\n",
    "            geocoding_failed TEXT,\n",
    "            relevance REAL,\n",
    "            lat REAL,\n",
    "            lng REAL,\n",
    "            geocoding_date TEXT,\n",
    "            bounding_box TEXT,\n",
    "            match_level TEXT,\n",
    "            formatted_address TEXT,\n",
    "            mapped_location_id TEXT,\n",
    "            mapped_city TEXT,\n",
    "            mapped_district TEXT,\n",
    "            mapped_country TEXT,\n",
    "            FOREIGN KEY (entity_lei) REFERENCES entity(lei)\n",
    "        );\n",
    "\n",
    "        -- Table: conformity\n",
    "        CREATE TABLE IF NOT EXISTS conformity (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            entity_lei TEXT NOT NULL,\n",
    "            conformity_flag TEXT,\n",
    "            FOREIGN KEY (entity_lei) REFERENCES entity(lei)\n",
    "        );\n",
    "\n",
    "        -- Indexes for Performance Optimization\n",
    "        CREATE INDEX IF NOT EXISTS idx_address_entity_lei ON address(entity_lei);\n",
    "        CREATE INDEX IF NOT EXISTS idx_registration_authority_entity_lei ON registration_authority(entity_lei);\n",
    "        CREATE INDEX IF NOT EXISTS idx_validation_authority_entity_lei ON validation_authority(entity_lei);\n",
    "        CREATE INDEX IF NOT EXISTS idx_legal_entity_events_entity_lei ON legal_entity_events(entity_lei);\n",
    "        CREATE INDEX IF NOT EXISTS idx_geocoding_entity_lei ON geocoding(entity_lei);\n",
    "        CREATE INDEX IF NOT EXISTS idx_conformity_entity_lei ON conformity(entity_lei);\n",
    "        \"\"\"\n",
    "        cursor.executescript(schema)\n",
    "        conn.commit()\n",
    "    \n",
    "    def unpacking_GLEIF_zip_files(self , str_download_link , str_zip_file_path_name , str_unpacked_zip_file_name):\n",
    "        session = requests.Session()\n",
    "        zip_file = session.get(url = str_download_link)\n",
    "\n",
    "        with open(rf\"file_lib\\{str_zip_file_path_name}\", 'wb') as f:\n",
    "            f.write(zip_file.content)\n",
    "\n",
    "        with zipfile.ZipFile(rf\"file_lib\\{str_zip_file_path_name}\", 'r') as zip_ref:\n",
    "            os.makedirs(rf\"file_lib\\{str_unpacked_zip_file_name}\", exist_ok=True)\n",
    "            zip_ref.extractall(rf\"file_lib\\{str_unpacked_zip_file_name}\")\n",
    "        \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"file_lib\\{str_unpacked_zip_file_name}\")[0]\n",
    "        str_json_file_path = rf\"file_lib\\{str_unpacked_zip_file_name}\" + \"\\\\\" + str_unpacked_zip_file_name\n",
    "        \n",
    "        return str_json_file_path\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"file_lib\"):\n",
    "    os.makedirs(\"../file_lib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel1Data:\n",
    "    def __init__(self , bool_log = True , db_name = \"GLEIF_Data.db\" , bool_downloaded = True):\n",
    "        if bool_log:\n",
    "            if not os.path.exists(\"logging\"): #if the logging folder doesnt exist in the directory, make it\n",
    "                os.makedirs(\"../logging\")\n",
    "            logging.basicConfig(filename = \"logging/Company_Facts_Baseline.log\" , level = logging.DEBUG, format = '%(levelname)s: %(message)s' , filemode = \"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "        obj_backfill_helpers = GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "        str_level_1_download_link = obj_backfill_helpers.get_level_download_links()\n",
    "        self.str_json_file_path = obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link)\n",
    "\n",
    "    def helper_parse_entity_data(self):\n",
    "    \n",
    "    \n",
    "    def parse_lei_record(self, record):\n",
    "        # Extract data for the entity table\n",
    "        entity_data = {\n",
    "            'lei': record.get('LEI', {}).get('$'),\n",
    "            'legal_name': record.get('Entity', {}).get('LegalName', {}).get('$'),\n",
    "            'legal_jurisdiction': record.get('Entity', {}).get('LegalJurisdiction', {}).get('$'),\n",
    "            'entity_category': record.get('Entity', {}).get('EntityCategory', {}).get('$'),\n",
    "            'entity_status': record.get('Entity', {}).get('EntityStatus', {}).get('$'),\n",
    "            'entity_creation_date': record.get('Entity', {}).get('EntityCreationDate', {}).get('$'),\n",
    "            'successor_lei': record.get('Entity', {}).get('SuccessorEntity', {}).get('SuccessorLEI', {}).get('$'),\n",
    "            'successor_entity_name': record.get('Entity', {}).get('SuccessorEntity', {}).get('SuccessorName', {}).get('$'),\n",
    "            'entity_legal_form_code': record.get('Entity', {}).get('LegalForm', {}).get('EntityLegalFormCode', {}).get('$'),\n",
    "            'other_legal_form': record.get('Entity', {}).get('LegalForm', {}).get('OtherLegalForm', {}).get('$')\n",
    "        }\n",
    "\n",
    "        # Extract data for the address table\n",
    "        addresses = []\n",
    "        for address_type in ['LegalAddress', 'HeadquartersAddress']:\n",
    "            address_info = record.get('Entity', {}).get(address_type)\n",
    "            if address_info:\n",
    "                additional_address_line = address_info.get('AdditionalAddressLine')\n",
    "                # Handle case where AdditionalAddressLine is a list\n",
    "                if isinstance(additional_address_line, list):\n",
    "                    additional_address_line = ', '.join([\n",
    "                        line.get('$', '') if isinstance(line, dict) else line\n",
    "                        for line in additional_address_line\n",
    "                    ])\n",
    "                elif isinstance(additional_address_line, dict):\n",
    "                    additional_address_line = additional_address_line.get('$', '')\n",
    "                else:\n",
    "                    additional_address_line = additional_address_line\n",
    "\n",
    "                address_data = {\n",
    "                    'entity_lei': entity_data['lei'],\n",
    "                    'address_type': 'LEGAL' if address_type == 'LegalAddress' else 'HEADQUARTERS',\n",
    "                    'first_address_line': address_info.get('FirstAddressLine', {}).get('$'),\n",
    "                    'additional_address_line': additional_address_line,\n",
    "                    'city': address_info.get('City', {}).get('$'),\n",
    "                    'country': address_info.get('Country', {}).get('$'),\n",
    "                    'postal_code': address_info.get('PostalCode', {}).get('$')\n",
    "                }\n",
    "                addresses.append(address_data)\n",
    "\n",
    "        # Extract data for the registration_authority table\n",
    "        registration_authority_info = record.get('Entity', {}).get('RegistrationAuthority')\n",
    "        registration_authority_data = None\n",
    "        if registration_authority_info:\n",
    "            registration_authority_data = {\n",
    "                'entity_lei': entity_data['lei'],\n",
    "                'registration_authority_id': registration_authority_info.get('RegistrationAuthorityID', {}).get('$'),\n",
    "                'registration_authority_entity_id': registration_authority_info.get('RegistrationAuthorityEntityID', {}).get('$')\n",
    "            }\n",
    "\n",
    "        # Extract data for the registration table\n",
    "        registration_info = record.get('Registration', {})\n",
    "        registration_data = {\n",
    "            'entity_lei': entity_data['lei'],\n",
    "            'initial_registration_date': registration_info.get('InitialRegistrationDate', {}).get('$'),\n",
    "            'last_update_date': registration_info.get('LastUpdateDate', {}).get('$'),\n",
    "            'registration_status': registration_info.get('RegistrationStatus', {}).get('$'),\n",
    "            'next_renewal_date': registration_info.get('NextRenewalDate', {}).get('$'),\n",
    "            'managing_lou': registration_info.get('ManagingLOU', {}).get('$'),\n",
    "            'validation_sources': registration_info.get('ValidationSources', {}).get('$')\n",
    "        }\n",
    "\n",
    "        # Extract data for the validation_authority table\n",
    "        validation_authority_info = registration_info.get('ValidationAuthority')\n",
    "        validation_authorities = []\n",
    "        if validation_authority_info:\n",
    "            validation_authority_data = {\n",
    "                'entity_lei': entity_data['lei'],\n",
    "                'is_other_authority': 0,\n",
    "                'validation_authority_id': validation_authority_info.get('ValidationAuthorityID', {}).get('$'),\n",
    "                'other_validation_authority_id': None,\n",
    "                'validation_authority_entity_id': validation_authority_info.get('ValidationAuthorityEntityID', {}).get('$')\n",
    "            }\n",
    "            validation_authorities.append(validation_authority_data)\n",
    "\n",
    "        return {\n",
    "            'entity': entity_data,\n",
    "            'addresses': addresses,\n",
    "            'registration_authority': registration_authority_data,\n",
    "            'registration': registration_data,\n",
    "            'validation_authorities': validation_authorities\n",
    "            # Include other parsed data as needed\n",
    "        }\n",
    "\n",
    "    def insert_entity(self, conn, entity_data):\n",
    "        cursor = conn.cursor()\n",
    "        sql = \"\"\"\n",
    "        INSERT OR REPLACE INTO entity (\n",
    "            lei,\n",
    "            legal_name,\n",
    "            legal_jurisdiction,\n",
    "            entity_category,\n",
    "            entity_status,\n",
    "            entity_creation_date,\n",
    "            successor_lei,\n",
    "            successor_entity_name,\n",
    "            entity_legal_form_code,\n",
    "            other_legal_form\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "        \"\"\"\n",
    "        cursor.execute(sql, (\n",
    "            entity_data['lei'],\n",
    "            entity_data['legal_name'],\n",
    "            entity_data.get('legal_jurisdiction'),\n",
    "            entity_data.get('entity_category'),\n",
    "            entity_data.get('entity_status'),\n",
    "            entity_data.get('entity_creation_date'),\n",
    "            entity_data.get('successor_lei'),\n",
    "            entity_data.get('successor_entity_name'),\n",
    "            entity_data.get('entity_legal_form_code'),\n",
    "            entity_data.get('other_legal_form')\n",
    "        ))\n",
    "        # Do not commit here; commit in the main function to optimize performance\n",
    "\n",
    "    def insert_address(self, conn, address_data):\n",
    "        cursor = conn.cursor()\n",
    "        sql = \"\"\"\n",
    "        INSERT INTO address (\n",
    "            entity_lei,\n",
    "            address_type,\n",
    "            first_address_line,\n",
    "            additional_address_line,\n",
    "            city,\n",
    "            country,\n",
    "            postal_code\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?);\n",
    "        \"\"\"\n",
    "        cursor.execute(sql, (\n",
    "            address_data['entity_lei'],\n",
    "            address_data['address_type'],\n",
    "            address_data.get('first_address_line'),\n",
    "            address_data.get('additional_address_line'),\n",
    "            address_data.get('city'),\n",
    "            address_data.get('country'),\n",
    "            address_data.get('postal_code')\n",
    "        ))\n",
    "        # Do not commit here; commit in the main function to optimize performance\n",
    "\n",
    "    def insert_registration_authority(self, conn, registration_authority_data):\n",
    "        cursor = conn.cursor()\n",
    "        sql = \"\"\"\n",
    "        INSERT INTO registration_authority (\n",
    "            entity_lei,\n",
    "            registration_authority_id,\n",
    "            registration_authority_entity_id\n",
    "        ) VALUES (?, ?, ?);\n",
    "        \"\"\"\n",
    "        cursor.execute(sql, (\n",
    "            registration_authority_data['entity_lei'],\n",
    "            registration_authority_data.get('registration_authority_id'),\n",
    "            registration_authority_data.get('registration_authority_entity_id')\n",
    "        ))\n",
    "        # Do not commit here; commit in the main function to optimize performance\n",
    "\n",
    "    def insert_registration(self, conn, registration_data):\n",
    "        cursor = conn.cursor()\n",
    "        sql = \"\"\"\n",
    "        INSERT OR REPLACE INTO registration (\n",
    "            entity_lei,\n",
    "            initial_registration_date,\n",
    "            last_update_date,\n",
    "            registration_status,\n",
    "            next_renewal_date,\n",
    "            managing_lou,\n",
    "            validation_sources\n",
    "        ) VALUES (?, ?, ?, ?, ?, ?, ?);\n",
    "        \"\"\"\n",
    "        cursor.execute(sql, (\n",
    "            registration_data['entity_lei'],\n",
    "            registration_data.get('initial_registration_date'),\n",
    "            registration_data.get('last_update_date'),\n",
    "            registration_data.get('registration_status'),\n",
    "            registration_data.get('next_renewal_date'),\n",
    "            registration_data.get('managing_lou'),\n",
    "            registration_data.get('validation_sources')\n",
    "        ))\n",
    "        # Do not commit here; commit in the main function to optimize performance\n",
    "\n",
    "    def insert_validation_authority(self, conn, validation_authority_data):\n",
    "        cursor = conn.cursor()\n",
    "        sql = \"\"\"\n",
    "        INSERT INTO validation_authority (\n",
    "            entity_lei,\n",
    "            is_other_authority,\n",
    "            validation_authority_id,\n",
    "            other_validation_authority_id,\n",
    "            validation_authority_entity_id\n",
    "        ) VALUES (?, ?, ?, ?, ?);\n",
    "        \"\"\"\n",
    "        cursor.execute(sql, (\n",
    "            validation_authority_data['entity_lei'],\n",
    "            validation_authority_data.get('is_other_authority', 0),\n",
    "            validation_authority_data.get('validation_authority_id'),\n",
    "            validation_authority_data.get('other_validation_authority_id'),\n",
    "            validation_authority_data.get('validation_authority_entity_id')\n",
    "        ))\n",
    "        # Do not commit here; commit in the main function to optimize performance\n",
    "\n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        # Download and unpack the GLEIF data\n",
    "        str_level_1_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "        str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link, str_zip_file_path=self.str_level_1_zip_file_path, str_unpacked_zip_file_path = self.str_level_1_unpacked_zip_file_path)\n",
    "        # Create database and schema\n",
    "        conn = self.obj_backfill_helpers.create_sql_instance(str_db_name=\"GLEIF_Data\")\n",
    "\n",
    "        with open(str_json_file_path, 'rb') as file:\n",
    "            dict_leis = bigjson.load(file)\n",
    "            # Begin a transaction\n",
    "            conn.execute('BEGIN')\n",
    "            try:\n",
    "                for dict_lei in dict_leis['LEIRecords']:\n",
    "                    record = dict_lei.to_python()\n",
    "                    parsed_data = self.parse_lei_record(record)\n",
    "                    # Insert data into tables\n",
    "                    self.insert_entity(conn, parsed_data['entity'])\n",
    "                    for address in parsed_data['addresses']:\n",
    "                        self.insert_address(conn, address)\n",
    "                    self.insert_registration_authority(conn, parsed_data['registration_authority'])\n",
    "                    self.insert_registration(conn, parsed_data['registration'])\n",
    "                    for validation_authority in parsed_data['validation_authorities']:\n",
    "                        self.insert_validation_authority(conn, validation_authority)\n",
    "                    # Handle other tables similarly\n",
    "                conn.commit()\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                conn.rollback()\n",
    "            finally:\n",
    "                conn.close()\n",
    "        \n",
    "        obj_system_helpers = System_Helpers.SystemHelpers()\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_1_unpacked_zip_file_path , bool_directory = True)\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_1_zip_file_path , bool_file = True)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel2Data:\n",
    "    def __init__(self):\n",
    "        self.str_level_2_unpacked_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_2_Data\\RR_CDF_Data\\Unpacked_Zip\"\n",
    "        self.str_level_2_zip_file_path = r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\Zip_Files\\GLEIF\\Level_2_Data\\RR_CDF_Data\\Level_2_RR_CDF.zip\"\n",
    "        self.obj_backfill_helpers = GLEIF_Backill_Helpers(bool_Level_2_Trees = True)\n",
    "\n",
    "    def insert_json_data(self, json_data , conn , cursor , str_table_name):\n",
    "        cursor.execute(f'''\n",
    "        INSERT INTO {str_table_name}  (data)\n",
    "        VALUES (?)\n",
    "        ''', (json.dumps(json_data),))\n",
    "        conn.commit()\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        str_level_2_download_link = self.obj_data_helpers.get_level_download_links()\n",
    "        str_json_file_path = self.obj_data_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_2_download_link , str_zip_file_path = self.str_level_2_zip_file_path , str_unpacked_zip_file_path = self.str_level_2_unpacked_zip_file_path)\n",
    "        conn, cursor = self.obj_data_helpers.create_sql_instance(str_table_name = \"Level_2_Tree_Data\" , str_db_name = \"GLEIF_Data\")\n",
    "        \n",
    "        with open(str_json_file_path, 'r' , encoding='utf-8') as file:\n",
    "            test = bigjson.load(file)\n",
    "            for dict_lei in test[\"relations\"]:\n",
    "                self.insert_json_data(json_data = dict_lei.to_python() , conn = conn , cursor = cursor , str_table_name = \"Level_2_Tree_Data\")\n",
    "                    \n",
    "            \n",
    "        conn.close()\n",
    "\n",
    "        obj_system_helpers = System_Helpers.SystemHelpers()\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_2_unpacked_zip_file_path , bool_directory = True)\n",
    "        obj_system_helpers.delete_file_directory(str_file_path = self.str_level_2_zip_file_path , bool_file = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_table(old_table_name, new_table_name):\n",
    "    \"\"\"\n",
    "    Renames a table in the SQLite database.\n",
    "\n",
    "    Parameters:\n",
    "        db_name (str): The SQLite database name.\n",
    "        old_table_name (str): The current name of the table.\n",
    "        new_table_name (str): The new name for the table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(r\"C:\\Users\\mattp\\Work_Related\\Systematic_Trading\\Library\\B_Notebooks\\GLIEF_company_data_pipeline\\GLEIF_Data.db\", check_same_thread=False)\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Rename the table\n",
    "        cursor.execute(f\"ALTER TABLE {old_table_name} RENAME TO {new_table_name}\")\n",
    "        conn.commit()\n",
    "\n",
    "        print(f\"Table '{old_table_name}' has been renamed to '{new_table_name}'.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"Error renaming table: {e}\")\n",
    "    finally:\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
