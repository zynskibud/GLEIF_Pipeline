{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import logging\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import bigjson\n",
    "import json\n",
    "import sys\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "from Infrastructure import System_Helpers\n",
    "\n",
    "class GLEIF_Backill_Helpers:\n",
    "    def __init__(self, bool_Level_1 = False, bool_Level_2_Trees = False, bool_Level_2_Reporting_Exceptions = False):\n",
    "        self.bool_Level_1 = bool_Level_1\n",
    "        self.bool_Level_2_Trees = bool_Level_2_Trees\n",
    "        self.bool_Level_2_Reporting_Exceptions = bool_Level_2_Reporting_Exceptions\n",
    "        \n",
    "\n",
    "    def get_level_download_links(self):\n",
    "        \"\"\"\n",
    "        This function uses selenium to webscrape the download link for all Level 1 Data in the GLEIF database.\n",
    "        \n",
    "        @return: str_download_link - the link which is used to download the entire GLEIF level 1\n",
    "        \"\"\"\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(url = \"https://www.gleif.org/en/lei-data/gleif-golden-copy/download-the-golden-copy#/\")\n",
    "\n",
    "        cookie_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, 'CybotCookiebotDialogBodyButton'))\n",
    "        )\n",
    "\n",
    "        cookie_button.click()\n",
    "\n",
    "        download_buttons = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'gc-download-button'))\n",
    "        )\n",
    "        \n",
    "        if self.bool_Level_1 == True:\n",
    "            download_buttons[0].click()\n",
    "        if self.bool_Level_2_Trees == True:\n",
    "            download_buttons[1].click()\n",
    "        if self.bool_Level_2_Reporting_Exceptions == True:\n",
    "            download_buttons[2].click()\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        str_download_link = ((soup.find_all(\"a\" , class_ = \"gc-icon gc-icon--json\"))[0])[\"href\"]\n",
    "        \n",
    "        return str_download_link        \n",
    "    \n",
    "    def unpacking_GLEIF_zip_files(self , str_download_link , str_zip_file_path_name , str_unpacked_zip_file_path_name):\n",
    "        session = requests.Session()\n",
    "        zip_file = session.get(url = str_download_link)\n",
    "\n",
    "        with open(rf\"../file_lib/{str_zip_file_path_name}\", 'wb') as f:\n",
    "            f.write(zip_file.content)\n",
    "\n",
    "        with zipfile.ZipFile(rf\"../file_lib/{str_zip_file_path_name}\", 'r') as zip_ref:\n",
    "            os.makedirs(rf\"../file_lib/{str_unpacked_zip_file_path_name}\", exist_ok=True)\n",
    "            zip_ref.extractall(rf\"../file_lib/{str_unpacked_zip_file_path_name}\")\n",
    "        \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/{str_unpacked_zip_file_path_name}\")[0]\n",
    "        str_json_file_path = rf\"../file_lib/{str_unpacked_zip_file_path_name}\" + \"//\" + str_unpacked_zip_file_name\n",
    "        \n",
    "        return str_json_file_path\n",
    "    \n",
    "    def flatten_dict(self , dict_input):\n",
    "        dict_flattened = {}\n",
    "\n",
    "        def flatten(current_element, parent_key=''):\n",
    "            if isinstance(current_element, dict):\n",
    "                for key, value in current_element.items():\n",
    "                    new_key = f\"{parent_key}_{key}\" if parent_key else key\n",
    "                    flatten(value, new_key)\n",
    "            elif isinstance(current_element, list):\n",
    "                for index, item in enumerate(current_element, start=1):\n",
    "                    indexed_key = f\"{parent_key}_{index}\"\n",
    "                    flatten(item, indexed_key)\n",
    "            else:\n",
    "                dict_flattened[parent_key] = current_element\n",
    "\n",
    "        flatten(dict_input)\n",
    "        return dict_flattened\n",
    "    \n",
    "    def clean_keys(self , input_dict):\n",
    "        cleaned_dict = {}\n",
    "        \n",
    "        for key, value in input_dict.items():\n",
    "            if not '@xml:lang' in key:\n",
    "                if key.endswith('_$'):\n",
    "                    new_key = key[:-2]  # Remove the last 2 characters ('_$')\n",
    "                else:\n",
    "                    new_key = key  # Keep the key as is\n",
    "                \n",
    "                cleaned_dict[new_key] = value\n",
    "        \n",
    "        return cleaned_dict\n",
    "    \n",
    "    def organize_by_prefix(self , input_dict):\n",
    "        dict_organized = {}\n",
    "        \n",
    "        for key, value in input_dict.items():\n",
    "            prefix, _, sub_key = key.partition('_')\n",
    "            \n",
    "            if prefix not in dict_organized:\n",
    "                dict_organized[prefix] = {}\n",
    "            \n",
    "            # Add the key-value pair to the corresponding sub-dictionary\n",
    "            dict_organized[prefix][sub_key] = value\n",
    "        \n",
    "        return dict_organized\n",
    "\n",
    "    def extract_other_entity_names(self , data_dict, base_keyword, exclude_keywords=None):\n",
    "        \"\"\"\n",
    "        Extracts and organizes `OtherEntityNames` keys into a list of tuples.\n",
    "        Each tuple contains the `@type` and the main value for a given numeric suffix.\n",
    "\n",
    "        :param data_dict: Dictionary containing the raw data.\n",
    "        :param base_keyword: Common substring to identify relevant keys (e.g., \"OtherEntityNames\").\n",
    "        :param exclude_keywords: List of keywords to exclude (e.g., [\"TranslatedOtherEntityNames\"]).\n",
    "        :return: A list of tuples: (type, value) for each numeric suffix group.\n",
    "        \"\"\"\n",
    "        grouped_data = {}\n",
    "\n",
    "        # Default empty list for exclude_keywords\n",
    "        if exclude_keywords is None:\n",
    "            exclude_keywords = []\n",
    "\n",
    "        # Group keys by numeric suffix\n",
    "        for key, value in data_dict.items():\n",
    "            # Skip keys with excluded keywords\n",
    "            if any(exclude in key for exclude in exclude_keywords):\n",
    "                continue\n",
    "\n",
    "            if base_keyword in key:\n",
    "                # Extract the numeric suffix using regex\n",
    "                match = re.search(r\"_(\\d+)\", key)\n",
    "                if not match:\n",
    "                    continue  # Skip keys without a numeric suffix\n",
    "                index = int(match.group(1))\n",
    "\n",
    "                if index not in grouped_data:\n",
    "                    grouped_data[index] = {\"type\": None, \"value\": None}\n",
    "\n",
    "                # Check if this key is `@type` or the main value\n",
    "                if \"@type\" in key:\n",
    "                    grouped_data[index][\"type\"] = value\n",
    "                else:\n",
    "                    grouped_data[index][\"value\"] = value\n",
    "\n",
    "        # Convert grouped data into a list of tuples\n",
    "        result = [(grouped_data[index][\"type\"], grouped_data[index][\"value\"]) for index in sorted(grouped_data.keys())]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def extract_event_data(self , dict_data, base_keyword, target_keys):\n",
    "        \"\"\"\n",
    "        Extracts and organizes data for repeated keys in a dictionary based on a base keyword and target keys.\n",
    "\n",
    "        :param dict_data: Dictionary containing the raw data.\n",
    "        :param base_keyword: Common substring to identify relevant keys (e.g., \"LegalEntityEvents\").\n",
    "        :param target_keys: List of substrings to match keys that should be included in the tuple.\n",
    "        :return: A list of tuples, one for each numeric suffix group, containing values for the target keys.\n",
    "        \"\"\"\n",
    "        grouped_data = {}\n",
    "\n",
    "\n",
    "        # Group keys by numeric suffix\n",
    "        for key, value in dict_data.items():\n",
    "            if base_keyword in key:\n",
    "                # Extract the numeric suffix using regex\n",
    "                match = re.search(r\"_(\\d+)_\", key)\n",
    "                if not match:\n",
    "                    continue  # Skip keys without a numeric suffix\n",
    "                index = int(match.group(1))\n",
    "\n",
    "                # Extract the part of the key after the numeric suffix\n",
    "                key_suffix = key.split(f\"_{index}_\")[-1]\n",
    "\n",
    "                if index not in grouped_data:\n",
    "                    grouped_data[index] = {}\n",
    "                \n",
    "                # Check if this key matches any of the target keys as a substring\n",
    "                for target in target_keys:\n",
    "                    if target in key_suffix:\n",
    "                        grouped_data[index][target] = value\n",
    "                        break\n",
    "\n",
    "        # Create tuples for each group of keys\n",
    "        result = []\n",
    "        for index in sorted(grouped_data.keys()):\n",
    "            # Create a tuple of values for the target keys, using None if a key is missing\n",
    "            tuple_values = tuple(grouped_data[index].get(target, None) for target in target_keys)\n",
    "            result.append(tuple_values)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_target_values(self , dict_data, target_keys, subset_string=False):\n",
    "        \"\"\"\n",
    "        Retrieves values for a set of target keys from a dictionary.\n",
    "        If a key is not present, it returns None for that key.\n",
    "        Optionally allows searching for subsets of key strings.\n",
    "\n",
    "        :param dict_data: Dictionary containing the raw data.\n",
    "        :param target_keys: List of keys (or substrings) to retrieve values for.\n",
    "        :param subset_string: Boolean indicating whether to search for subsets of key strings.\n",
    "        :return: A tuple of values corresponding to the target keys (or None if a key is missing).\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for target in target_keys:\n",
    "            if subset_string:\n",
    "                # Search for a key containing the target substring\n",
    "                found_key = next((key for key in dict_data if target in key), None)\n",
    "                result.append(dict_data.get(found_key, None))\n",
    "            else:\n",
    "                # Direct key lookup\n",
    "                result.append(dict_data.get(target, None))\n",
    "        return list(result)\n",
    "\n",
    "    def split_into_list_of_dictionaries(self , dict_data):\n",
    "        \"\"\"\n",
    "        Splits a dictionary into a list of dictionaries, grouped by numeric suffixes.\n",
    "        Keys without numeric suffixes are excluded.\n",
    "\n",
    "        :param data_dict: Dictionary containing the keys and values.\n",
    "        :return: A list of dictionaries, one for each numeric suffix.\n",
    "        \"\"\"\n",
    "        grouped_dicts = {}\n",
    "\n",
    "        # Iterate through the dictionary and group by numeric suffix\n",
    "        for key, value in dict_data.items():\n",
    "            match = re.search(r\"_(\\d+)_\", key)\n",
    "            if match:\n",
    "                # Extract the numeric suffix\n",
    "                group_number = int(match.group(1))\n",
    "                if group_number not in grouped_dicts:\n",
    "                    grouped_dicts[group_number] = {}\n",
    "                grouped_dicts[group_number][key] = value\n",
    "\n",
    "        # Convert the grouped dictionaries to a list\n",
    "        return [grouped_dicts[group] for group in sorted(grouped_dicts.keys())]\n",
    "\n",
    "\n",
    "    def further_flatten_geocoding(self , dict_data):\n",
    "        \"\"\"\n",
    "        Flattens a dictionary by extracting bounding box values and adding them as separate keys.\n",
    "        \n",
    "        :param data_dict: Dictionary containing geocoding data with bounding box values.\n",
    "        :return: A new dictionary with flattened bounding box values.\n",
    "        \"\"\"\n",
    "        flattened_dict = {}\n",
    "\n",
    "        for key, value in dict_data.items():\n",
    "            # Check for bounding box keys and split their values\n",
    "            if \"bounding_box\" in key:\n",
    "                # Extract numeric suffix if present\n",
    "                match = re.search(r\"_(\\d+)_\", key)\n",
    "                group_number = f\"_{match.group(1)}_\" if match else \"_\"\n",
    "\n",
    "                # Parse the bounding box values\n",
    "                bounding_box_values = value.split(\", \")\n",
    "                for box_value in bounding_box_values:\n",
    "                    # Split key-value pair (e.g., \"TopLeft.Latitude: 39.7496542\")\n",
    "                    box_key, box_val = box_value.split(\": \")\n",
    "                    # Create a new key with group number\n",
    "                    new_key = f\"{key.split('bounding_box')[0]}{box_key.strip()}{group_number}\".strip(\"_\")\n",
    "                    flattened_dict[new_key] = box_val.strip()\n",
    "            else:\n",
    "                # If not bounding box, keep the original key-value pair\n",
    "                flattened_dict[key] = value\n",
    "\n",
    "        return flattened_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel2Data:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_Data.db\" , bool_downloaded = True):\n",
    "        self.obj_backfill_helpers = GLEIF_Backill_Helpers(bool_Level_2_Trees = True)\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Backfill_level_2.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            str_level_2_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "            self.str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_2_download_link , str_unpacked_zip_file_path_name = \"Level_2_unpacked\" , str_zip_file_path_name = \"Level_2.zip\")\n",
    "    \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/Level_2_unpacked\")[0]\n",
    "        self.str_json_file_path = rf\"../file_lib/Level_2_unpacked\" + \"//\" + str_unpacked_zip_file_name\n",
    "        self.conn = sqlite3.connect(f'{str_db_name}.db', timeout=10)\n",
    "    \n",
    "    def create_table(self, conn):\n",
    "        with conn:\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_relationship_data (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                StartNode TEXT,                \n",
    "                EndNode TEXT,\n",
    "                RelationshipType TEXT,\n",
    "                RelationshipStatus TEXT,\n",
    "                RegistrationStatus TEXT,\n",
    "                NextRenewalDate TEXT\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "        conn.commit()\n",
    "    \n",
    "    def insert_relationship(self, list_relationship_data):\n",
    "        \n",
    "        with self.conn:\n",
    "            self.conn.execute(\"\"\"\n",
    "                INSERT INTO GLEIF_relationship_data (\n",
    "                StartNode, EndNode, RelationshipType, \n",
    "                RelationshipStatus, RegistrationStatus, NextRenewalDate\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?);\n",
    "            \"\"\", (list_relationship_data[0], list_relationship_data[1], list_relationship_data[2], list_relationship_data[3], list_relationship_data[4], list_relationship_data[5]))\n",
    "        \n",
    "    def process_relationships(self , dict_relationship):\n",
    "        dict_relationship_flattened = self.obj_backfill_helpers.flatten_dict(dict_input = dict_relationship)\n",
    "        list_relationship_data = self.obj_backfill_helpers.get_target_values(dict_data = dict_relationship_flattened, subset_string = True, target_keys = [\"StartNode\" , \"EndNode\" , \"RelationshipType\" , \"RelationshipStatus\" , \"RegistrationStatus\" , \"NextRenewalDate\"])\n",
    "        self.insert_relationship(list_relationship_data = list_relationship_data)\n",
    "        \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        self.create_table(conn = self.conn)\n",
    "        \n",
    "        with open(self.str_json_file_path, 'r' , encoding='utf-8') as file:\n",
    "            \n",
    "            dict_relationship_data = bigjson.load(file)\n",
    "            '''for dict_relationship in dict_relationship_data[\"relations\"]:\n",
    "                self.process_relationships(dict_relationship = dict_relationship) '''     \n",
    "            for counter, dict_lei in enumerate(dict_relationship_data['relations']):\n",
    "                if counter == 10000:\n",
    "                    break\n",
    "                dict_record = dict_lei.to_python()\n",
    "                self.process_relationships(dict_relationship = dict_record)              \n",
    "        \n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = GLEIFLevel2Data(bool_log = True)\n",
    "obj.storing_GLEIF_data_in_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
