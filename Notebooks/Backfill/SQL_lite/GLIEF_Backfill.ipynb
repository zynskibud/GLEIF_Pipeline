{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the jupyter notebook pertaining to the GLEIF_Backfill.py file.\n",
    "\n",
    "This code is responsible for obtaining all Level 1 and Level 2 Relationship data from the GLIEF, and backfilling the data and adding whatever features are needed to make the data accessible for further use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import logging\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import sqlite3\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "import bigjson\n",
    "import json\n",
    "import sys\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "from Infrastructure import System_Helpers\n",
    "\n",
    "class GLEIF_Backill_Helpers:\n",
    "    def __init__(self, bool_Level_1 = False, bool_Level_2_Trees = False, bool_Level_2_Reporting_Exceptions = False):\n",
    "        self.bool_Level_1 = bool_Level_1\n",
    "        self.bool_Level_2_Trees = bool_Level_2_Trees\n",
    "        self.bool_Level_2_Reporting_Exceptions = bool_Level_2_Reporting_Exceptions\n",
    "        \n",
    "\n",
    "    def get_level_download_links(self):\n",
    "        \"\"\"\n",
    "        This function uses selenium to webscrape the download link for all Level 1 Data in the GLEIF database.\n",
    "        \n",
    "        @return: str_download_link - the link which is used to download the entire GLEIF level 1\n",
    "        \"\"\"\n",
    "\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service)\n",
    "        driver.get(url = \"https://www.gleif.org/en/lei-data/gleif-golden-copy/download-the-golden-copy#/\")\n",
    "\n",
    "        cookie_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.CLASS_NAME, 'CybotCookiebotDialogBodyButton'))\n",
    "        )\n",
    "\n",
    "        cookie_button.click()\n",
    "\n",
    "        download_buttons = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CLASS_NAME, 'gc-download-button'))\n",
    "        )\n",
    "        \n",
    "        if self.bool_Level_1 == True:\n",
    "            download_buttons[0].click()\n",
    "        if self.bool_Level_2_Trees == True:\n",
    "            download_buttons[1].click()\n",
    "        if self.bool_Level_2_Reporting_Exceptions == True:\n",
    "            download_buttons[2].click()\n",
    "        \n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        driver.close()\n",
    "\n",
    "        str_download_link = ((soup.find_all(\"a\" , class_ = \"gc-icon gc-icon--json\"))[0])[\"href\"]\n",
    "        \n",
    "        return str_download_link        \n",
    "    \n",
    "    def unpacking_GLEIF_zip_files(self , str_download_link , str_zip_file_path_name , str_unpacked_zip_file_path_name):\n",
    "        session = requests.Session()\n",
    "        zip_file = session.get(url = str_download_link)\n",
    "\n",
    "        with open(rf\"../file_lib/{str_zip_file_path_name}\", 'wb') as f:\n",
    "            f.write(zip_file.content)\n",
    "\n",
    "        with zipfile.ZipFile(rf\"../file_lib/{str_zip_file_path_name}\", 'r') as zip_ref:\n",
    "            os.makedirs(rf\"../file_lib/{str_unpacked_zip_file_path_name}\", exist_ok=True)\n",
    "            zip_ref.extractall(rf\"../file_lib/{str_unpacked_zip_file_path_name}\")\n",
    "        \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/{str_unpacked_zip_file_path_name}\")[0]\n",
    "        str_json_file_path = rf\"../file_lib/{str_unpacked_zip_file_path_name}\" + \"//\" + str_unpacked_zip_file_name\n",
    "        \n",
    "        return str_json_file_path\n",
    "    \n",
    "    def flatten_dict(self , dict_input):\n",
    "        dict_flattened = {}\n",
    "\n",
    "        def flatten(current_element, parent_key=''):\n",
    "            if isinstance(current_element, dict):\n",
    "                for key, value in current_element.items():\n",
    "                    new_key = f\"{parent_key}_{key}\" if parent_key else key\n",
    "                    flatten(value, new_key)\n",
    "            elif isinstance(current_element, list):\n",
    "                for index, item in enumerate(current_element, start=1):\n",
    "                    indexed_key = f\"{parent_key}_{index}\"\n",
    "                    flatten(item, indexed_key)\n",
    "            else:\n",
    "                dict_flattened[parent_key] = current_element\n",
    "\n",
    "        flatten(dict_input)\n",
    "        return dict_flattened\n",
    "    \n",
    "    def clean_keys(self , input_dict):\n",
    "        cleaned_dict = {}\n",
    "        \n",
    "        for key, value in input_dict.items():\n",
    "            if not '@xml:lang' in key:\n",
    "                if key.endswith('_$'):\n",
    "                    new_key = key[:-2]  # Remove the last 2 characters ('_$')\n",
    "                else:\n",
    "                    new_key = key  # Keep the key as is\n",
    "                \n",
    "                cleaned_dict[new_key] = value\n",
    "        \n",
    "        return cleaned_dict\n",
    "    \n",
    "    def organize_by_prefix(self , input_dict):\n",
    "        dict_organized = {}\n",
    "        \n",
    "        for key, value in input_dict.items():\n",
    "            prefix, _, sub_key = key.partition('_')\n",
    "            \n",
    "            if prefix not in dict_organized:\n",
    "                dict_organized[prefix] = {}\n",
    "            \n",
    "            # Add the key-value pair to the corresponding sub-dictionary\n",
    "            dict_organized[prefix][sub_key] = value\n",
    "        \n",
    "        return dict_organized\n",
    "\n",
    "    def extract_other_entity_names(self , data_dict, base_keyword, exclude_keywords=None):\n",
    "        \"\"\"\n",
    "        Extracts and organizes `OtherEntityNames` keys into a list of tuples.\n",
    "        Each tuple contains the `@type` and the main value for a given numeric suffix.\n",
    "\n",
    "        :param data_dict: Dictionary containing the raw data.\n",
    "        :param base_keyword: Common substring to identify relevant keys (e.g., \"OtherEntityNames\").\n",
    "        :param exclude_keywords: List of keywords to exclude (e.g., [\"TranslatedOtherEntityNames\"]).\n",
    "        :return: A list of tuples: (type, value) for each numeric suffix group.\n",
    "        \"\"\"\n",
    "        grouped_data = {}\n",
    "\n",
    "        # Default empty list for exclude_keywords\n",
    "        if exclude_keywords is None:\n",
    "            exclude_keywords = []\n",
    "\n",
    "        # Group keys by numeric suffix\n",
    "        for key, value in data_dict.items():\n",
    "            # Skip keys with excluded keywords\n",
    "            if any(exclude in key for exclude in exclude_keywords):\n",
    "                continue\n",
    "\n",
    "            if base_keyword in key:\n",
    "                # Extract the numeric suffix using regex\n",
    "                match = re.search(r\"_(\\d+)\", key)\n",
    "                if not match:\n",
    "                    continue  # Skip keys without a numeric suffix\n",
    "                index = int(match.group(1))\n",
    "\n",
    "                if index not in grouped_data:\n",
    "                    grouped_data[index] = {\"type\": None, \"value\": None}\n",
    "\n",
    "                # Check if this key is `@type` or the main value\n",
    "                if \"@type\" in key:\n",
    "                    grouped_data[index][\"type\"] = value\n",
    "                else:\n",
    "                    grouped_data[index][\"value\"] = value\n",
    "\n",
    "        # Convert grouped data into a list of tuples\n",
    "        result = [(grouped_data[index][\"type\"], grouped_data[index][\"value\"]) for index in sorted(grouped_data.keys())]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def extract_event_data(self , dict_data, base_keyword, target_keys):\n",
    "        \"\"\"\n",
    "        Extracts and organizes data for repeated keys in a dictionary based on a base keyword and target keys.\n",
    "\n",
    "        :param dict_data: Dictionary containing the raw data.\n",
    "        :param base_keyword: Common substring to identify relevant keys (e.g., \"LegalEntityEvents\").\n",
    "        :param target_keys: List of substrings to match keys that should be included in the tuple.\n",
    "        :return: A list of tuples, one for each numeric suffix group, containing values for the target keys.\n",
    "        \"\"\"\n",
    "        grouped_data = {}\n",
    "\n",
    "\n",
    "        # Group keys by numeric suffix\n",
    "        for key, value in dict_data.items():\n",
    "            if base_keyword in key:\n",
    "                # Extract the numeric suffix using regex\n",
    "                match = re.search(r\"_(\\d+)_\", key)\n",
    "                if not match:\n",
    "                    continue  # Skip keys without a numeric suffix\n",
    "                index = int(match.group(1))\n",
    "\n",
    "                # Extract the part of the key after the numeric suffix\n",
    "                key_suffix = key.split(f\"_{index}_\")[-1]\n",
    "\n",
    "                if index not in grouped_data:\n",
    "                    grouped_data[index] = {}\n",
    "                \n",
    "                # Check if this key matches any of the target keys as a substring\n",
    "                for target in target_keys:\n",
    "                    if target in key_suffix:\n",
    "                        grouped_data[index][target] = value\n",
    "                        break\n",
    "\n",
    "        # Create tuples for each group of keys\n",
    "        result = []\n",
    "        for index in sorted(grouped_data.keys()):\n",
    "            # Create a tuple of values for the target keys, using None if a key is missing\n",
    "            tuple_values = tuple(grouped_data[index].get(target, None) for target in target_keys)\n",
    "            result.append(tuple_values)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def get_target_values(self , dict_data, target_keys, subset_string=False):\n",
    "        \"\"\"\n",
    "        Retrieves values for a set of target keys from a dictionary.\n",
    "        If a key is not present, it returns None for that key.\n",
    "        Optionally allows searching for subsets of key strings.\n",
    "\n",
    "        :param dict_data: Dictionary containing the raw data.\n",
    "        :param target_keys: List of keys (or substrings) to retrieve values for.\n",
    "        :param subset_string: Boolean indicating whether to search for subsets of key strings.\n",
    "        :return: A tuple of values corresponding to the target keys (or None if a key is missing).\n",
    "        \"\"\"\n",
    "        result = []\n",
    "        for target in target_keys:\n",
    "            if subset_string:\n",
    "                # Search for a key containing the target substring\n",
    "                found_key = next((key for key in dict_data if target in key), None)\n",
    "                result.append(dict_data.get(found_key, None))\n",
    "            else:\n",
    "                # Direct key lookup\n",
    "                result.append(dict_data.get(target, None))\n",
    "        return list(result)\n",
    "\n",
    "    def split_into_list_of_dictionaries(self , dict_data):\n",
    "        \"\"\"\n",
    "        Splits a dictionary into a list of dictionaries, grouped by numeric suffixes.\n",
    "        Keys without numeric suffixes are excluded.\n",
    "\n",
    "        :param data_dict: Dictionary containing the keys and values.\n",
    "        :return: A list of dictionaries, one for each numeric suffix.\n",
    "        \"\"\"\n",
    "        grouped_dicts = {}\n",
    "\n",
    "        # Iterate through the dictionary and group by numeric suffix\n",
    "        for key, value in dict_data.items():\n",
    "            match = re.search(r\"_(\\d+)_\", key)\n",
    "            if match:\n",
    "                # Extract the numeric suffix\n",
    "                group_number = int(match.group(1))\n",
    "                if group_number not in grouped_dicts:\n",
    "                    grouped_dicts[group_number] = {}\n",
    "                grouped_dicts[group_number][key] = value\n",
    "\n",
    "        # Convert the grouped dictionaries to a list\n",
    "        return [grouped_dicts[group] for group in sorted(grouped_dicts.keys())]\n",
    "\n",
    "\n",
    "    def further_flatten_geocoding(self , dict_data):\n",
    "        \"\"\"\n",
    "        Flattens a dictionary by extracting bounding box values and adding them as separate keys.\n",
    "        \n",
    "        :param data_dict: Dictionary containing geocoding data with bounding box values.\n",
    "        :return: A new dictionary with flattened bounding box values.\n",
    "        \"\"\"\n",
    "        flattened_dict = {}\n",
    "\n",
    "        for key, value in dict_data.items():\n",
    "            # Check for bounding box keys and split their values\n",
    "            if \"bounding_box\" in key:\n",
    "                # Extract numeric suffix if present\n",
    "                match = re.search(r\"_(\\d+)_\", key)\n",
    "                group_number = f\"_{match.group(1)}_\" if match else \"_\"\n",
    "\n",
    "                # Parse the bounding box values\n",
    "                bounding_box_values = value.split(\", \")\n",
    "                for box_value in bounding_box_values:\n",
    "                    # Split key-value pair (e.g., \"TopLeft.Latitude: 39.7496542\")\n",
    "                    box_key, box_val = box_value.split(\": \")\n",
    "                    # Create a new key with group number\n",
    "                    new_key = f\"{key.split('bounding_box')[0]}{box_key.strip()}{group_number}\".strip(\"_\")\n",
    "                    flattened_dict[new_key] = box_val.strip()\n",
    "            else:\n",
    "                # If not bounding box, keep the original key-value pair\n",
    "                flattened_dict[key] = value\n",
    "\n",
    "        return flattened_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel1Data:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_Data.db\" , bool_downloaded = True):\n",
    "        \n",
    "        self.obj_backfill_helpers = GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Backfill_level_1.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            str_level_1_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "            self.str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link , str_unpacked_zip_file_name = \"Level_1_unpacked\" , str_zip_file_path_name = \"Level_1.zip\")\n",
    "    \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/Level_1_unpacked\")[0]\n",
    "        self.str_json_file_path = rf\"../file_lib/Level_1_unpacked\" + \"//\" + str_unpacked_zip_file_name\n",
    "        self.conn = sqlite3.connect(f'{str_db_name}.db', timeout=10)\n",
    "    \n",
    "    def create_tables(self, conn):\n",
    "        with conn:\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_entity_data (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                lei TEXT NOT NULL,                \n",
    "                LegalName TEXT,\n",
    "                SuccessorEntityLEI TEXT,\n",
    "                SuccessorEntityName TEXT,\n",
    "                LegalJurisdiction TEXT,\n",
    "                EntityCategory TEXT,\n",
    "                EntitySubCategory TEXT,\n",
    "                LegalForm_EntityLegalFormCode TEXT,\n",
    "                LegalForm_OtherLegalForm TEXT,\n",
    "                EntityStatus TEXT,\n",
    "                EntityCreationDate TEXT,\n",
    "                RegistrationAuthority_RegistrationAuthorityID TEXT,\n",
    "                RegistrationAuthority_RegistrationAuthorityEntityID TEXT\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_other_legal_names (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                lei TEXT NOT NULL,                \n",
    "                OtherEntityNames,\n",
    "                Type,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_LegalAddress (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                lei TEXT NOT NULL,\n",
    "                LegalAddress_FirstAddressLine TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_1 TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_2 TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_3 TEXT,\n",
    "                LegalAddress_City TEXT,\n",
    "                LegalAddress_Region TEXT,\n",
    "                LegalAddress_Country TEXT,\n",
    "                LegalAddress_PostalCode TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_HeadquartersAddress (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                lei TEXT NOT NULL,\n",
    "                HeadquartersAddress_FirstAddressLine TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_1 TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_2 TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_3 TEXT,\n",
    "                HeadquartersAddress_City TEXT,\n",
    "                HeadquartersAddress_Region TEXT,\n",
    "                HeadquartersAddress_Country TEXT,\n",
    "                HeadquartersAddress_PostalCode TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "                        \n",
    "            #entity\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_LegalEntityEvents (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                lei TEXT NOT NULL,\n",
    "                group_type TEXT,\n",
    "                event_status TEXT,\n",
    "                LegalEntityEventType TEXT,\n",
    "                LegalEntityEventEffectiveDate TEXT,\n",
    "                LegalEntityEventRecordedDate TEXT,\n",
    "                ValidationDocuments TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "                \"\"\")\n",
    "            \n",
    "            #registration\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_registration_data (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                lei TEXT NOT NULL,\n",
    "                InitialRegistrationDate TEXT NOT NULL,\n",
    "                LastUpdateDate TEXT,\n",
    "                RegistrationStatus TEXT,\n",
    "                NextRenewalDate TEXT,\n",
    "                ManagingLOU TEXT,\n",
    "                ValidationSources TEXT,\n",
    "                ValidationAuthority TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "\n",
    "            #geoencoding\n",
    "            conn.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_geocoding (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                lei TEXT NOT NULL,\n",
    "                relevance REAL,\n",
    "                match_type TEXT,\n",
    "                lat REAL,\n",
    "                lng REAL,\n",
    "                geocoding_date TEXT,\n",
    "                TopLeft_Latitude REAL,\n",
    "                TopLeft_Longitude REAL,\n",
    "                BottomRight_Latitude REAL,\n",
    "                BottomRight_Longitude REAL,\n",
    "                match_level TEXT,\n",
    "                mapped_street TEXT,\n",
    "                mapped_housenumber TEXT,\n",
    "                mapped_postalcode TEXT,\n",
    "                mapped_city TEXT,\n",
    "                mapped_district TEXT,\n",
    "                mapped_state TEXT,\n",
    "                mapped_country TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "            \n",
    "            \n",
    "        conn.commit()\n",
    "    \n",
    "    def insert_entity_data(self , str_lei , list_entity_data):\n",
    "        \n",
    "        list_entity_data = list_entity_data if list_entity_data else [None] * 12\n",
    "\n",
    "        \n",
    "        with self.conn:\n",
    "            self.conn.execute(\"\"\"\n",
    "                INSERT INTO GLEIF_entity_data (\n",
    "                    lei, LegalName, SuccessorEntityLEI, SuccessorEntityName, LegalJurisdiction,\n",
    "                    EntityCategory, EntitySubCategory, LegalForm_EntityLegalFormCode,\n",
    "                    LegalForm_OtherLegalForm, EntityStatus, EntityCreationDate,\n",
    "                    RegistrationAuthority_RegistrationAuthorityID,\n",
    "                    RegistrationAuthority_RegistrationAuthorityEntityID\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "            \"\"\", [str_lei] + list_entity_data)\n",
    "            \n",
    "    def insert_other_name_data(self, str_lei, list_other_name_data):\n",
    "        \"\"\"\n",
    "        Inserts other entity names into the GLEIF_other_legal_names table.\n",
    "\n",
    "        Args:\n",
    "            lei (str): The LEI of the entity.\n",
    "            other_name_data (list): A list of dictionaries containing 'OtherEntityNames' and 'Type'.\n",
    "        \"\"\"\n",
    "        with self.conn:\n",
    "            if list_other_name_data:\n",
    "                for name_record in list_other_name_data:\n",
    "                    self.conn.execute(\"\"\"\n",
    "                        INSERT INTO GLEIF_other_legal_names (\n",
    "                            lei, OtherEntityNames, Type\n",
    "                        ) VALUES (?, ?, ?);\n",
    "                    \"\"\", (str_lei, name_record[1], name_record[0]))\n",
    "            else:\n",
    "                self.conn.execute(\"\"\"\n",
    "                        INSERT INTO GLEIF_other_legal_names (\n",
    "                            lei, OtherEntityNames, Type\n",
    "                        ) VALUES (?, ?, ?);\n",
    "                    \"\"\", (str_lei, None, None))\n",
    "\n",
    "                \n",
    "    def insert_legal_address_data(self, str_lei, list_legal_address_data):\n",
    "        \"\"\"\n",
    "        Inserts legal address data into the GLEIF_LegalAddress table.\n",
    "\n",
    "        Args:\n",
    "            lei (str): The LEI of the entity.\n",
    "            legal_address_data (list): A list containing legal address data in the following order:\n",
    "                [LegalAddress_FirstAddressLine, LegalAddress_AdditionalAddressLine_1,\n",
    "                LegalAddress_AdditionalAddressLine_2, LegalAddress_AdditionalAddressLine_3,\n",
    "                LegalAddress_City, LegalAddress_Region, LegalAddress_Country,\n",
    "                LegalAddress_PostalCode]\n",
    "        \"\"\"\n",
    "        list_legal_address_data = list_legal_address_data if list_legal_address_data else [None] * 8\n",
    "        \n",
    "        with self.conn:\n",
    "            self.conn.execute(\"\"\"\n",
    "                INSERT INTO GLEIF_LegalAddress (\n",
    "                    lei, LegalAddress_FirstAddressLine, LegalAddress_AdditionalAddressLine_1,\n",
    "                    LegalAddress_AdditionalAddressLine_2, LegalAddress_AdditionalAddressLine_3,\n",
    "                    LegalAddress_City, LegalAddress_Region, LegalAddress_Country,\n",
    "                    LegalAddress_PostalCode\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "            \"\"\", [str_lei] + list_legal_address_data)\n",
    "            \n",
    "    def insert_headquarter_address_data(self, str_lei, list_hq_address_data):\n",
    "        \"\"\"\n",
    "        Inserts headquarters address data into the GLEIF_HeadquartersAddress table.\n",
    "\n",
    "        Args:\n",
    "            lei (str): The LEI of the entity.\n",
    "            hq_address_data (list): A list containing headquarters address data in the following order:\n",
    "                [HeadquartersAddress_FirstAddressLine, HeadquartersAddress_AdditionalAddressLine_1,\n",
    "                HeadquartersAddress_AdditionalAddressLine_2, HeadquartersAddress_AdditionalAddressLine_3,\n",
    "                HeadquartersAddress_City, HeadquartersAddress_Region, HeadquartersAddress_Country,\n",
    "                HeadquartersAddress_PostalCode]\n",
    "        \"\"\"\n",
    "        list_hq_address_data = list_hq_address_data if list_hq_address_data else [None] * 8        \n",
    "        \n",
    "        with self.conn:\n",
    "            self.conn.execute(\"\"\"\n",
    "                INSERT INTO GLEIF_HeadquartersAddress (\n",
    "                    lei, HeadquartersAddress_FirstAddressLine, HeadquartersAddress_AdditionalAddressLine_1,\n",
    "                    HeadquartersAddress_AdditionalAddressLine_2, HeadquartersAddress_AdditionalAddressLine_3,\n",
    "                    HeadquartersAddress_City, HeadquartersAddress_Region, HeadquartersAddress_Country,\n",
    "                    HeadquartersAddress_PostalCode\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "            \"\"\", [str_lei] + list_hq_address_data)\n",
    "\n",
    "    def insert_legal_entity_events_data(self, str_lei, list_legal_entity_events_data):\n",
    "        \"\"\"\n",
    "        Inserts legal entity event data into the GLEIF_LegalEntityEvents table.\n",
    "\n",
    "        Args:\n",
    "            lei (str): The LEI of the entity.\n",
    "            legal_entity_events_data (list): A list of dictionaries containing event data.\n",
    "        \"\"\"\n",
    "        with self.conn:\n",
    "            if list_legal_entity_events_data:\n",
    "                if len(list_legal_entity_events_data) == 1:\n",
    "                    self.conn.execute(\"\"\"\n",
    "                            INSERT INTO GLEIF_LegalEntityEvents (\n",
    "                                lei, group_type, event_status, LegalEntityEventType,\n",
    "                                LegalEntityEventEffectiveDate, LegalEntityEventRecordedDate,\n",
    "                                ValidationDocuments\n",
    "                            ) VALUES (?, ?, ?, ?, ?, ?, ?);\n",
    "                        \"\"\", (str_lei, list_legal_entity_events_data[0][0], list_legal_entity_events_data[0][1], list_legal_entity_events_data[0][2], list_legal_entity_events_data[0][3], list_legal_entity_events_data[0][4], list_legal_entity_events_data[0][5]))\n",
    "                else:\n",
    "                    for event in list_legal_entity_events_data:\n",
    "                        self.conn.execute(\"\"\"\n",
    "                            INSERT INTO GLEIF_LegalEntityEvents (\n",
    "                                lei, group_type, event_status, LegalEntityEventType,\n",
    "                                LegalEntityEventEffectiveDate, LegalEntityEventRecordedDate,\n",
    "                                ValidationDocuments\n",
    "                            ) VALUES (?, ?, ?, ?, ?, ?, ?);\n",
    "                        \"\"\", (str_lei, event[0], event[1], event[2], event[3], event[4], event[5]))\n",
    "            else:\n",
    "                self.conn.execute(\"\"\"\n",
    "                        INSERT INTO GLEIF_LegalEntityEvents (\n",
    "                            lei, group_type, event_status, LegalEntityEventType,\n",
    "                            LegalEntityEventEffectiveDate, LegalEntityEventRecordedDate,\n",
    "                            ValidationDocuments\n",
    "                        ) VALUES (?, ?, ?, ?, ?, ?, ?);\n",
    "                    \"\"\", (str_lei, None, None, None, None, None, None))\n",
    "\n",
    "    def insert_registration_data(self, str_lei, list_registration_data):\n",
    "        \"\"\"\n",
    "        Inserts registration data into the GLEIF_registration_data table.\n",
    "\n",
    "        Args:\n",
    "            lei (str): The LEI of the entity.\n",
    "            registration_data (list): A list containing registration data in the following order:\n",
    "                [InitialRegistrationDate, LastUpdateDate, RegistrationStatus, NextRenewalDate,\n",
    "                ManagingLOU, ValidationSources, ValidationAuthority]\n",
    "        \"\"\"\n",
    "        list_registration_data = list_registration_data if list_registration_data else [None] * 7        \n",
    "        \n",
    "        with self.conn:\n",
    "            self.conn.execute(\"\"\"\n",
    "                INSERT INTO GLEIF_registration_data (\n",
    "                    lei, InitialRegistrationDate, LastUpdateDate, RegistrationStatus,\n",
    "                    NextRenewalDate, ManagingLOU, ValidationSources, ValidationAuthority\n",
    "                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\n",
    "            \"\"\", [str_lei] + list_registration_data)\n",
    "\n",
    "    def insert_geocoding_data(self, str_lei, list_geocoding_data):\n",
    "        \"\"\"\n",
    "        Inserts geocoding data into the GLEIF_geocoding table.\n",
    "\n",
    "        Args:\n",
    "            lei (str): The LEI of the entity.\n",
    "            geocoding_data_list (list): A list of dictionaries containing geocoding data.\n",
    "        \"\"\"\n",
    "        with self.conn:\n",
    "            if all(not isinstance(item , list) for item in list_geocoding_data):\n",
    "                self.conn.execute(\"\"\"\n",
    "                        INSERT INTO GLEIF_geocoding (\n",
    "                            lei, relevance, match_type, lat, lng, geocoding_date,\n",
    "                            \"TopLeft_Latitude\", \"TopLeft_Longitude\", \"BottomRight_Latitude\", \"BottomRight_Longitude\",\n",
    "                            match_level, mapped_street, mapped_housenumber, mapped_postalcode,\n",
    "                            mapped_city, mapped_district, mapped_state, mapped_country\n",
    "                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "                    \"\"\", (\n",
    "                        str_lei,\n",
    "                        list_geocoding_data[0],\n",
    "                        list_geocoding_data[1],\n",
    "                        list_geocoding_data[2],\n",
    "                        list_geocoding_data[3],\n",
    "                        list_geocoding_data[4],\n",
    "                        list_geocoding_data[5],\n",
    "                        list_geocoding_data[6],\n",
    "                        list_geocoding_data[7],\n",
    "                        list_geocoding_data[8],\n",
    "                        list_geocoding_data[9],\n",
    "                        list_geocoding_data[10],\n",
    "                        list_geocoding_data[11],\n",
    "                        list_geocoding_data[12],\n",
    "                        list_geocoding_data[13],\n",
    "                        list_geocoding_data[14],\n",
    "                        list_geocoding_data[15],\n",
    "                        list_geocoding_data[16]\n",
    "                    ))\n",
    "            else:\n",
    "                for geocoding_data in list_geocoding_data:                    \n",
    "                    self.conn.execute(\"\"\"\n",
    "                        INSERT INTO GLEIF_geocoding (\n",
    "                            lei, relevance, match_type, lat, lng, geocoding_date,\n",
    "                            \"TopLeft_Latitude\", \"TopLeft_Longitude\", \"BottomRight_Latitude\", \"BottomRight_Longitude\",\n",
    "                            match_level, mapped_street, mapped_housenumber, mapped_postalcode,\n",
    "                            mapped_city, mapped_district, mapped_state, mapped_country\n",
    "                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?);\n",
    "                    \"\"\", (\n",
    "                        str_lei,\n",
    "                        geocoding_data[0],\n",
    "                        geocoding_data[1],\n",
    "                        geocoding_data[2],\n",
    "                        geocoding_data[3],\n",
    "                        geocoding_data[4],\n",
    "                        geocoding_data[5],\n",
    "                        geocoding_data[6],\n",
    "                        geocoding_data[7],\n",
    "                        geocoding_data[8],\n",
    "                        geocoding_data[9],\n",
    "                        geocoding_data[10],\n",
    "                        geocoding_data[11],\n",
    "                        geocoding_data[12],\n",
    "                        geocoding_data[13],\n",
    "                        geocoding_data[14],\n",
    "                        geocoding_data[15],\n",
    "                        geocoding_data[16]\n",
    "                    ))\n",
    "\n",
    "    def process_entity_data(self , dict_entity_data , str_lei):\n",
    "        list_gleif_entity_data = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity_data , target_keys = [\"LegalName\" , \"SuccessorEntityLEI\" , \"SuccessorEntityName\" , \"LegalJurisdiction\" , \"EntityCategory\" , \"EntitySubCategory\" , \"LegalForm_EntityLegalFormCode\" , \"LegalForm_OtherLegalForm\" , \"EntityStatus\" , \"EntityCreationDate\" , \"RegistrationAuthority_RegistrationAuthorityID\" , \"RegistrationAuthority_RegistrationAuthorityEntityID\" ])      \n",
    "        list_gleif_other_name_data = self.obj_backfill_helpers.extract_other_entity_names(data_dict = dict_entity_data, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "        list_gleif_legal_address_data = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity_data , target_keys = [\"LegalAddress_FirstAddressLine\" , \"LegalAddress_AdditionalAddressLine_1\" , \"LegalAddress_AdditionalAddressLine_2\" , \"LegalAddress_AdditionalAddressLine_3\" , \"LegalAddress_City\" , \"LegalAddress_Region\" , \"LegalAddress_Country\" , \"LegalAddress_PostalCode\"])\n",
    "        list_gleif_headquarter_address_data = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity_data , target_keys = [\"HeadquartersAddress_FirstAddressLine\" , \"HeadquartersAddress_AdditionalAddressLine_1\" , \"HeadquartersAddress_AdditionalAddressLine_2\" , \"HeadquartersAddress_AdditionalAddressLine_3\" , \"HeadquartersAddress_City\" , \"HeadquartersAddress_Region\" , \"HeadquartersAddress_Country\" , \"HeadquartersAddress_PostalCode\"])\n",
    "        list_gleif_legal_entity_event_data = self.obj_backfill_helpers.extract_event_data(dict_data = dict_entity_data , base_keyword=\"LegalEntityEvents\" , target_keys=[\"group_type\", \"event_status\", \"LegalEntityEventType\", \"LegalEntityEventEffectiveDate\", \"LegalEntityEventRecordedDate\", \"ValidationDocuments\"])\n",
    "\n",
    "        self.insert_entity_data(str_lei = str_lei , list_entity_data = list_gleif_entity_data)\n",
    "        self.insert_other_name_data(str_lei = str_lei , list_other_name_data = list_gleif_other_name_data)\n",
    "        self.insert_legal_address_data(str_lei = str_lei , list_legal_address_data = list_gleif_legal_address_data)\n",
    "        self.insert_headquarter_address_data(str_lei = str_lei , list_hq_address_data = list_gleif_headquarter_address_data)\n",
    "        self.insert_legal_entity_events_data(str_lei = str_lei , list_legal_entity_events_data = list_gleif_legal_entity_event_data)\n",
    "    \n",
    "    def process_registration_data(self , dict_registration_data , str_lei):\n",
    "        list_gleif_registration_dta = self.obj_backfill_helpers.get_target_values(dict_data = dict_registration_data , target_keys = [\"InitialRegistrationDate\" , \"LastUpdateDate\" , \"RegistrationStatus\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationAuthority\"])    \n",
    "        self.insert_registration_data(str_lei = str_lei , list_registration_data = list_gleif_registration_dta)\n",
    "    \n",
    "    def process_extension_data(self , dict_extension_data , str_lei):\n",
    "        dict_extension_data_flattened = self.obj_backfill_helpers.further_flatten_geocoding(dict_data = dict_extension_data)\n",
    "        list_extension_data = []\n",
    "        \n",
    "        if any(re.search(r\"_\\d+_\", key) for key in dict_extension_data_flattened.keys()):\n",
    "            list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data = dict_extension_data_flattened)\n",
    "            counter = 1\n",
    "            for dict_extension in list_dicts:\n",
    "                list_extension_data.append(self.obj_backfill_helpers.get_target_values(dict_data = dict_extension , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"]))\n",
    "            counter +=1\n",
    "        else:\n",
    "            list_extension_data = self.obj_backfill_helpers.get_target_values(dict_data = dict_extension_data_flattened , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "        \n",
    "        self.insert_geocoding_data(str_lei = str_lei , list_geocoding_data = list_extension_data) \n",
    "            \n",
    "    def process_data(self , dict_lei_pre_processing):\n",
    "        dict_lei = self.obj_backfill_helpers.flatten_dict(dict_lei_pre_processing)\n",
    "        dict_lei_clean = self.obj_backfill_helpers.clean_keys(dict_lei)\n",
    "        dict_organized = (self.obj_backfill_helpers.organize_by_prefix(dict_lei_clean))\n",
    "\n",
    "        self.process_entity_data(dict_entity_data = dict_organized[\"Entity\"] , str_lei = dict_organized[\"LEI\"][\"\"])\n",
    "        self.process_registration_data(dict_registration_data = dict_organized[\"Registration\"] , str_lei =  dict_organized[\"LEI\"][\"\"])\n",
    "        self.process_extension_data(dict_extension_data = dict_organized[\"Extension\"] , str_lei = dict_organized[\"LEI\"][\"\"])\n",
    "        \n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        self.create_tables(conn = self.conn)\n",
    "        \n",
    "        \n",
    "        with open(self.str_json_file_path, 'rb') as file:\n",
    "            dict_leis = bigjson.load(file)\n",
    "            \n",
    "            counter = 0\n",
    "            \n",
    "            for dict_lei in dict_leis['records']:\n",
    "                if counter != 10000:\n",
    "                    dict_record = dict_lei.to_python()\n",
    "                    self.process_data(dict_lei_pre_processing = dict_record) \n",
    "                else:\n",
    "                    break\n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_dict(dict_input):\n",
    "    dict_flattened = {}\n",
    "\n",
    "    def flatten(current_element, parent_key=''):\n",
    "        if isinstance(current_element, dict):\n",
    "            for key, value in current_element.items():\n",
    "                new_key = f\"{parent_key}_{key}\" if parent_key else key\n",
    "                flatten(value, new_key)\n",
    "        elif isinstance(current_element, list):\n",
    "            for index, item in enumerate(current_element, start=1):\n",
    "                indexed_key = f\"{parent_key}_{index}\"\n",
    "                flatten(item, indexed_key)\n",
    "        else:\n",
    "            dict_flattened[parent_key] = current_element\n",
    "\n",
    "    flatten(dict_input)\n",
    "    return dict_flattened\n",
    "    \n",
    "def clean_keys(input_dict):\n",
    "    cleaned_dict = {}\n",
    "    \n",
    "    for key, value in input_dict.items():\n",
    "        if not '@xml:lang' in key:\n",
    "            if key.endswith('_$'):\n",
    "                new_key = key[:-2]  # Remove the last 2 characters ('_$')\n",
    "            else:\n",
    "                new_key = key  # Keep the key as is\n",
    "            \n",
    "            cleaned_dict[new_key] = value\n",
    "    \n",
    "    return cleaned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = GLEIFLevel1Data(bool_log = True)\n",
    "str_json_file_path = obj.str_json_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_entity_data(dict_entity_data , str_lei):\n",
    "    list_gleif_entity_data = obj_backfill_helpers.get_target_values(dict_data = dict_entity_data , target_keys = [\"LegalName\" , \"SuccessorEntityLEI\" , \"SuccessorEntityName\" , \"LegalJurisdiction\" , \"EntityCategory\" , \"EntitySubCategory\" , \"LegalForm_EntityLegalFormCode\" , \"LegalForm_OtherLegalForm\" , \"EntityStatus\" , \"EntityCreationDate\" , \"RegistrationAuthority_RegistrationAuthorityID\" , \"RegistrationAuthority_RegistrationAuthorityEntityID\" ])      \n",
    "    list_gleif_other_name_data  = obj_backfill_helpers.extract_other_entity_names(data_dict = dict_entity_data, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "    list_gleif_legal_address_data = obj_backfill_helpers.get_target_values(dict_data = dict_entity_data , target_keys = [\"LegalAddress_FirstAddressLine\" , \"LegalAddress_AdditionalAddressLine_1\" , \"LegalAddress_AdditionalAddressLine_2\" , \"LegalAddress_AdditionalAddressLine_3\" , \"LegalAddress_City\" , \"LegalAddress_Region\" , \"LegalAddress_Country\" , \"LegalAddress_PostalCode\"])\n",
    "    list_gleif_headquarter_address_data = obj_backfill_helpers.get_target_values(dict_data = dict_entity_data , target_keys = [\"HeadquartersAddress_FirstAddressLine\" , \"HeadquartersAddress_AdditionalAddressLine_1\" , \"HeadquartersAddress_AdditionalAddressLine_2\" , \"HeadquartersAddress_AdditionalAddressLine_3\" , \"HeadquartersAddress_City\" , \"HeadquartersAddress_Region\" , \"HeadquartersAddress_Country\" , \"HeadquartersAddress_PostalCode\"])\n",
    "    list_gleif_legal_entity_event_data = obj_backfill_helpers.extract_event_data(dict_data = dict_entity_data , base_keyword=\"LegalEntityEvents\" , target_keys=[\"group_type\", \"event_status\", \"LegalEntityEventType\", \"LegalEntityEventEffectiveDate\", \"LegalEntityEventRecordedDate\", \"ValidationDocuments\"])\n",
    "\n",
    "    \"\"\"insert_entity_data(str_lei = str_lei , list_entity_data = list_gleif_entity_data)\n",
    "    insert_other_name_data(str_lei = str_lei , list_other_name_data = list_gleif_other_name_data)\n",
    "    insert_legal_address_data(str_lei = str_lei , list_legal_address_data = list_gleif_legal_address_data)\n",
    "    insert_headquarter_address_data(str_lei = str_lei , list_headquarter_address_data = list_gleif_headquarter_address_data)\n",
    "    insert_legal_entity_events_data(str_lei = str_lei , list_legal_entity_event_data = list_gleif_legal_entity_event_data)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_registration_data(dict_registration_data , str_lei):\n",
    "    list_gleif_registration_dta = obj_backfill_helpers.get_target_values(dict_data = dict_registration_data , target_keys = [\"InitialRegistrationDate\" , \"LastUpdateDate\" , \"RegistrationStatus\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationAuthority\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_extension_data(dict_extension_data , str_lei):\n",
    "    \n",
    "    dict_extension_data_flattened = obj_backfill_helpers.further_flatten_geocoding(dict_data = dict_extension_data)\n",
    "    \n",
    "    list_extension_data = []\n",
    "    \n",
    "    \n",
    "    if any(re.search(r\"_\\d+_\", key) for key in dict_extension_data_flattened.keys()):\n",
    "        list_dicts = obj_backfill_helpers.split_into_list_of_dictionaries(dict_data = dict_extension_data_flattened)\n",
    "        counter = 1\n",
    "        for dict_extension in list_dicts:\n",
    "            list_extension_data.append(obj_backfill_helpers.get_target_values(dict_data = dict_extension , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"]))\n",
    "        counter +=1\n",
    "    else:\n",
    "        list_extension_data = obj_backfill_helpers.get_target_values(dict_data = dict_extension_data_flattened , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "        \n",
    "    return list_extension_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(dict_lei_pre_processing):\n",
    "    dict_lei = flatten_dict(dict_lei_pre_processing)\n",
    "    dict_lei_clean = clean_keys(dict_lei)\n",
    "    dict_organized = (obj_backfill_helpers.organize_by_prefix(dict_lei_clean))\n",
    "\n",
    "    process_entity_data(dict_entity_data = dict_organized[\"Entity\"] , str_lei = dict_organized[\"LEI\"][\"\"])\n",
    "    process_registration_data(dict_registration_data = dict_organized[\"Registration\"] , str_lei =  dict_organized[\"LEI\"][\"\"])\n",
    "    process_extension_data(dict_extentsion_data = dict_organized[\"Extension\"] , str_lei = dict_organized[\"LEI\"][\"\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LEI': {'$': '001GPB6A9XPE8XJICC14'},\n",
       " 'Entity': {'LegalName': {'@xml:lang': 'en',\n",
       "   '$': 'Fidelity Advisor Leveraged Company Stock Fund'},\n",
       "  'OtherEntityNames': {'OtherEntityName': [{'@xml:lang': 'en',\n",
       "     '@type': 'PREVIOUS_LEGAL_NAME',\n",
       "     '$': 'FIDELITY ADVISOR SERIES I - Fidelity Advisor Leveraged Company Stock Fund'}]},\n",
       "  'LegalAddress': {'@xml:lang': 'en',\n",
       "   'FirstAddressLine': {'$': '245 SUMMER STREET'},\n",
       "   'City': {'$': 'BOSTON'},\n",
       "   'Region': {'$': 'US-MA'},\n",
       "   'Country': {'$': 'US'},\n",
       "   'PostalCode': {'$': '02210'}},\n",
       "  'HeadquartersAddress': {'@xml:lang': 'en',\n",
       "   'FirstAddressLine': {'$': 'C/O FIDELITY MANAGEMENT & RESEARCH COMPANY LLC'},\n",
       "   'AdditionalAddressLine': [{'$': 'CORPORATION TRUST CENTER'},\n",
       "    {'$': '1209 ORANGE ST'}],\n",
       "   'City': {'$': 'WILMINGTON'},\n",
       "   'Region': {'$': 'US-MA'},\n",
       "   'Country': {'$': 'US'},\n",
       "   'PostalCode': {'$': '19801'}},\n",
       "  'RegistrationAuthority': {'RegistrationAuthorityID': {'$': 'RA000665'},\n",
       "   'RegistrationAuthorityEntityID': {'$': 'S000005113'}},\n",
       "  'LegalJurisdiction': {'$': 'US'},\n",
       "  'EntityCategory': {'$': 'FUND'},\n",
       "  'LegalForm': {'EntityLegalFormCode': {'$': '8888'},\n",
       "   'OtherLegalForm': {'$': 'FUND'}},\n",
       "  'EntityStatus': {'$': 'ACTIVE'},\n",
       "  'EntityCreationDate': {'$': '2012-11-29T00:00:00.000Z'}},\n",
       " 'Registration': {'InitialRegistrationDate': {'$': '2012-11-29T16:33:00.000Z'},\n",
       "  'LastUpdateDate': {'$': '2024-03-28T15:27:52.162Z'},\n",
       "  'RegistrationStatus': {'$': 'ISSUED'},\n",
       "  'NextRenewalDate': {'$': '2025-04-18T15:48:53.604Z'},\n",
       "  'ManagingLOU': {'$': '5493001KJTIIGC8Y1R12'},\n",
       "  'ValidationSources': {'$': 'FULLY_CORROBORATED'},\n",
       "  'ValidationAuthority': {'ValidationAuthorityID': {'$': 'RA000665'},\n",
       "   'ValidationAuthorityEntityID': {'$': 'S000005113'}}},\n",
       " 'Extension': {'gleif:Geocoding': {'gleif:original_address': {'$': 'CORPORATION TRUST CENTER, 1209 ORANGE ST, 19801, WILMINGTON, US-MA, US'},\n",
       "   'gleif:relevance': {'$': '0.57'},\n",
       "   'gleif:match_type': {'$': 'pointAddress'},\n",
       "   'gleif:lat': {'$': '34.23431'},\n",
       "   'gleif:lng': {'$': '-77.93327'},\n",
       "   'gleif:geocoding_date': {'$': '2024-02-11T01:36:09'},\n",
       "   'gleif:bounding_box': {'$': 'TopLeft.Latitude: 34.2354342, TopLeft.Longitude: -77.9346297, BottomRight.Latitude: 34.2331858, BottomRight.Longitude: -77.9319103'},\n",
       "   'gleif:match_level': {'$': 'houseNumber'},\n",
       "   'gleif:formatted_address': {'$': '1209 Orange St, Wilmington, NC 28401, United States'},\n",
       "   'gleif:mapped_location_id': {'$': 'NT_Qr18nTUbvpjQKWsGly5fgD_xIDM5A'},\n",
       "   'gleif:mapped_street': {'$': 'Orange St'},\n",
       "   'gleif:mapped_housenumber': {'$': '1209'},\n",
       "   'gleif:mapped_postalcode': {'$': '28401'},\n",
       "   'gleif:mapped_city': {'$': 'Wilmington'},\n",
       "   'gleif:mapped_district': {'$': 'South Side'},\n",
       "   'gleif:mapped_state': {'$': 'NC'},\n",
       "   'gleif:mapped_country': {'$': 'USA'}},\n",
       "  'gleif:conformity': {'gleif:conformityflag': {'$': 'CONFORMING'}}}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'LegalName': 'Fidelity Advisor Leveraged Company Stock Fund',\n",
       " 'OtherEntityNames_OtherEntityName_1_@type': 'PREVIOUS_LEGAL_NAME',\n",
       " 'OtherEntityNames_OtherEntityName_1': 'FIDELITY ADVISOR SERIES I - Fidelity Advisor Leveraged Company Stock Fund',\n",
       " 'LegalAddress_FirstAddressLine': '245 SUMMER STREET',\n",
       " 'LegalAddress_City': 'BOSTON',\n",
       " 'LegalAddress_Region': 'US-MA',\n",
       " 'LegalAddress_Country': 'US',\n",
       " 'LegalAddress_PostalCode': '02210',\n",
       " 'HeadquartersAddress_FirstAddressLine': 'C/O FIDELITY MANAGEMENT & RESEARCH COMPANY LLC',\n",
       " 'HeadquartersAddress_AdditionalAddressLine_1': 'CORPORATION TRUST CENTER',\n",
       " 'HeadquartersAddress_AdditionalAddressLine_2': '1209 ORANGE ST',\n",
       " 'HeadquartersAddress_City': 'WILMINGTON',\n",
       " 'HeadquartersAddress_Region': 'US-MA',\n",
       " 'HeadquartersAddress_Country': 'US',\n",
       " 'HeadquartersAddress_PostalCode': '19801',\n",
       " 'RegistrationAuthority_RegistrationAuthorityID': 'RA000665',\n",
       " 'RegistrationAuthority_RegistrationAuthorityEntityID': 'S000005113',\n",
       " 'LegalJurisdiction': 'US',\n",
       " 'EntityCategory': 'FUND',\n",
       " 'LegalForm_EntityLegalFormCode': '8888',\n",
       " 'LegalForm_OtherLegalForm': 'FUND',\n",
       " 'EntityStatus': 'ACTIVE',\n",
       " 'EntityCreationDate': '2012-11-29T00:00:00.000Z'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(str_json_file_path, 'rb') as file:\n",
    "    dict_leis = bigjson.load(file)\n",
    "    \n",
    "    first_record = (dict_leis['records'][0]).to_python()\n",
    "    display(first_record)\n",
    "dict_flat = flatten_dict(first_record)\n",
    "clear_dict = clean_keys(dict_flat)\n",
    "organized = obj.obj_backfill_helpers.organize_by_prefix(clear_dict)\n",
    "display(organized[\"Entity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PREVIOUS_LEGAL_NAME',\n",
       "  'FIDELITY ADVISOR SERIES I - Fidelity Advisor Leveraged Company Stock Fund')]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_gleif_other_name_data  = obj.obj_backfill_helpers.extract_other_entity_names(data_dict = organized[\"Entity\"], base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "display(list_gleif_other_name_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display((process_extension_data(dict_extension_data = organized[\"Extension\"] , str_lei = \"idk\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_gleif_other_name_data  = obj_backfill_helpers.extract_other_entity_names(data_dict = organized[\"Entity\"] , base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(list_gleif_other_name_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_registration_data = list_gleif_other_name_data if list_gleif_other_name_data else [None] * 2       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
