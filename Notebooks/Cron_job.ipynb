{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime, timezone\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "\n",
    "from Production.Update import GLEIF_Update_Helpers\n",
    "from Production.Update import GLIEF_Update_level_1\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n",
    "from Production.Backfill import GLEIF_Backfill_Level_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CronJobLevel1:\n",
    "    def __init__(self , str_db_name = \"GLEIF_test_db\"):\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.obj_gleif_update_helpers = GLEIF_Update_Helpers.GLEIF_Update_Helpers()\n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers\n",
    "        #self.obj_backfill_level_1 = GLEIF_Backfill_Level_1.GLEIFLevel1Data()\n",
    "        #self.obj_update_level_1 = GLIEF_Update_level_1.GLEIFUpdateLevel1()\n",
    "        \n",
    "    def get_file_recordings(self , str_table_name = \"gleif_files_processed\"):\n",
    "        \n",
    "        sql = f\"\"\"\n",
    "            SELECT DISTINCT ON (data_title)\n",
    "                time_interval, date, file_name, data_title\n",
    "            FROM {str_table_name}\n",
    "            WHERE data_title IN ('Level_1_meta_data')\n",
    "            ORDER BY data_title, date DESC, time_interval DESC\n",
    "        \"\"\"\n",
    "        self.cursor.execute(sql)\n",
    "        rows = self.cursor.fetchall()  # up to 2 rows here (one for each data_title)\n",
    "        \n",
    "        # Separate them into two lists\n",
    "        level_1_list = []\n",
    "        \n",
    "        for row in rows:\n",
    "            level_1_list.append(row)\n",
    "        \n",
    "        return level_1_list\n",
    "    \n",
    "    def obtain_necessary_date_elements(self , level_1_list):\n",
    "        #level_1_list = self.get_file_recordings()\n",
    "        tup_most_recent_entry = level_1_list[-1]\n",
    "        str_time_interval = tup_most_recent_entry[0]\n",
    "        str_parsed_time_interval = str_time_interval[:2] + str_time_interval[3:5]\n",
    "        str_date_processed = (tup_most_recent_entry[1]).replace(\"-\", \"/\")\n",
    "        str_date_processed_naive_dt = datetime.strptime(str_date_processed, \"%Y/%m/%d\")\n",
    "        str_date_processed_dt = str_date_processed_naive_dt.replace(tzinfo=timezone.utc)\n",
    "        \n",
    "        str_current_date = datetime.now(timezone.utc)\n",
    "        #str_formatted_current_date = str_current_date.strftime(\"%m/%d/%Y\")\n",
    "        str_current_interval = self.obj_gleif_update_helpers.get_current_interval()\n",
    "        \n",
    "        return str_parsed_time_interval , str_date_processed_dt , str_current_date , str_current_interval\n",
    "    \n",
    "    def analyze_for_error(self , level_1_list):\n",
    "        # This function will be used to analyze the level 1 data for errors in cron job scheduling\n",
    "        \n",
    "        str_parsed_time_interval , str_date_processed_dt , str_current_date , str_current_interval = self.obtain_necessary_date_elements(level_1_list = level_1_list)\n",
    "                \n",
    "        date_dif = str_current_date - str_date_processed_dt\n",
    "        date_dif_days = date_dif.days\n",
    "        \n",
    "        if date_dif_days > 1:\n",
    "            return -1\n",
    "        \n",
    "        if str_current_interval != 0000:\n",
    "            str_predicted_last_interval = str_current_interval - 800\n",
    "            if str_parsed_time_interval != str_predicted_last_interval:\n",
    "                return -1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            if str_parsed_time_interval != 1600:\n",
    "                return -1\n",
    "            else:\n",
    "                return 0\n",
    "        \n",
    "    def cron_job_level_1(self):\n",
    "        level_1_list = self.get_file_recordings()\n",
    "        int_test_result = self.analyze_for_error(level_1_list = level_1_list)\n",
    "        \n",
    "        if int_test_result != -1:\n",
    "            obj_update_level_1 = GLIEF_Update_level_1.GLEIFUpdateLevel1(bool_downloaded = False)\n",
    "            obj_update_level_1.storing_GLEIF_data_in_database()\n",
    "        else:\n",
    "            obj_backfill_level_1 = GLEIF_Backfill_Level_1.GLEIFLevel1Data(bool_downloaded = False)\n",
    "            obj_backfill_level_1.drop_table(lst_table_name = [\"GLEIF_entity_data\" , \"GLEIF_other_legal_names\" , \"GLEIF_LegalAddress\" , \"GLEIF_HeadquartersAddress\" , \"GLEIF_LegalEntityEvents\" , \"GLEIF_registration_data\" , \"GLEIF_geocoding\"])\n",
    "            obj_backfill_level_1.storing_GLEIF_data_in_database()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cron_job = CronJobLevel1()\n",
    "obj_cron_job.cron_job_level_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import os\n",
    "import sys\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "\n",
    "from Production.Backfill import GLEIF_Backfill_Relationships\n",
    "from Production.Update import GLEIF_Update_Relationships\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CronJobLevel2:\n",
    "    def __innit__(self , str_db_name = \"GLEIF_test_db\"):\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.obj_cron_job_1 = CronJobLevel1()\n",
    "    \n",
    "    def get_file_recordings(self , str_table_name = \"gleif_files_processed\"):\n",
    "        \n",
    "        \n",
    "        sql = f\"\"\"\n",
    "            SELECT DISTINCT ON (data_title)\n",
    "                time_interval, date, file_name, data_title\n",
    "            FROM {str_table_name}\n",
    "            WHERE data_title IN ('Level_2_Relationships')\n",
    "            ORDER BY data_title, date DESC, time_interval DESC\n",
    "        \"\"\"\n",
    "        self.cursor.execute(sql)\n",
    "        rows = self.cursor.fetchall()  # up to 2 rows here (one for each data_title)\n",
    "        \n",
    "        # Separate them into two lists\n",
    "        level_2_list = []\n",
    "        \n",
    "        for row in rows:\n",
    "            level_2_list.append(row)\n",
    "        \n",
    "        return level_2_list\n",
    "    \n",
    "    def cron_job_level_2(self):\n",
    "        level_2_list = self.get_file_recordings()\n",
    "        int_test_result = self.obj_cron_job_1.analyze_for_error(level_1_list = level_2_list)\n",
    "        \n",
    "        if int_test_result != -1:\n",
    "            obj_update_level_2 = GLEIF_Update_Relationships.GLEIFUpdateLevel2(bool_downloaded = False)\n",
    "            obj_update_level_2.updating_GLEIF_data_in_database()\n",
    "        else:\n",
    "            obj_backfill_level_2 = GLEIF_Backfill_Relationships.GLEIFLevel2Data(bool_downloaded = False)\n",
    "            obj_backfill_level_2.drop_table(lst_table_name = [\"GLEIF_relationship_data\" , \"GLEIF_relationship_date_data\" , \"GLEIF_relationship_qualifiers\" , \"GLEIF_relationship_quantifiers\"])\n",
    "            obj_backfill_level_2.storing_GLEIF_data_in_database()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
