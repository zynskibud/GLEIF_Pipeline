{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestingBackfillHelpers:\n",
    "    def __init__(self):\n",
    "        self.obj_backfill_helpers_level_1 = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "        self.obj_backfill_helpers_relationships = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_2_Trees = True)\n",
    "        self.obj_backfill_helpers_exceptions = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_2_Reporting_Exceptions = True)\n",
    "        \n",
    "    def random_key(self , length=5):\n",
    "        return ''.join(random.choices(string.ascii_lowercase, k=length))\n",
    "\n",
    "    def random_value(self):\n",
    "        # Random scalar value\n",
    "        if random.random() < 0.5:\n",
    "            return random.randint(0, 1000)\n",
    "        return ''.join(random.choices(string.ascii_letters, k=8))\n",
    "\n",
    "    def generate_nested_dict(self , depth=3, width=3):\n",
    "        \"\"\"\n",
    "        Recursively generate a nested dictionary.\n",
    "        \"\"\"\n",
    "        if depth == 0:\n",
    "            # Return a scalar\n",
    "            return self.random_value()\n",
    "\n",
    "        d = {}\n",
    "        for _ in range(random.randint(1, width)):\n",
    "            # Sometimes nest, sometimes scalar\n",
    "            if random.random() < 0.5:\n",
    "                d[self.random_key()] = self.generate_nested_dict(depth - 1, width)\n",
    "            else:\n",
    "                # Make a list or scalar\n",
    "                if random.random() < 0.5:\n",
    "                    d[self.random_key()] = [self.generate_nested_dict(depth - 1, width) for __ in range(random.randint(1,3))]\n",
    "                else:\n",
    "                    d[self.random_key()] = self.random_value()\n",
    "\n",
    "        return d\n",
    "\n",
    "    def count_leaf_nodes(self, d):\n",
    "        \"\"\"\n",
    "        Count the total number of leaf nodes in a nested structure of dicts/lists.\n",
    "        \"\"\"\n",
    "        if isinstance(d, dict):\n",
    "            return sum(self.count_leaf_nodes(v) for v in d.values())\n",
    "        elif isinstance(d, list):\n",
    "            return sum(self.count_leaf_nodes(i) for i in d)\n",
    "        else:\n",
    "            # Scalar\n",
    "            return 1\n",
    "    \n",
    "    def generate_dirty_keys_dict(self , num_keys=10):\n",
    "        d = {}\n",
    "        for _ in range(num_keys):\n",
    "            base_key = self.random_key()\n",
    "            # Randomly decide to make it dirty\n",
    "            if random.random() < 0.5:\n",
    "                # Add \"_$\"\n",
    "                key = base_key + \"_$\"\n",
    "            else:\n",
    "                # Insert \"@xml:lang\" somewhere\n",
    "                key = base_key.replace('_','',1) + \"@xml:lang\"\n",
    "            d[key] = self.random_value()\n",
    "\n",
    "        # Add some clean keys as well\n",
    "        for _ in range(3):\n",
    "            d[self.random_key()] = self.random_value()\n",
    "\n",
    "        return d\n",
    "\n",
    "    def generate_other_entity_names_dict(self , num=3):\n",
    "        # Create OtherEntityNames keys\n",
    "        d = {}\n",
    "        for i in range(1, num+1):\n",
    "            d[f\"OtherEntityNames_{i}_@type\"] = f\"Type{i}\"\n",
    "            d[f\"OtherEntityNames_{i}\"] = f\"Name{i}\"\n",
    "\n",
    "        # Create TranslatedOtherEntityNames keys\n",
    "        for i in range(1, num+1):\n",
    "            d[f\"TranslatedOtherEntityNames_{i}_@type\"] = f\"TransType{i}\"\n",
    "            d[f\"TranslatedOtherEntityNames_{i}\"] = f\"TransName{i}\"\n",
    "\n",
    "        return d\n",
    "    \n",
    "    def generate_prefix_dict(self , num_prefixes=3, keys_per_prefix=4):\n",
    "        prefixes = [self.random_key() for _ in range(num_prefixes)]\n",
    "        d = {}\n",
    "        for p in prefixes:\n",
    "            for _ in range(keys_per_prefix):\n",
    "                d[p + \"_\" + self.random_key()] = self.random_value()\n",
    "        return d, prefixes\n",
    "    \n",
    "    def generate_event_data_dict(self , common_base=\"Mike\", names=[\"Jerry\",\"Allen\",\"Jared\"], index=1):\n",
    "        d = {}\n",
    "        for n in names:\n",
    "            # Example key: Mike_1_{n}\n",
    "            key = f\"{common_base}_{index}_{n}\"\n",
    "            d[key] = f\"Value_for_{n}\"\n",
    "        # Add some unrelated keys\n",
    "        d[\"RandomKey\"] = \"RandomValue\"\n",
    "        return d, common_base, names\n",
    "    \n",
    "    def generate_exact_keys_dict(self):\n",
    "        d = {\n",
    "            \"Nico\": \"NicoValue\",\n",
    "            \"Matt\": \"MattValue\",\n",
    "            \"Gradient\": \"GradientValue\",\n",
    "            # Add extras\n",
    "            \"Other\": \"OtherValue\"\n",
    "        }\n",
    "        return d\n",
    "\n",
    "    def generate_substring_keys_dict(self):\n",
    "        d = {\n",
    "            \"SomeLongPath_Nico_end\": \"NicoValue\",\n",
    "            \"X_Matt_Y\": \"MattValue\",\n",
    "            \"Gradient_Something\": \"GradientValue\",\n",
    "            \"OtherKey\": \"OtherValue\"\n",
    "        }\n",
    "        return d\n",
    "    \n",
    "    def generate_split_dict(self , num_entities=3):\n",
    "        d = {}\n",
    "        for i in range(1, num_entities+1):\n",
    "            d[f\"Entity_{i}_Name\"] = f\"Name{i}\"\n",
    "            d[f\"Entity_{i}_Type\"] = f\"Type{i}\"\n",
    "        # Add a non-matching key\n",
    "        d[\"NoNumber_Key\"] = \"NoNumberValue\"\n",
    "        return d, num_entities\n",
    "    \n",
    "    \"\"\"def generate_geocoding_dict(self):\n",
    "        # Possible corner descriptors and coordinate types\n",
    "        corners = [\"TopLeft\", \"TopRight\", \"BottomLeft\", \"BottomRight\", \"Center\"]\n",
    "        coord_types = [\"Latitude\", \"Longitude\"]\n",
    "\n",
    "        # Decide how many key-value pairs in bounding box (at least 2, at most 6)\n",
    "        num_pairs = random.randint(2, 6)\n",
    "\n",
    "        bbox_pairs = []\n",
    "        for _ in range(num_pairs):\n",
    "            corner = random.choice(corners)\n",
    "            ctype = random.choice(coord_types)\n",
    "            # Random latitude/longitude values in a plausible range\n",
    "            val = round(random.uniform(-180, 180), 4)\n",
    "            bbox_pairs.append(f\"{corner}.{ctype}: {val}\")\n",
    "\n",
    "        bounding_box_value = \", \".join(bbox_pairs)\n",
    "\n",
    "        # Create a dictionary with bounding_box and some other random keys\n",
    "        d = {\n",
    "            \"bounding_box\": bounding_box_value,\n",
    "            \"SomeOtherKey\": \"SomeValue\"\n",
    "        }\n",
    "\n",
    "        # Return the dictionary and the count of pairs to verify after flattening\n",
    "        return d, num_pairs\"\"\"\n",
    "\n",
    "    def generate_geocoding_dict(self):\n",
    "        # Possible corner descriptors and coordinate types\n",
    "        corners = [\"TopLeft\", \"TopRight\", \"BottomLeft\", \"BottomRight\", \"Center\"]\n",
    "        coord_types = [\"Latitude\", \"Longitude\"]\n",
    "\n",
    "        # Create all possible (corner, coordinate) pairs\n",
    "        all_pairs = [(c, ct) for c in corners for ct in coord_types]\n",
    "\n",
    "        # Decide how many key-value pairs in bounding box (at least 2, at most number of all_pairs)\n",
    "        num_pairs = random.randint(2, len(all_pairs))\n",
    "\n",
    "        # Select unique pairs without replacement\n",
    "        selected_pairs = random.sample(all_pairs, num_pairs)\n",
    "\n",
    "        bbox_pairs = []\n",
    "        for corner, ctype in selected_pairs:\n",
    "            # Random latitude/longitude values in a plausible range\n",
    "            val = round(random.uniform(-180, 180), 4)\n",
    "            bbox_pairs.append(f\"{corner}.{ctype}: {val}\")\n",
    "\n",
    "        bounding_box_value = \", \".join(bbox_pairs)\n",
    "\n",
    "        # Create a dictionary with bounding_box and some other random keys\n",
    "        d = {\n",
    "            \"bounding_box\": bounding_box_value,\n",
    "            \"SomeOtherKey\": \"SomeValue\"\n",
    "        }\n",
    "\n",
    "        # Return the dictionary and the count of pairs to verify after flattening\n",
    "        return d, num_pairs\n",
    "        \n",
    "    #Testing the retrieval of data from data source for all three cases\n",
    "    def helper_test_get_level_download_links_level_1(self):\n",
    "        str_download_link = self.obj_backfill_helpers_level_1.get_level_download_links()\n",
    "        \n",
    "        if not str_download_link:\n",
    "            print(\"Assert False\")\n",
    "    \n",
    "    def helper_test_get_level_download_links_relationships(self):\n",
    "        str_download_link = self.obj_backfill_helpers_relationships.get_level_download_links()\n",
    "        \n",
    "        if not str_download_link:\n",
    "            print(\"Assert False\") \n",
    "\n",
    "    def helper_test_get_level_download_links_exceptions(self):\n",
    "        str_download_link = self.obj_backfill_helpers_exceptions.get_level_download_links()\n",
    "        \n",
    "        if not str_download_link:\n",
    "            print(\"Assert False\") \n",
    "    \n",
    "    def helper_test_unpacking_GLEIF_zip_files_level_1(self):\n",
    "        str_json_file_path = self.obj_backfill_helpers_level_1.unpacking_GLEIF_zip_files()\n",
    "        \n",
    "        if not str_json_file_path:\n",
    "            print(\"Assert False\")\n",
    "    \n",
    "    def helper_test_unpacking_GLEIF_zip_files_relationships(self):\n",
    "        str_json_file_path = self.obj_backfill_helpers_relationships.unpacking_GLEIF_zip_files()\n",
    "        \n",
    "        if not str_json_file_path:\n",
    "            print(\"Assert False\")\n",
    "    \n",
    "    def helper_test_unpacking_GLEIF_zip_files_exceptions(self):\n",
    "        str_json_file_path = self.obj_backfill_helpers_exceptions.unpacking_GLEIF_zip_files()\n",
    "        \n",
    "        if not str_json_file_path:\n",
    "            print(\"Assert False\")\n",
    "    \n",
    "    \n",
    "    # Testing the helper functions used to process all of the data in the rest of the backfill part of the pipeline\n",
    "    def helper_test_flatten_dict(self , obj):\n",
    "        input_dict = self.generate_nested_dict(depth=3, width=3)\n",
    "        expected_leaf_count = self.count_leaf_nodes(input_dict)\n",
    "        flattened = obj.flatten_dict(input_dict)\n",
    "        if len(flattened) != expected_leaf_count:\n",
    "            print(\"Assert False in test_flatten_dict\")\n",
    "            \n",
    "    def helper_test_clean_keys(self, obj):\n",
    "        input_dict = self.generate_dirty_keys_dict()\n",
    "        cleaned = obj.clean_keys(input_dict)\n",
    "        \n",
    "        # Check if any dirty patterns remain\n",
    "        for k in cleaned.keys():\n",
    "            if \"_$\" in k or \"@xml:lang\" in k:\n",
    "                print(\"Assert False in test_clean_keys\")\n",
    "                \n",
    "    def helper_test_organize_by_prefix(self , obj):\n",
    "        input_dict, prefixes = self.generate_prefix_dict()\n",
    "        organized = obj.organize_by_prefix(input_dict)\n",
    "        if len(organized) != len(prefixes):\n",
    "            print(\"Assert False in test_organize_by_prefix\")\n",
    "            \n",
    "    def helper_test_extract_other_entity_names(self , obj):\n",
    "        base_keyword = \"OtherEntityNames\"\n",
    "        exclude_keywords = [\"TranslatedOtherEntityNames\"]\n",
    "        input_dict = self.generate_other_entity_names_dict()\n",
    "        result = obj.extract_other_entity_names(input_dict, base_keyword, exclude_keywords)\n",
    "\n",
    "        # Check that none of the TranslatedOtherEntityNames values are included\n",
    "        # TranslatedOtherEntityNames were: TransType1, TransName1, etc.\n",
    "        for t_i in range(1,4):\n",
    "            if any(f\"TransName{t_i}\" in r for r in result) or any(f\"TransType{t_i}\" in r for r in result):\n",
    "                print(\"Assert False in test_extract_other_entity_names\")\n",
    "                \n",
    "    def helper_test_extract_event_data(self , obj):\n",
    "        # target keys we look for\n",
    "        target_keys = [\"Jerry\", \"Allen\", \"Jared\"]\n",
    "        input_dict, base_keyword, names = self.generate_event_data_dict()\n",
    "        # Run extraction\n",
    "        result = obj.extract_event_data(input_dict, base_keyword, target_keys)\n",
    "\n",
    "        # result should be a list of tuples, each tuple has values for (Jerry, Allen, Jared)\n",
    "        # Check if we got the correct values\n",
    "        if not result:  # Should not be empty\n",
    "            print(\"Assert False in test_extract_event_data\")\n",
    "            return\n",
    "        # Just check first tuple\n",
    "        first_tuple = result[0]\n",
    "        # Compare each name\n",
    "        expected = tuple(f\"Value_for_{n}\" for n in names)\n",
    "        if first_tuple != expected:\n",
    "            print(\"Assert False in test_extract_event_data\")  \n",
    "            \n",
    "    def helper_test_get_target_values_exact(self , obj):\n",
    "        input_dict = self.generate_exact_keys_dict()\n",
    "        target_keys = [\"Nico\", \"Matt\", \"Gradient\"]\n",
    "        result = obj.get_target_values(input_dict, target_keys, subset_string=False)\n",
    "        expected = [\"NicoValue\", \"MattValue\", \"GradientValue\"]\n",
    "        if result != expected:\n",
    "            print(\"Assert False in test_get_target_values_exact\")\n",
    "\n",
    "    def helper_test_get_target_values_subset(self , obj):\n",
    "        input_dict = self.generate_substring_keys_dict()\n",
    "        target_keys = [\"Nico\", \"Matt\", \"Gradient\"]\n",
    "        result = obj.get_target_values(input_dict, target_keys, subset_string=True)\n",
    "        expected = [\"NicoValue\", \"MattValue\", \"GradientValue\"]\n",
    "        if result != expected:\n",
    "            print(\"Assert False in test_get_target_values_subset\")\n",
    "            \n",
    "    def helper_test_split_into_list_of_dictionaries(self , obj):\n",
    "        input_dict, num_entities = self.generate_split_dict()\n",
    "        result = obj.split_into_list_of_dictionaries(input_dict)\n",
    "\n",
    "        # The length of result should match num_entities\n",
    "        if len(result) != num_entities:\n",
    "            print(\"Assert False in test_split_into_list_of_dictionaries\")\n",
    "            \n",
    "    def helper_test_further_flatten_geocoding(self , obj):\n",
    "        input_dict, num_pairs = self.generate_geocoding_dict()\n",
    "        result = obj.further_flatten_geocoding(input_dict)\n",
    "        # Expected keys:\n",
    "        # - bounding_box is replaced by one key per pair in bbox\n",
    "        # - plus the original non-bounding_box keys (in this case, 1: \"SomeOtherKey\")\n",
    "        expected_total_keys = num_pairs + 1\n",
    "\n",
    "        if len(result) != expected_total_keys:\n",
    "            print(\"Assert False in test_further_flatten_geocoding\")\n",
    "            \n",
    "    def testing_GLIEF_Backfill_helpers(self , test_data_retrieval = False , test_processing_helpers = True):\n",
    "        if test_data_retrieval == True:\n",
    "            self.helper_test_get_level_download_links_level_1()\n",
    "            self.helper_test_get_level_download_links_relationships()\n",
    "            self.helper_test_get_level_download_links_exceptions()\n",
    "            self.helper_test_unpacking_GLEIF_zip_files_level_1()\n",
    "            self.helper_test_unpacking_GLEIF_zip_files_relationships()\n",
    "            self.helper_test_unpacking_GLEIF_zip_files_exceptions()\n",
    "        \n",
    "        if test_processing_helpers == True:\n",
    "            self.helper_test_flatten_dict(obj = self.obj_backfill_helpers_level_1)\n",
    "            self.helper_test_clean_keys(obj = self.obj_backfill_helpers_level_1)\n",
    "            self.helper_test_organize_by_prefix(obj = self.obj_backfill_helpers_level_1)\n",
    "            self.helper_test_organize_by_prefix(obj = self.obj_backfill_helpers_level_1)\n",
    "            self.helper_test_extract_other_entity_names(obj = self.obj_backfill_helpers_level_1)\n",
    "            self.helper_test_extract_event_data(obj = self.obj_backfill_helpers_level_1)\n",
    "            self.helper_test_get_target_values_exact(obj = self.obj_backfill_helpers_level_1)\n",
    "            self.helper_test_get_target_values_subset(obj = self.obj_backfill_helpers_level_1)\n",
    "            self.helper_test_split_into_list_of_dictionaries(obj = self.obj_backfill_helpers_level_1)\n",
    "            self.helper_test_further_flatten_geocoding(obj = self.obj_backfill_helpers_level_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = TestingBackfillHelpers()\n",
    "obj.testing_GLIEF_Backfill_helpers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
