{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "import psycopg2\n",
    "import sys\n",
    "import io\n",
    "import ijson\n",
    "import concurrent.futures\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timezone\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from psycopg2 import sql\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n",
    "from Infrastructure import Scraper_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Testing_Level_1_data:\n",
    "    def __init__(self):\n",
    "        self.obj_scraper_helpers = Scraper_helpers.Scraper_Helpers()\n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers()\n",
    "    \n",
    "    def get_lei_ids(self, str_db_name=\"GLEIF_test_db\"):\n",
    "        conn = psycopg2.connect(dbname=str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        query = \"SELECT lei FROM gleif_entity_data ORDER BY lei;\"  # ORDER BY clause\n",
    "        cursor.execute(query)\n",
    "\n",
    "        column_values = [row[0] for row in cursor.fetchall()]\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        return column_values\n",
    "    \n",
    "    def list_subset_create(self , list_input , batch_size):\n",
    "        return [\n",
    "            list_input[i : i + batch_size] \n",
    "            for i in range(0, len(list_input), batch_size)\n",
    "        ]\n",
    "        \n",
    "    def fetch_lei_records_batch(self , batch_of_leis):\n",
    "        \"\"\"\n",
    "        Fetches LEI records for a batch of LEIs in a single request.\n",
    "        Returns the JSON list `[\"data\"]` if status=200,\n",
    "        otherwise returns an empty list or some error placeholder.\n",
    "        \"\"\"\n",
    "        base_url = \"https://api.gleif.org/api/v1/lei-records\"\n",
    "        lei_filter = \",\".join(batch_of_leis)\n",
    "        url = f\"{base_url}?page[size]=200&filter[lei]={lei_filter}\"\n",
    "        \n",
    "        # Perform the request using your custom function\n",
    "        response = self.obj_scraper_helpers.spotty_network(url=url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            json_data = response.json()\n",
    "            return json_data.get(\"data\", [])\n",
    "        else:\n",
    "            time.sleep(60)\n",
    "            response = self.obj_scraper_helpers.spotty_network(url=url)\n",
    "            if response.status_code == 200:\n",
    "                json_data = response.json()\n",
    "                return json_data.get(\"data\", [])\n",
    "            else:\n",
    "                print(f\"bad batch {batch_of_leis}\")\n",
    "                return None\n",
    "            \n",
    "    def multithread_lei_batches(self , list_batched_leis, batch_size=100, max_workers=10):\n",
    "        \"\"\"\n",
    "        1. Splits the LEIs into batches of `batch_size`.\n",
    "        2. Multi-threads the requests for each batch using `ThreadPoolExecutor`.\n",
    "        3. Collects `[\"data\"]` from each batch-response into one big list.\n",
    "        \"\"\"\n",
    "        # Create sub-lists (batches) of LEIs\n",
    "        list_more_batches_leis = self.list_subset_create(batch_size = batch_size , list_input = list_batched_leis)\n",
    "        \n",
    "        list_data = []\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            # Submit a batch request to the executor\n",
    "            future_to_batch = {\n",
    "                executor.submit(self.fetch_lei_records_batch, batch): batch \n",
    "                for batch in list_more_batches_leis\n",
    "            }\n",
    "            \n",
    "            # As each future completes, extend the `all_data` list with the result\n",
    "            for future in concurrent.futures.as_completed(future_to_batch):\n",
    "                batch_result = future.result()  # This should be a list of JSON items\n",
    "                list_data.extend(batch_result)\n",
    "        \n",
    "        return list_data\n",
    "    \n",
    "    \"\"\"def get_lei_data_limit(self , str_table_name, max_rows=100000):\n",
    "        conn = psycopg2.connect(dbname=\"GLEIF_test_db\", user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Use ORDER BY on the same column to match the ordering\n",
    "        query = f\"SELECT * FROM {str_table_name} ORDER BY lei LIMIT %s;\"\n",
    "        cursor.execute(query, (max_rows,))\n",
    "\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Convert each row to a list, optionally exclude the first column\n",
    "        all_data = [list(row)[1:] for row in results]\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        return all_data\"\"\"\n",
    "    \n",
    "    \n",
    "    def get_all_lei_data(self , str_table_name):\n",
    "        conn = psycopg2.connect(\n",
    "                dbname=\"GLEIF_test_db\",\n",
    "                user=\"Matthew_Pisinski\",\n",
    "                password=\"matt1\",\n",
    "                host=\"localhost\",\n",
    "                port=\"5432\"\n",
    "            )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Define the SQL query to fetch all rows, ordered by 'lei'\n",
    "        query = f\"SELECT * FROM {str_table_name} ORDER BY lei;\"\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Retrieve all results\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Convert each row to a list and exclude the first column (e.g., an ID or primary key)\n",
    "        all_data = [list(row)[1:] for row in results]\n",
    "            \n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "        \n",
    "        return all_data\n",
    "\n",
    "    def get_lei_data_for_leis(self, str_table_name, list_leis):\n",
    "        \"\"\"\n",
    "        Fetch all rows from the specified table where 'lei' is in the provided list of LEIs.\n",
    "        \n",
    "        Args:\n",
    "            str_table_name (str): Name of the database table.\n",
    "            list_leis (list of str): List of LEI IDs to fetch records for.\n",
    "        \n",
    "        Returns:\n",
    "            list of lists: Each inner list represents a row from the table, excluding the first column.\n",
    "        \"\"\"\n",
    "        if not list_leis:\n",
    "            return []\n",
    "\n",
    "        # Establish connection\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=\"GLEIF_test_db\",\n",
    "            user=\"Matthew_Pisinski\",\n",
    "            password=\"matt1\",\n",
    "            host=\"localhost\",\n",
    "            port=\"5432\"\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Dynamically create placeholders based on the number of LEIs\n",
    "        placeholders = ','.join(['%s'] * len(list_leis))\n",
    "        query = sql.SQL(\"SELECT * FROM {table} WHERE lei IN ({placeholders}) ORDER BY lei;\").format(\n",
    "            table=sql.Identifier(str_table_name),\n",
    "            placeholders=sql.SQL(placeholders)\n",
    "        )\n",
    "\n",
    "        # Execute the query with the list of LEIs as parameters\n",
    "        cursor.execute(query, list_leis)\n",
    "\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Convert each row to a list and exclude the first column if needed\n",
    "        all_data = [list(row)[1:] for row in results]\n",
    "\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        return all_data\n",
    "\n",
    "        \n",
    "    def get_dict_map(self , list_input):\n",
    "        dict_db_data = defaultdict(list)\n",
    "\n",
    "        for item in list_input:\n",
    "            dict_db_data[item[0]].append(item)\n",
    "\n",
    "    # Convert defaultdict to a regular dictionary (optional)\n",
    "        dict_db_data = dict(dict_db_data)\n",
    "\n",
    "        return dict_db_data   \n",
    "    \n",
    "    def unify_date(self , date_str):\n",
    "        \"\"\"\n",
    "        If there's a 'T', parse the date-time, normalize to UTC, and return 'YYYY-MM-DD'.\n",
    "        Otherwise, assume it's already just 'YYYY-MM-DD' and return it as-is.\n",
    "        \"\"\"\n",
    "        if date_str is None:\n",
    "            return None\n",
    "\n",
    "        # If there's no 'T', skip time-zone parsing entirely\n",
    "        if 'T' not in date_str:\n",
    "            return date_str  # e.g. \"1969-04-17\"\n",
    "\n",
    "        try:\n",
    "            # Replace 'Z' with '+00:00' so Python recognizes the time zone\n",
    "            dt = datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
    "            dt_utc = dt.astimezone(timezone.utc)\n",
    "            # Return just the date portion (YYYY-MM-DD)\n",
    "            return dt_utc.strftime('%Y-%m-%d')\n",
    "        except Exception as e:\n",
    "            print(f\"Error normalizing date '{date_str}': {e}\")\n",
    "            return date_str  # Fallback: return original string\n",
    "    \n",
    "    def clean_date_string(self , list_input , date_indexes , bool_many = False):\n",
    "        if bool_many == True:\n",
    "            for sublist in list_input:\n",
    "                for idx in date_indexes:\n",
    "                        sublist[idx] = (self.unify_date(sublist[idx]))\n",
    "        else:\n",
    "            for idx in date_indexes:\n",
    "                list_input[idx] = (self.unify_date(list_input[idx]))\n",
    "            \n",
    "        return list_input\n",
    "    \n",
    "    def extract_other_names(self , dict_record):\n",
    "        \"\"\"\n",
    "        Extracts all 'otherNames_{index}_...' fields from the dictionary\n",
    "        and returns a list of arrays, one array per 'otherNames' index.\n",
    "        Each array will include [LEI, name, language, type].\n",
    "        \"\"\"\n",
    "        \n",
    "        lei = dict_record.get(\"attributes_lei\")\n",
    "        \n",
    "        # Pattern to match keys like 'attributes_entity_otherNames_1_name'\n",
    "        pattern = re.compile(r\"^attributes_entity_otherNames_(\\d+)_(\\w+)$\")\n",
    "        \n",
    "        # Dictionary to group subfields by index\n",
    "        # e.g. subdicts['1'] = {'name': 'AUTOCONT s.r.o.', 'language': 'sk', 'type': 'PREVIOUS_LEGAL_NAME'}\n",
    "        subdicts = {}\n",
    "        \n",
    "        for key, value in dict_record.items():\n",
    "            match = pattern.match(key)\n",
    "            if match:\n",
    "                index = match.group(1)  # e.g. '1'\n",
    "                field_name = match.group(2)  # e.g. 'name', 'language', 'type'\n",
    "                \n",
    "                if index not in subdicts:\n",
    "                    subdicts[index] = {}\n",
    "                \n",
    "                subdicts[index][field_name] = value\n",
    "        \n",
    "        # Build a list of arrays, each containing the LEI + the extracted fields\n",
    "        rows = []\n",
    "        for index, fields in subdicts.items():\n",
    "            # You can decide which fields to include and in which order\n",
    "            name = fields.get(\"name\")\n",
    "            language = fields.get(\"language\")\n",
    "            name_type = fields.get(\"type\")\n",
    "            \n",
    "            row = [lei, name, language, name_type]\n",
    "            rows.append(row)\n",
    "        \n",
    "        return rows\n",
    "    \n",
    "    def sort_api_data(self , list_leis , list_unsorted):\n",
    "        lei_index_map = {lei: idx for idx, lei in enumerate(list_leis)}\n",
    "\n",
    "        # 2. Sort list_all_data_flattened using this map\n",
    "        list_all_data_flattened_sorted = sorted(\n",
    "            list_unsorted,\n",
    "            key=lambda d: lei_index_map[d[\"id\"]]\n",
    "        )\n",
    "\n",
    "        return list_all_data_flattened_sorted\n",
    "    \n",
    "    def helper_test_gleif_entity_data(self , list_dict_flat):\n",
    "        list_leis_api = [dict_api[\"id\"] for dict_api in list_dict_flat]\n",
    "        list_db_data = self.get_lei_data_for_leis(list_leis = list_leis_api , str_table_name = \"gleif_entity_data\")\n",
    "        \n",
    "        for dict_flat, list_db in tqdm(zip(list_dict_flat, list_db_data), \n",
    "                                        total=len(list_dict_flat), \n",
    "                                        desc=\"Processing Records\", \n",
    "                                        unit=\"record\"):\n",
    "            list_entity_data = self.obj_backfill_helpers.get_target_values(dict_data = dict_flat, subset_string=\"attributes\" , target_keys = [\"id\" , \"legalName_name\", \"entity_jurisdiction\", \"entity_category\", \"entity_subCategory\", \"entity_legalForm_id\", \"entity_legalForm_other\", \"entity_status\", \"entity_creationDate\", \"entity_registeredAt_id\", \"entity_registeredAs\"])\n",
    "            \n",
    "            list_entity_data = self.clean_date_string(date_indexes = [8 , 8] , list_input = list_entity_data)\n",
    "            list_db = self.clean_date_string(date_indexes = [8 , 8] , list_input = list_db)\n",
    "            \n",
    "            \"\"\"date_indexes = [8, 8]  # Specify the indexes of date fields to normalize\n",
    "\n",
    "            for idx in date_indexes:\n",
    "                list_entity_data[idx] = (self.unify_date(list_entity_data[idx]))\n",
    "                list_db[idx] = (self.unify_date(list_db[idx]))\"\"\"\n",
    "                \n",
    "            if list_entity_data != list_db:\n",
    "                print(\"Assert False entity_data\")\n",
    "            \n",
    "    def helper_test_other_names_data(self , list_dict_flat):\n",
    "        \"\"\"Testing all of the data in the gleif_other_legal_names table\"\"\"\n",
    "        \n",
    "        list_db_other_name_data = self.get_all_lei_data(str_table_name = \"gleif_other_legal_names\")\n",
    "        dict_other_name = self.get_dict_map(list_input = list_db_other_name_data)\n",
    "            \n",
    "        for dict_flat in tqdm(list_dict_flat, desc=\"Processing records\"):        \n",
    "            list_other_possible = (self.extract_other_names(dict_record = dict_flat))\n",
    "            \n",
    "            if list_other_possible:\n",
    "                for list_api_row in list_other_possible:\n",
    "                    list_api_row.pop(2)\n",
    "                    list_api_row[1], list_api_row[2] = list_api_row[2], list_api_row[1]\n",
    "                    \n",
    "                    list_db_other_name_rows = dict_other_name[list_api_row[0]]\n",
    "                    if list_api_row not in list_db_other_name_rows:\n",
    "                        print(f\"Test case failed for API row: {list_api_row}\")\n",
    "                        print(f\"Database rows for LEI {list_api_row[0]}: {list_db_other_name_rows}\")\n",
    "    \n",
    "    def helper_test_headquarters(self, list_dict_flat):\n",
    "        \"\"\"Testing all of the data in the gleif_headquartersaddress table\"\"\"\n",
    "        list_leis_api = [dict_api[\"id\"] for dict_api in list_dict_flat]\n",
    "        list_db_data = self.get_lei_data_for_leis(list_leis = list_leis_api , str_table_name = \"gleif_headquartersaddress\")\n",
    "            \n",
    "        for dict_flat, list_db in tqdm(zip(list_dict_flat, list_db_data), \n",
    "                                        total=len(list_dict_flat), \n",
    "                                        desc=\"Processing Records\", \n",
    "                                        unit=\"record\"):\n",
    "            list_hq_data = self.obj_backfill_helpers.get_target_values(dict_data = dict_flat, subset_string=\"attributes\" , target_keys = [\"id\" , \"entity_headquartersAddress_addressLines_1\", \"entity_headquartersAddress_addressLines_2\", \"entity_headquartersAddress_addressLines_3\", \"entity_headquartersAddress_addressLines_4\", \"entity_headquartersAddress_city\", \"entity_headquartersAddress_region\", \"entity_headquartersAddress_country\", \"entity_headquartersAddress_postalCode\"])\n",
    "            \n",
    "                \n",
    "            if list_hq_data != list_db:\n",
    "                print(\"Assert False entity_data\")\n",
    "    \n",
    "    def helper_test_legal_address(self , list_dict_flat):\n",
    "        \"\"\"Testing all of the data in the gleif_legaladdress table\"\"\"\n",
    "        list_leis_api = [dict_api[\"id\"] for dict_api in list_dict_flat]\n",
    "        list_db_data = self.get_lei_data_for_leis(list_leis = list_leis_api , str_table_name = \"gleif_legaladdress\")\n",
    "            \n",
    "        for dict_flat, list_db in tqdm(zip(list_dict_flat, list_db_data), \n",
    "                                        total=len(list_dict_flat), \n",
    "                                        desc=\"Processing Records\", \n",
    "                                        unit=\"record\"):\n",
    "            list_legal_address_data = self.obj_backfill_helpers.get_target_values(dict_data = dict_flat, subset_string=\"attributes\" , target_keys = [\"id\" , \"entity_legalAddress_addressLines_1\", \"entity_legalAddress_addressLines_2\", \"entity_legalAddress_addressLines_3\", \"entity_legalAddress_addressLines_4\", \"entity_legalAddress_city\", \"entity_legalAddress_region\", \"entity_legalAddress_country\", \"entity_legalAddress_postalCode\"])\n",
    "            \n",
    "                \n",
    "            if list_legal_address_data != list_db:\n",
    "                print(\"Assert False entity_data\")\n",
    "                \n",
    "    \n",
    "    \n",
    "    def helper_test_registration(self , list_dict_flat):\n",
    "        \"\"\"Testing all of the data in the gleif_registration_data table\"\"\"\n",
    "        list_leis_api = [dict_api[\"id\"] for dict_api in list_dict_flat]\n",
    "        list_db_data = self.get_lei_data_for_leis(list_leis = list_leis_api , str_table_name = \"gleif_registration_data\")\n",
    "            \n",
    "        for dict_flat, list_db in tqdm(zip(list_dict_flat, list_db_data), \n",
    "                                        total=len(list_dict_flat), \n",
    "                                        desc=\"Processing Records\", \n",
    "                                        unit=\"record\"):\n",
    "            list_registration_data = self.obj_backfill_helpers.get_target_values(dict_data = dict_flat, subset_string=\"attributes\" , target_keys = [\"id\" , \"registration_initialRegistrationDate\", \"registration_lastUpdateDate\", \"registration_status\", \"registration_nextRenewalDate\", \"registration_managingLou\", \"registration_corroborationLevel\", \"registration_validatedAt_id\", \"registration_validatedAs\"])\n",
    "            \n",
    "            list_registration_data = self.clean_date_string(bool_many = True , date_indexes = [1 , 2 , 4], list_input = list_registration_data)\n",
    "            list_db = self.clean_date_string(bool_many = True , date_indexes = [1 , 2 , 4], list_input = list_db)\n",
    "            \n",
    "            \"\"\"date_indexes = [1, 2, 4]  # Specify the indexes of date fields to normalize\n",
    "\n",
    "            for idx in date_indexes:\n",
    "                list_registration_data[idx] = (self.unify_date(list_registration_data[idx]))\n",
    "                list_db[idx] = (self.unify_date(list_db[idx]))\"\"\"\n",
    "                \n",
    "            if list_registration_data != list_db:\n",
    "                print(\"Assert False entity_data\")\n",
    "                display(list_registration_data)\n",
    "                display(list_db)\n",
    "    \n",
    "    def helper_test_legal_events(self , list_dict_flat):\n",
    "        \"\"\"Testing all of the data in the gleif_legalentityevents table\"\"\"\n",
    "        list_db_other_name_data = self.get_all_lei_data(str_table_name = \"gleif_legalentityevents\")\n",
    "        dict_legal_events = self.get_dict_map(list_input = list_db_other_name_data)\n",
    "        \n",
    "        for dict_flat in tqdm(list_dict_flat, desc=\"Processing records\"):        \n",
    "                list_api_legal_possible = self.obj_backfill_helpers.extract_event_data(bool_test= True, dict_data = dict_flat, base_keyword = \"attributes_entity_eventGroups\" , target_keys=[\"groupType\", \"status\", \"type\", \"effectiveDate\", \"recordedDate\", \"validationDocuments\"])            \n",
    "                \n",
    "                if list_api_legal_possible:\n",
    "                    \n",
    "                    str_lei = dict_flat[\"id\"]\n",
    "                    \n",
    "                    for index, sublist in enumerate(list_api_legal_possible):\n",
    "                        # Prepend str_lei to the sublist\n",
    "                        list_api_legal_possible[index] = [str_lei] + sublist\n",
    "                    \n",
    "                    list_api_legal_possible = self.clean_date_string(bool_many=True, date_indexes=[4,5], list_input= list_api_legal_possible)\n",
    "                    \n",
    "                    \n",
    "                    for list_api_legal in list_api_legal_possible:\n",
    "                        \n",
    "                        list_db_legal_rows = dict_legal_events[str_lei]\n",
    "                        \n",
    "                        list_db_legal_rows = self.clean_date_string(bool_many=True, date_indexes=[4,5], list_input=list_db_legal_rows)\n",
    "                        \n",
    "                        if list_api_legal not in list_db_legal_rows:\n",
    "                            print(f\"Test case failed for API row: {list_api_legal}\")\n",
    "                            print(f\"Database rows for LEI {list_api_legal[0]}: {list_db_legal_rows}\")\n",
    "                            \n",
    "    def testing_level_1_data(self , subset_size = 100000):\n",
    "        list_leis = self.get_lei_ids()\n",
    "        \n",
    "        list_batched = self.list_subset_create(list_input = list_leis , batch_size = subset_size)\n",
    "        #int_total_num_subsets = math.ceil(len(list_leis)/list_batched)\n",
    "        \n",
    "        for list_leis in list_batched:\n",
    "            list_all_data = self.multithread_lei_batches(list_leis = list_leis , batch_size = 200 , max_workers = 5) \n",
    "            list_all_data_flattened = [self.obj_backfill_helpers.flatten_dict(dict_input = dict_data) for dict_data in list_all_data]\n",
    "            list_all_data_flattened_sorted = self.sort_api_data(list_leis = list_leis , list_unsorted = list_all_data_flattened)\n",
    "            self.helper_test_gleif_entity_data(list_dict_flat = list_all_data_flattened_sorted)\n",
    "            self.helper_test_other_names_data(list_dict_flat = list_all_data_flattened_sorted)\n",
    "            self.helper_test_headquarters(list_dict_flat = list_all_data_flattened_sorted)\n",
    "            self.helper_test_legal_address(list_dict_flat = list_all_data_flattened_sorted)\n",
    "            self.helper_test_registration(list_dict_flat = list_all_data_flattened_sorted)\n",
    "            self.helper_test_legal_events(list_dict_flat = list_all_data_flattened_sorted)\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_testing_level_1_data = Testing_Level_1_data()\n",
    "obj_testing_level_1_data.testing_level_1_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_scraper_helpers = Scraper_helpers.Scraper_Helpers()\n",
    "obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers()\n",
    "obj_testing_level_1_data = Testing_Level_1_data()\n",
    "\n",
    "list_leis = obj_testing_level_1_data.get_lei_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2800023"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(len(list_leis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = obj_testing_level_1_data.get_lei_data_limit(str_table_name = 'gleif_entity_data', max_rows=100000)\n",
    "display(all_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(list_leis[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_db_data = obj_testing_level_1_data.get_lei_data_limit(str_table_name = \"gleif_other_legal_names\")\n",
    "display(list_db_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(obj_testing_level_1_data.get_dict_map(list_input = list_db_data))\n",
    "dict_db_data = obj_testing_level_1_data.get_dict_map(list_input = list_db_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dict_db_data[\"2138001DVAHXYVS7AM84\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing_level_1_data(subset_size = 100000):\n",
    "    list_leis = obj_testing_level_1_data.get_lei_ids()\n",
    "    \n",
    "    list_batched = obj_testing_level_1_data.list_subset_create(list_input = list_leis , batch_size = subset_size)\n",
    "    #int_total_num_subsets = math.ceil(len(list_leis)/list_batched)\n",
    "    \n",
    "    for list_leis in list_batched:\n",
    "        list_all_data = obj_testing_level_1_data.multithread_lei_batches(list_leis = list_leis , batch_size = 200 , max_workers = 5) \n",
    "        list_all_data_flattened = [obj_backfill_helpers.flatten_dict(dict_input = dict_data) for dict_data in list_all_data]\n",
    "        list_all_data_flattened_sorted = obj_testing_level_1_data.sort_api_data(list_leis = list_leis , list_unsorted = list_all_data_flattened)\n",
    "        obj_testing_level_1_data.helper_test_gleif_entity_data(list_dict_flat = list_all_data_flattened_sorted)\n",
    "        obj_testing_level_1_data.helper_test_other_names_data(list_dict_flat = list_all_data_flattened_sorted)\n",
    "        obj_testing_level_1_data.helper_test_headquarters(list_dict_flat = list_all_data_flattened_sorted)\n",
    "        obj_testing_level_1_data.helper_test_legal_address(list_dict_flat = list_all_data_flattened_sorted)\n",
    "        obj_testing_level_1_data.helper_test_registration(list_dict_flat = list_all_data_flattened_sorted)\n",
    "        obj_testing_level_1_data.helper_test_legal_events(list_dict_flat = list_all_data_flattened_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_data = obj_testing_level_1_data.multithread_lei_batches(list_batched_leis = list_leis[0:100000] , batch_size = 200 , max_workers = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(len(list_all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_all_data_flattened = [obj_backfill_helpers.flatten_dict(dict_input = dict_data) for dict_data in list_all_data]\n",
    "list_all_data_flattened_sorted = obj_testing_level_1_data.sort_api_data(list_leis = list_leis[0:100000] , list_unsorted = list_all_data_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dict_flat.pickle\" , \"wb\") as file:\n",
    "    pickle.dump(list_all_data_flattened_sorted , file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dict_flat.pickle\" , \"rb\") as file:\n",
    "    list_flat_use_case = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'lei-records',\n",
       " 'id': '00TR8NKAEL48RGTZEW89',\n",
       " 'attributes_lei': '00TR8NKAEL48RGTZEW89',\n",
       " 'attributes_entity_legalName_name': 'ESG DOMESTIC OPPORTUNITY OFFSHORE FUND LTD.',\n",
       " 'attributes_entity_legalName_language': 'en',\n",
       " 'attributes_entity_legalAddress_language': 'en',\n",
       " 'attributes_entity_legalAddress_addressLines_1': '89 Nexus Way',\n",
       " 'attributes_entity_legalAddress_addressNumber': None,\n",
       " 'attributes_entity_legalAddress_addressNumberWithinBuilding': None,\n",
       " 'attributes_entity_legalAddress_mailRouting': None,\n",
       " 'attributes_entity_legalAddress_city': 'Camana Bay',\n",
       " 'attributes_entity_legalAddress_region': None,\n",
       " 'attributes_entity_legalAddress_country': 'KY',\n",
       " 'attributes_entity_legalAddress_postalCode': 'KY1-9009',\n",
       " 'attributes_entity_headquartersAddress_language': 'en',\n",
       " 'attributes_entity_headquartersAddress_addressLines_1': 'C/O Citco Fund Services (Curacao) B.V.',\n",
       " 'attributes_entity_headquartersAddress_addressLines_2': 'Kaya Flamboyan 9',\n",
       " 'attributes_entity_headquartersAddress_addressNumber': None,\n",
       " 'attributes_entity_headquartersAddress_addressNumberWithinBuilding': None,\n",
       " 'attributes_entity_headquartersAddress_mailRouting': None,\n",
       " 'attributes_entity_headquartersAddress_city': 'Willemstad',\n",
       " 'attributes_entity_headquartersAddress_region': None,\n",
       " 'attributes_entity_headquartersAddress_country': 'CW',\n",
       " 'attributes_entity_headquartersAddress_postalCode': None,\n",
       " 'attributes_entity_registeredAt_id': 'RA000087',\n",
       " 'attributes_entity_registeredAt_other': None,\n",
       " 'attributes_entity_registeredAs': '574206',\n",
       " 'attributes_entity_jurisdiction': 'KY',\n",
       " 'attributes_entity_category': 'GENERAL',\n",
       " 'attributes_entity_legalForm_id': '9999',\n",
       " 'attributes_entity_legalForm_other': 'CAYMAN ISLANDS ORDINARY NON-RESIDENT COMPANY',\n",
       " 'attributes_entity_associatedEntity_lei': None,\n",
       " 'attributes_entity_associatedEntity_name': None,\n",
       " 'attributes_entity_status': 'INACTIVE',\n",
       " 'attributes_entity_expiration_date': None,\n",
       " 'attributes_entity_expiration_reason': None,\n",
       " 'attributes_entity_successorEntity_lei': None,\n",
       " 'attributes_entity_successorEntity_name': None,\n",
       " 'attributes_entity_creationDate': None,\n",
       " 'attributes_entity_subCategory': None,\n",
       " 'attributes_entity_eventGroups_1_groupType': 'STANDALONE',\n",
       " 'attributes_entity_eventGroups_1_events_1_validationDocuments': 'SUPPORTING_DOCUMENTS',\n",
       " 'attributes_entity_eventGroups_1_events_1_effectiveDate': '2018-11-08T23:00:00Z',\n",
       " 'attributes_entity_eventGroups_1_events_1_recordedDate': '2022-03-24T19:41:39Z',\n",
       " 'attributes_entity_eventGroups_1_events_1_type': 'DISSOLUTION',\n",
       " 'attributes_entity_eventGroups_1_events_1_status': 'COMPLETED',\n",
       " 'attributes_registration_initialRegistrationDate': '2012-08-24T18:53:00Z',\n",
       " 'attributes_registration_lastUpdateDate': '2024-12-10T15:57:57Z',\n",
       " 'attributes_registration_status': 'RETIRED',\n",
       " 'attributes_registration_nextRenewalDate': '2019-02-14T00:32:00Z',\n",
       " 'attributes_registration_managingLou': '39120001KULK7200U106',\n",
       " 'attributes_registration_corroborationLevel': 'FULLY_CORROBORATED',\n",
       " 'attributes_registration_validatedAt_id': 'RA000087',\n",
       " 'attributes_registration_validatedAt_other': None,\n",
       " 'attributes_registration_validatedAs': '574206',\n",
       " 'attributes_bic': None,\n",
       " 'attributes_mic': None,\n",
       " 'attributes_ocid': None,\n",
       " 'attributes_spglobal_1': '217633137',\n",
       " 'attributes_conformityFlag': 'NOT_APPLICABLE',\n",
       " 'relationships_managing-lou_links_related': 'https://api.gleif.org/api/v1/lei-records/00TR8NKAEL48RGTZEW89/managing-lou',\n",
       " 'relationships_lei-issuer_links_related': 'https://api.gleif.org/api/v1/lei-records/00TR8NKAEL48RGTZEW89/lei-issuer',\n",
       " 'relationships_field-modifications_links_related': 'https://api.gleif.org/api/v1/lei-records/00TR8NKAEL48RGTZEW89/field-modifications',\n",
       " 'relationships_direct-parent_links_reporting-exception': 'https://api.gleif.org/api/v1/lei-records/00TR8NKAEL48RGTZEW89/direct-parent-reporting-exception',\n",
       " 'relationships_ultimate-parent_links_reporting-exception': 'https://api.gleif.org/api/v1/lei-records/00TR8NKAEL48RGTZEW89/ultimate-parent-reporting-exception',\n",
       " 'links_self': 'https://api.gleif.org/api/v1/lei-records/00TR8NKAEL48RGTZEW89'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(list_flat_use_case[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['STANDALONE',\n",
       "  'COMPLETED',\n",
       "  'DISSOLUTION',\n",
       "  '2018-11-08T23:00:00Z',\n",
       "  '2022-03-24T19:41:39Z',\n",
       "  'SUPPORTING_DOCUMENTS']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(obj_backfill_helpers.extract_event_data(bool_test= True, dict_data = list_flat_use_case[6] , base_keyword = \"attributes_entity_eventGroups\" , target_keys=[\"groupType\", \"status\", \"type\", \"effectiveDate\", \"recordedDate\", \"validationDocuments\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Records: 100%|██████████| 100000/100000 [00:05<00:00, 16798.99record/s]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from psycopg2 import sql\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_lei_data_for_leis(self, str_table_name, list_leis):\n",
    "    \"\"\"\n",
    "    Fetch all rows from the specified table where 'lei' is in the provided list of LEIs.\n",
    "    \n",
    "    Args:\n",
    "        str_table_name (str): Name of the database table.\n",
    "        list_leis (list of str): List of LEI IDs to fetch records for.\n",
    "    \n",
    "    Returns:\n",
    "        list of lists: Each inner list represents a row from the table, excluding the first column.\n",
    "    \"\"\"\n",
    "    if not list_leis:\n",
    "        return []\n",
    "\n",
    "    # Establish connection\n",
    "    conn = psycopg2.connect(\n",
    "        dbname=\"GLEIF_test_db\",\n",
    "        user=\"Matthew_Pisinski\",\n",
    "        password=\"matt1\",\n",
    "        host=\"localhost\",\n",
    "        port=\"5432\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Dynamically create placeholders based on the number of LEIs\n",
    "    placeholders = ','.join(['%s'] * len(list_leis))\n",
    "    query = sql.SQL(\"SELECT * FROM {table} WHERE lei IN ({placeholders}) ORDER BY lei;\").format(\n",
    "        table=sql.Identifier(str_table_name),\n",
    "        placeholders=sql.SQL(placeholders)\n",
    "    )\n",
    "\n",
    "    # Execute the query with the list of LEIs as parameters\n",
    "    cursor.execute(query, list_leis)\n",
    "\n",
    "    results = cursor.fetchall()\n",
    "\n",
    "    # Convert each row to a list and exclude the first column if needed\n",
    "    all_data = [list(row)[1:] for row in results]\n",
    "\n",
    "\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    return all_data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
