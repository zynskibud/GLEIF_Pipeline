{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Configure Chrome to download files automatically\n",
    "        download_directory = str_download_path  # Change to your desired directory\n",
    "\n",
    "        options = Options()\n",
    "        prefs = {\n",
    "            \"download.default_directory\": download_directory,  # Set default download directory\n",
    "            \"download.prompt_for_download\": False,  # Disable download prompt\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"safebrowsing.enabled\": True,  # Disable safety prompts\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "        # Start the driver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel1Data:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        \n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Backfill_level_1.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            str_level_1_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "            self.str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link , str_unpacked_zip_file_name = \"Level_1_unpacked\" , str_zip_file_path_name = \"Level_1.zip\")\n",
    "    \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/Level_1_unpacked\")[0]\n",
    "        self.str_json_file_path = rf\"../file_lib/Level_1_unpacked\" + \"//\" + str_unpacked_zip_file_name\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def delete_table(self, table_name):\n",
    "        \"\"\"\n",
    "        Deletes a single table from the PostgreSQL database.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to delete.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Sanitize table name to prevent SQL injection\n",
    "            drop_query = sql.SQL(\"DROP TABLE IF EXISTS {table} CASCADE;\").format(\n",
    "                table=sql.Identifier(table_name)\n",
    "            )\n",
    "\n",
    "            # Execute the DROP TABLE command\n",
    "            self.cursor.execute(drop_query)\n",
    "\n",
    "            # Log and print success message\n",
    "            logging.info(f\"Successfully deleted table: {table_name}\")\n",
    "            print(f\"Successfully deleted table: {table_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log and print error message\n",
    "            logging.error(f\"Error deleting table {table_name}: {e}\")\n",
    "            print(f\"Error deleting table {table_name}: {e}\")\n",
    "\n",
    "    \n",
    "    def create_tables(self):\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_entity_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                LegalName TEXT,\n",
    "                LegalJurisdiction TEXT,\n",
    "                EntityCategory TEXT,\n",
    "                EntitySubCategory TEXT,\n",
    "                LegalForm_EntityLegalFormCode TEXT,\n",
    "                LegalForm_OtherLegalForm TEXT,\n",
    "                EntityStatus TEXT,\n",
    "                EntityCreationDate TEXT,\n",
    "                RegistrationAuthority_RegistrationAuthorityID TEXT,\n",
    "                RegistrationAuthority_RegistrationAuthorityEntityID TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_other_legal_names (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                OtherEntityNames TEXT,\n",
    "                Type TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_LegalAddress (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                LegalAddress_FirstAddressLine TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_1 TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_2 TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_3 TEXT,\n",
    "                LegalAddress_City TEXT,\n",
    "                LegalAddress_Region TEXT,\n",
    "                LegalAddress_Country TEXT,\n",
    "                LegalAddress_PostalCode TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_HeadquartersAddress (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                HeadquartersAddress_FirstAddressLine TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_1 TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_2 TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_3 TEXT,\n",
    "                HeadquartersAddress_City TEXT,\n",
    "                HeadquartersAddress_Region TEXT,\n",
    "                HeadquartersAddress_Country TEXT,\n",
    "                HeadquartersAddress_PostalCode TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "                        \n",
    "        # LegalEntityEvents\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_LegalEntityEvents (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                group_type TEXT,\n",
    "                event_status TEXT,\n",
    "                LegalEntityEventType TEXT,\n",
    "                LegalEntityEventEffectiveDate TEXT,\n",
    "                LegalEntityEventRecordedDate TEXT,\n",
    "                ValidationDocuments TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Registration Data\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_registration_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                InitialRegistrationDate TEXT,\n",
    "                LastUpdateDate TEXT,\n",
    "                RegistrationStatus TEXT,\n",
    "                NextRenewalDate TEXT,\n",
    "                ManagingLOU TEXT,\n",
    "                ValidationSources TEXT,\n",
    "                ValidationAuthorityID TEXT,\n",
    "                ValidationAuthorityEntityID TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Geoencoding\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_geocoding (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                relevance TEXT,\n",
    "                match_type TEXT,\n",
    "                lat TEXT,\n",
    "                lng TEXT,\n",
    "                geocoding_date TEXT,\n",
    "                TopLeft_Latitude TEXT,\n",
    "                TopLeft_Longitude TEXT,\n",
    "                BottomRight_Latitude TEXT,\n",
    "                BottomRight_longitude TEXT,\n",
    "                match_level TEXT,\n",
    "                mapped_street TEXT,\n",
    "                mapped_housenumber TEXT,\n",
    "                mapped_postalcode TEXT,\n",
    "                mapped_city TEXT,\n",
    "                mapped_district TEXT,\n",
    "                mapped_state TEXT,\n",
    "                mapped_country TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        self.conn.commit()\n",
    "\n",
    "        \n",
    "    #copy from other code\n",
    "\n",
    "    def load_batches_as_list(self , file_path , batch_size=100000):\n",
    "        \"\"\"Yield records in batches as lists of dictionaries.\"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Create an iterator for the 'records' key\n",
    "            records = ijson.items(file, \"records.item\")\n",
    "            \n",
    "            batch = []\n",
    "            for index, record in enumerate(records, start=1):\n",
    "                batch.append(record)  # Add record to the batch\n",
    "                \n",
    "                if index % batch_size == 0:  # Yield the batch when size is reached\n",
    "                    yield batch  # Yield the batch as a list\n",
    "                    batch = []  # Reset for the next batch\n",
    "            \n",
    "            # Yield any remaining records\n",
    "            if batch:\n",
    "                yield batch\n",
    "    \n",
    "    def bulk_insert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"Perform a bulk insert using PostgreSQL COPY with an in-memory buffer\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to insert into\n",
    "            columns (list): List of column names for the table\n",
    "            data (list): List of tuples with the data to be inserted\n",
    "        \"\"\"\n",
    "\n",
    "        buffer = io.StringIO()\n",
    "\n",
    "        for row in data:\n",
    "            # Escape backslashes and replace None with \\N for NULL\n",
    "            row_converted = []\n",
    "            for x in row:\n",
    "                if x is None:\n",
    "                    row_converted.append('\\\\N')  # NULL representation\n",
    "                elif isinstance(x, str):\n",
    "                    x = x.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "                    row_converted.append(x)\n",
    "                else:\n",
    "                    row_converted.append(str(x))\n",
    "            buffer.write('\\t'.join(row_converted) + '\\n')\n",
    "        \n",
    "        buffer.seek(0)  # Reset buffer position\n",
    "\n",
    "        # Construct the COPY query\n",
    "        copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH (FORMAT text, DELIMITER E'\\t', NULL '\\\\N')\"\n",
    "        self.cursor.copy_expert(copy_query, buffer)\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def process_entity_data(self , list_dict_records):\n",
    "        list_entity_meta_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , subset_string = \"Entity\" , target_keys = [\"LegalName\", \"LegalJurisdiction\", \"EntityCategory\", \"EntitySubCategory\", \"LegalForm_EntityLegalFormCode\", \"LegalForm_OtherLegalForm\", \"EntityStatus\", \"EntityCreationDate\", \"RegistrationAuthority_RegistrationAuthorityID\", \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_entity_meta_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_insert_using_copy(data = list_entity_meta_data_tuples , table_name = \"GLEIF_entity_data\" , \n",
    "                            columns = \n",
    "                            [\"lei\",\n",
    "                                \"LegalName\",\n",
    "                                \"LegalJurisdiction\",\n",
    "                                \"EntityCategory\",\n",
    "                                \"EntitySubCategory\",\n",
    "                                \"LegalForm_EntityLegalFormCode\",\n",
    "                                \"LegalForm_OtherLegalForm\",\n",
    "                                \"EntityStatus\",\n",
    "                                \"EntityCreationDate\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "    \n",
    "    def process_other_legal_names(self , list_dict_records):\n",
    "        list_other_names_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_other_entity_names(data_dict = dict_entity, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup         \n",
    "            list_other_names_tuples.extend(list_output)\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_other_names_tuples , table_name = \"GLEIF_other_legal_names\" , columns = [\"lei\", \"OtherEntityNames\", \"Type\"])        \n",
    "    \n",
    "    def process_legal_address(self , list_dict_records):\n",
    "        list_legal_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , target_keys = [\"Entity_LegalAddress_FirstAddressLine\" , \"Entity_LegalAddress_AdditionalAddressLine_1\" , \"Entity_LegalAddress_AdditionalAddressLine_2\" , \"Entity_LegalAddress_AdditionalAddressLine_3\" , \"Entity_LegalAddress_City\" , \"Entity_LegalAddress_Region\" , \"Entity_LegalAddress_Country\" , \"Entity_LegalAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_legal_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_legal_address_tuples , table_name = \"GLEIF_LegalAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"LegalAddress_FirstAddressLine\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                                            \"LegalAddress_City\",\n",
    "                                            \"LegalAddress_Region\",\n",
    "                                            \"LegalAddress_Country\",\n",
    "                                            \"LegalAddress_PostalCode\"])  \n",
    "    \n",
    "    def process_headquarters_address(self , list_dict_records):\n",
    "        list_headquarters_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity , target_keys = [\"HeadquartersAddress_FirstAddressLine\" , \"HeadquartersAddress_AdditionalAddressLine_1\" , \"HeadquartersAddress_AdditionalAddressLine_2\" , \"HeadquartersAddress_AdditionalAddressLine_3\" , \"HeadquartersAddress_City\" , \"HeadquartersAddress_Region\" , \"HeadquartersAddress_Country\" , \"HeadquartersAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_headquarters_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_headquarters_address_tuples , table_name = \"GLEIF_HeadquartersAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"HeadquartersAddress_FirstAddressLine\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                                            \"HeadquartersAddress_City\",\n",
    "                                            \"HeadquartersAddress_Region\",\n",
    "                                            \"HeadquartersAddress_Country\",\n",
    "                                            \"HeadquartersAddress_PostalCode\"]) \n",
    "    \n",
    "    def process_legal_entity_events(self , list_dict_records):\n",
    "        list_legal_entity_events_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_event_data(dict_data = dict_entity , base_keyword=\"LegalEntityEvents\" , target_keys=[\"group_type\", \"event_status\", \"LegalEntityEventType\", \"LegalEntityEventEffectiveDate\", \"LegalEntityEventRecordedDate\", \"ValidationDocuments\"])\n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup \n",
    "            list_legal_entity_events_tuples.extend(list_output)\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_legal_entity_events_tuples , table_name = \"GLEIF_LegalEntityEvents\" , \n",
    "                                columns = [\"lei\",\n",
    "                                        \"group_type\",\n",
    "                                        \"event_status\",\n",
    "                                        \"LegalEntityEventType\",\n",
    "                                        \"LegalEntityEventEffectiveDate\",\n",
    "                                        \"LegalEntityEventRecordedDate\",\n",
    "                                        \"ValidationDocuments\"])\n",
    "        \n",
    "    def process_registration_data(self , list_dict_records):\n",
    "        list_registration_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_registration = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Registration\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_registration , target_keys = [\"InitialRegistrationDate\" , \"LastUpdateDate\" , \"RegistrationStatus\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationAuthority_ValidationAuthorityID\" , \"ValidationAuthority_ValidationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_registration_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_registration_tuples , table_name = \"GLEIF_registration_data\" , \n",
    "                                columns = [\n",
    "                                            \"lei\",\n",
    "                                            \"InitialRegistrationDate\",\n",
    "                                            \"LastUpdateDate\",\n",
    "                                            \"RegistrationStatus\",\n",
    "                                            \"NextRenewalDate\",\n",
    "                                            \"ManagingLOU\",\n",
    "                                            \"ValidationSources\",\n",
    "                                            \"ValidationAuthorityID\",\n",
    "                                            \"ValidationAuthorityEntityID\"]) \n",
    "    \n",
    "    def process_geoencoding_data(self , list_dict_records):\n",
    "        list_extension_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_extension = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Extension\"]\n",
    "            dict_mega_flat = self.obj_backfill_helpers.further_flatten_geocoding(dict_data = dict_extension)\n",
    "            if any(re.search(r\"_\\d+_\", key) for key in dict_mega_flat.keys()):\n",
    "                list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data = dict_mega_flat)\n",
    "                for dict_extension in list_dicts:\n",
    "                    list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_extension , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                    list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                    list_extension_data_tuples.append(tuple(list_output))\n",
    "            else:\n",
    "                list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_mega_flat , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                list_extension_data_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_extension_data_tuples , table_name = \"GLEIF_geocoding\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"relevance\",\n",
    "                                            \"match_type\",\n",
    "                                            \"lat\",\n",
    "                                            \"lng\",\n",
    "                                            \"geocoding_date\",\n",
    "                                            \"TopLeft_Latitude\",\n",
    "                                            \"TopLeft_Longitude\",\n",
    "                                            \"BottomRight_Latitude\",\n",
    "                                            \"BottomRight_longitude\",\n",
    "                                            \"match_level\",\n",
    "                                            \"mapped_street\",\n",
    "                                            \"mapped_housenumber\",\n",
    "                                            \"mapped_postalcode\",\n",
    "                                            \"mapped_city\",\n",
    "                                            \"mapped_district\",\n",
    "                                            \"mapped_state\",\n",
    "                                            \"mapped_country\"]) \n",
    "    \n",
    "    def process_all_data(self , list_dict_records):\n",
    "        self.process_entity_data(list_dict_records = list_dict_records)\n",
    "        self.process_other_legal_names(list_dict_records = list_dict_records)\n",
    "        self.process_legal_address(list_dict_records = list_dict_records)\n",
    "        self.process_headquarters_address(list_dict_records = list_dict_records)\n",
    "        self.process_legal_entity_events(list_dict_records = list_dict_records)\n",
    "        self.process_registration_data(list_dict_records = list_dict_records)\n",
    "        self.process_geoencoding_data(list_dict_records = list_dict_records)\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        self.create_tables()\n",
    "        \n",
    "        generator_batched_json = self.load_batches_as_list(file_path = self.str_json_file_path , batch_size = 100000)\n",
    "        \n",
    "        for index , list_dict_records in enumerate(generator_batched_json):\n",
    "            self.process_all_data(list_dict_records = list_dict_records)\n",
    "        \n",
    "        self.conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import json\n",
    "import psycopg2\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "from psycopg2 import sql\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "\n",
    "from Production.Update import GLEIF_Update_Helpers\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n",
    "\n",
    "class GLEIFUpdateLevel1:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        self.obj_update_helpers = GLEIF_Update_Helpers.GLEIF_Update_Helpers(bool_Level_1 = True)\n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Update_level_2.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            self.obj_update_helpers.download_on_machine()\n",
    "            self.str_json_file_path = self.obj_update_helpers.unpacking_GLEIF_zip_files()\n",
    "    \n",
    "        self.str_json_file_path = '../file_lib/Level_1_update_unpacked\\\\20241130-0000-gleif-goldencopy-lei2-intra-day.json'\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        #self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "    def bulk_upsert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"\n",
    "        Perform a bulk upsert using PostgreSQL COPY with a temporary table.\n",
    "        \n",
    "        Args:\n",
    "            table_name (str): Name of the target table.\n",
    "            columns (list): List of column names.\n",
    "            data (list): List of tuples containing the data to upsert.\n",
    "        \"\"\"\n",
    "        temp_table = f\"{table_name}_temp\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Create a temporary table\n",
    "            create_temp_table_query = f\"\"\"\n",
    "                CREATE TEMP TABLE {temp_table} (LIKE {table_name} INCLUDING ALL)\n",
    "                ON COMMIT DROP;\n",
    "            \"\"\"\n",
    "            self.cursor.execute(create_temp_table_query)\n",
    "            logging.info(f\"Temporary table {temp_table} created.\")\n",
    "\n",
    "            # Step 2: Copy data into the temporary table\n",
    "            buffer = io.StringIO()\n",
    "            for row in data:\n",
    "                # Escape backslashes, tabs, and newlines; replace None with \\N\n",
    "                row_converted = []\n",
    "                for item in row:\n",
    "                    if item is None:\n",
    "                        row_converted.append('\\\\N')\n",
    "                    elif isinstance(item, str):\n",
    "                        item = item.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "                        row_converted.append(item)\n",
    "                    else:\n",
    "                        row_converted.append(str(item))\n",
    "                buffer.write('\\t'.join(row_converted) + '\\n')\n",
    "            buffer.seek(0)\n",
    "\n",
    "            copy_query = f\"\"\"\n",
    "                COPY {temp_table} ({', '.join(columns)})\n",
    "                FROM STDIN WITH (FORMAT text, DELIMITER E'\\\\t', NULL '\\\\N')\n",
    "            \"\"\"\n",
    "            self.cursor.copy_expert(copy_query, buffer)\n",
    "            logging.info(f\"Data copied to temporary table {temp_table}.\")\n",
    "\n",
    "            # Step 3: Perform upsert from temporary table to target table\n",
    "            update_columns = [col for col in columns if col != \"lei\"]\n",
    "            set_clause = \", \".join([f\"{col}=EXCLUDED.{col}\" for col in update_columns])\n",
    "\n",
    "            upsert_query = f\"\"\"\n",
    "                INSERT INTO {table_name} ({', '.join(columns)})\n",
    "                SELECT {', '.join(columns)} FROM {temp_table}\n",
    "                ON CONFLICT (lei) DO UPDATE SET\n",
    "                    {set_clause};\n",
    "            \"\"\"\n",
    "            self.cursor.execute(upsert_query)\n",
    "            logging.info(f\"Upsert operation completed for table {table_name}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            logging.error(f\"Error during bulk upsert for table {table_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def bulk_insert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"\n",
    "        Perform a bulk insert using PostgreSQL COPY.\n",
    "        \n",
    "        Args:\n",
    "            table_name (str): Name of the target table.\n",
    "            columns (list): List of column names.\n",
    "            data (list): List of tuples containing the data to insert.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            buffer = io.StringIO()\n",
    "            for row in data:\n",
    "                # Escape backslashes, tabs, and newlines; replace None with \\N\n",
    "                row_converted = []\n",
    "                for item in row:\n",
    "                    if item is None:\n",
    "                        row_converted.append('\\\\N')\n",
    "                    elif isinstance(item, str):\n",
    "                        item = item.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "                        row_converted.append(item)\n",
    "                    else:\n",
    "                        row_converted.append(str(item))\n",
    "                buffer.write('\\t'.join(row_converted) + '\\n')\n",
    "            buffer.seek(0)\n",
    "\n",
    "            copy_query = f\"\"\"\n",
    "                COPY {table_name} ({', '.join(columns)})\n",
    "                FROM STDIN WITH (FORMAT text, DELIMITER E'\\\\t', NULL '\\\\N')\n",
    "            \"\"\"\n",
    "            self.cursor.copy_expert(copy_query, buffer)\n",
    "            logging.info(f\"Bulk insert completed for table {table_name}.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            logging.error(f\"Error during bulk insert for table {table_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def process_entity_data(self, list_dict_records):\n",
    "        list_entity_meta_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                dict_data=dict_clean,\n",
    "                subset_string=\"Entity\",\n",
    "                target_keys=[\n",
    "                    \"LegalName\",\n",
    "                    \"LegalJurisdiction\",\n",
    "                    \"EntityCategory\",\n",
    "                    \"EntitySubCategory\",\n",
    "                    \"LegalForm_EntityLegalFormCode\",\n",
    "                    \"LegalForm_OtherLegalForm\",\n",
    "                    \"EntityStatus\",\n",
    "                    \"EntityCreationDate\",\n",
    "                    \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                    \"RegistrationAuthority_RegistrationAuthorityEntityID\"\n",
    "                ]\n",
    "            )\n",
    "            list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "            list_entity_meta_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_entity_meta_data_tuples,\n",
    "            table_name=\"GLEIF_entity_data\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"LegalName\",\n",
    "                \"LegalJurisdiction\",\n",
    "                \"EntityCategory\",\n",
    "                \"EntitySubCategory\",\n",
    "                \"LegalForm_EntityLegalFormCode\",\n",
    "                \"LegalForm_OtherLegalForm\",\n",
    "                \"EntityStatus\",\n",
    "                \"EntityCreationDate\",\n",
    "                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                \"RegistrationAuthority_RegistrationAuthorityEntityID\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_other_legal_names(self, list_dict_records):\n",
    "        list_other_names_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            dict_entity = self.obj_backfill_helpers.organize_by_prefix(dict_clean).get(\"Entity\", {})\n",
    "            list_output = self.obj_backfill_helpers.extract_other_entity_names(\n",
    "                data_dict=dict_entity,\n",
    "                base_keyword=\"OtherEntityNames\",\n",
    "                exclude_keywords=[\"TranslatedOtherEntityNames\"]\n",
    "            )\n",
    "            for tup in list_output:\n",
    "                list_other_names_tuples.append((dict_clean.get(\"LEI\"),) + tup)\n",
    "        \n",
    "        self.bulk_insert_using_copy(\n",
    "            data=list_other_names_tuples,\n",
    "            table_name=\"GLEIF_other_legal_names\",\n",
    "            columns=[\"lei\", \"OtherEntityNames\", \"Type\"]\n",
    "        )\n",
    "\n",
    "    def process_legal_address(self, list_dict_records):\n",
    "        list_legal_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                dict_data=dict_clean,\n",
    "                target_keys=[\n",
    "                    \"Entity_LegalAddress_FirstAddressLine\",\n",
    "                    \"Entity_LegalAddress_AdditionalAddressLine_1\",\n",
    "                    \"Entity_LegalAddress_AdditionalAddressLine_2\",\n",
    "                    \"Entity_LegalAddress_AdditionalAddressLine_3\",\n",
    "                    \"Entity_LegalAddress_City\",\n",
    "                    \"Entity_LegalAddress_Region\",\n",
    "                    \"Entity_LegalAddress_Country\",\n",
    "                    \"Entity_LegalAddress_PostalCode\"\n",
    "                ]\n",
    "            )\n",
    "            list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "            list_legal_address_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_legal_address_tuples,\n",
    "            table_name=\"GLEIF_LegalAddress\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"LegalAddress_FirstAddressLine\",\n",
    "                \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                \"LegalAddress_City\",\n",
    "                \"LegalAddress_Region\",\n",
    "                \"LegalAddress_Country\",\n",
    "                \"LegalAddress_PostalCode\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_headquarters_address(self, list_dict_records):\n",
    "        list_headquarters_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            dict_entity = self.obj_backfill_helpers.organize_by_prefix(dict_clean).get(\"Entity\", {})\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                dict_data=dict_entity,\n",
    "                target_keys=[\n",
    "                    \"HeadquartersAddress_FirstAddressLine\",\n",
    "                    \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                    \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                    \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                    \"HeadquartersAddress_City\",\n",
    "                    \"HeadquartersAddress_Region\",\n",
    "                    \"HeadquartersAddress_Country\",\n",
    "                    \"HeadquartersAddress_PostalCode\"\n",
    "                ]\n",
    "            )\n",
    "            list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "            list_headquarters_address_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_headquarters_address_tuples,\n",
    "            table_name=\"GLEIF_HeadquartersAddress\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"HeadquartersAddress_FirstAddressLine\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                \"HeadquartersAddress_City\",\n",
    "                \"HeadquartersAddress_Region\",\n",
    "                \"HeadquartersAddress_Country\",\n",
    "                \"HeadquartersAddress_PostalCode\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_legal_entity_events(self, list_dict_records):\n",
    "        list_legal_entity_events_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            dict_entity = self.obj_backfill_helpers.organize_by_prefix(dict_clean).get(\"Entity\", {})\n",
    "            list_output = self.obj_backfill_helpers.extract_event_data(\n",
    "                dict_data=dict_entity,\n",
    "                base_keyword=\"LegalEntityEvents\",\n",
    "                target_keys=[\n",
    "                    \"group_type\",\n",
    "                    \"event_status\",\n",
    "                    \"LegalEntityEventType\",\n",
    "                    \"LegalEntityEventEffectiveDate\",\n",
    "                    \"LegalEntityEventRecordedDate\",\n",
    "                    \"ValidationDocuments\"\n",
    "                ]\n",
    "            )\n",
    "            for tup in list_output:\n",
    "                list_legal_entity_events_tuples.append((dict_clean.get(\"LEI\"),) + tup)\n",
    "        \n",
    "        self.bulk_insert_using_copy(\n",
    "            data=list_legal_entity_events_tuples,\n",
    "            table_name=\"GLEIF_LegalEntityEvents\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"group_type\",\n",
    "                \"event_status\",\n",
    "                \"LegalEntityEventType\",\n",
    "                \"LegalEntityEventEffectiveDate\",\n",
    "                \"LegalEntityEventRecordedDate\",\n",
    "                \"ValidationDocuments\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_registration_data(self, list_dict_records):\n",
    "        list_registration_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            dict_registration = self.obj_backfill_helpers.organize_by_prefix(dict_clean).get(\"Registration\", {})\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                dict_data=dict_registration,\n",
    "                target_keys=[\n",
    "                    \"InitialRegistrationDate\",\n",
    "                    \"LastUpdateDate\",\n",
    "                    \"RegistrationStatus\",\n",
    "                    \"NextRenewalDate\",\n",
    "                    \"ManagingLOU\",\n",
    "                    \"ValidationSources\",\n",
    "                    \"ValidationAuthority_ValidationAuthorityID\",\n",
    "                    \"ValidationAuthority_ValidationAuthorityEntityID\"\n",
    "                ]\n",
    "            )\n",
    "            list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "            list_registration_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_registration_tuples,\n",
    "            table_name=\"GLEIF_registration_data\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"InitialRegistrationDate\",\n",
    "                \"LastUpdateDate\",\n",
    "                \"RegistrationStatus\",\n",
    "                \"NextRenewalDate\",\n",
    "                \"ManagingLOU\",\n",
    "                \"ValidationSources\",\n",
    "                \"ValidationAuthorityID\",\n",
    "                \"ValidationAuthorityEntityID\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_geoencoding_data(self, list_dict_records):\n",
    "        list_extension_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            dict_extension = self.obj_backfill_helpers.organize_by_prefix(dict_clean).get(\"Extension\", {})\n",
    "            dict_mega_flat = self.obj_backfill_helpers.further_flatten_geocoding(dict_data=dict_extension)\n",
    "            if any(re.search(r\"_\\d+_\", key) for key in dict_mega_flat.keys()):\n",
    "                list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data=dict_mega_flat)\n",
    "                for dict_ext in list_dicts:\n",
    "                    list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                        dict_data=dict_ext,\n",
    "                        subset_string=True,\n",
    "                        target_keys=[\n",
    "                            \"relevance\",\n",
    "                            \"match_type\",\n",
    "                            \"lat\",\n",
    "                            \"lng\",\n",
    "                            \"geocoding_date\",\n",
    "                            \"TopLeft.Latitude\",\n",
    "                            \"TopLeft.Longitude\",\n",
    "                            \"BottomRight.Latitude\",\n",
    "                            \"BottomRight.Longitude\",\n",
    "                            \"match_level\",\n",
    "                            \"mapped_street\",\n",
    "                            \"mapped_housenumber\",\n",
    "                            \"mapped_postalcode\",\n",
    "                            \"mapped_city\",\n",
    "                            \"mapped_district\",\n",
    "                            \"mapped_state\",\n",
    "                            \"mapped_country\"\n",
    "                        ]\n",
    "                    )\n",
    "                    list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "                    list_extension_data_tuples.append(tuple(list_output))\n",
    "            else:\n",
    "                list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                    dict_data=dict_mega_flat,\n",
    "                    subset_string=True,\n",
    "                    target_keys=[\n",
    "                        \"relevance\",\n",
    "                        \"match_type\",\n",
    "                        \"lat\",\n",
    "                        \"lng\",\n",
    "                        \"geocoding_date\",\n",
    "                        \"TopLeft.Latitude\",\n",
    "                        \"TopLeft.Longitude\",\n",
    "                        \"BottomRight.Latitude\",\n",
    "                        \"BottomRight.Longitude\",\n",
    "                        \"match_level\",\n",
    "                        \"mapped_street\",\n",
    "                        \"mapped_housenumber\",\n",
    "                        \"mapped_postalcode\",\n",
    "                        \"mapped_city\",\n",
    "                        \"mapped_district\",\n",
    "                        \"mapped_state\",\n",
    "                        \"mapped_country\"\n",
    "                    ]\n",
    "                )\n",
    "                list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "                list_extension_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_insert_using_copy(\n",
    "            data=list_extension_data_tuples,\n",
    "            table_name=\"GLEIF_geocoding\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"relevance\",\n",
    "                \"match_type\",\n",
    "                \"lat\",\n",
    "                \"lng\",\n",
    "                \"geocoding_date\",\n",
    "                \"TopLeft_Latitude\",\n",
    "                \"TopLeft_Longitude\",\n",
    "                \"BottomRight_Latitude\",\n",
    "                \"BottomRight_longitude\",\n",
    "                \"match_level\",\n",
    "                \"mapped_street\",\n",
    "                \"mapped_housenumber\",\n",
    "                \"mapped_postalcode\",\n",
    "                \"mapped_city\",\n",
    "                \"mapped_district\",\n",
    "                \"mapped_state\",\n",
    "                \"mapped_country\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_all_data(self, list_dict_records):\n",
    "        \"\"\"\n",
    "        Processes all types of data and performs upserts/inserts accordingly.\n",
    "        \n",
    "        Args:\n",
    "            list_dict_records (list): List of record dictionaries.\n",
    "        \"\"\"\n",
    "        self.process_entity_data(list_dict_records=list_dict_records)\n",
    "        self.process_other_legal_names(list_dict_records=list_dict_records)\n",
    "        self.process_legal_address(list_dict_records=list_dict_records)\n",
    "        self.process_headquarters_address(list_dict_records=list_dict_records)\n",
    "        self.process_legal_entity_events(list_dict_records=list_dict_records)\n",
    "        self.process_registration_data(list_dict_records=list_dict_records)\n",
    "        self.process_geoencoding_data(list_dict_records=list_dict_records)\n",
    "\n",
    "    def remove_duplicates(self, table_name, unique_columns):\n",
    "        \"\"\"\n",
    "        Removes duplicate rows from a PostgreSQL table based on specified unique columns.\n",
    "        Keeps the row with the smallest id and deletes others.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to clean.\n",
    "            unique_columns (list of str): Columns that define a unique record.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Starting duplicate removal for table '{table_name}' based on columns {unique_columns}.\")\n",
    "            \n",
    "            # Construct the PARTITION BY clause\n",
    "            partition_by = sql.SQL(', ').join([sql.Identifier(col) for col in unique_columns])\n",
    "            \n",
    "            # Construct the DELETE query using ROW_NUMBER()\n",
    "            delete_query = sql.SQL(\"\"\"\n",
    "                DELETE FROM {table} a\n",
    "                USING (\n",
    "                    SELECT id, ROW_NUMBER() OVER (\n",
    "                        PARTITION BY {partition_by}\n",
    "                        ORDER BY id\n",
    "                    ) AS rnum\n",
    "                    FROM {table}\n",
    "                ) b\n",
    "                WHERE a.id = b.id AND b.rnum > 1;\n",
    "            \"\"\").format(\n",
    "                table=sql.Identifier(table_name),\n",
    "                partition_by=partition_by\n",
    "            )\n",
    "            \n",
    "            # Execute the DELETE query\n",
    "            self.cursor.execute(delete_query)\n",
    "            logging.info(f\"Duplicate removal completed for table '{table_name}'.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error removing duplicates from table '{table_name}': {e}\")\n",
    "            raise\n",
    "\n",
    "    def remove_duplicates(self, table_name, unique_columns):\n",
    "        \"\"\"\n",
    "        Removes duplicate rows from a PostgreSQL table based on specified unique columns.\n",
    "        Keeps the row with the smallest id and deletes others.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to clean.\n",
    "            unique_columns (list of str): Columns that define a unique record.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Starting duplicate removal for table '{table_name}' based on columns {unique_columns}.\")\n",
    "\n",
    "            # Construct the PARTITION BY clause\n",
    "            partition_by = sql.SQL(', ').join([sql.Identifier(col) for col in unique_columns])\n",
    "\n",
    "            # Construct the DELETE query using ROW_NUMBER()\n",
    "            delete_query = sql.SQL(\"\"\"\n",
    "                DELETE FROM {table} a\n",
    "                USING (\n",
    "                    SELECT id, ROW_NUMBER() OVER (\n",
    "                        PARTITION BY {partition_by}\n",
    "                        ORDER BY id\n",
    "                    ) AS rnum\n",
    "                    FROM {table}\n",
    "                ) b\n",
    "                WHERE a.id = b.id AND b.rnum > 1;\n",
    "            \"\"\").format(\n",
    "                table=sql.Identifier(table_name),\n",
    "                partition_by=partition_by\n",
    "            )\n",
    "\n",
    "            # Execute the DELETE query\n",
    "            self.cursor.execute(delete_query)\n",
    "            logging.info(f\"Duplicate removal completed for table '{table_name}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error removing duplicates from table '{table_name}': {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_all_duplicates(self):\n",
    "        \"\"\"\n",
    "        Cleans duplicates from all specified tables.\n",
    "        Defines the tables and their unique columns.\n",
    "        \"\"\"\n",
    "        tables_to_clean = {\n",
    "            \"gleif_other_legal_names\": [\"lei\", \"otherentitynames\", \"type\"],\n",
    "            \"gleif_legalentityevents\": [\n",
    "                \"lei\",\n",
    "                \"group_type\",\n",
    "                \"event_status\",\n",
    "                \"legalentityeventtype\",\n",
    "                \"legalentityeventeffectivedate\",\n",
    "                \"legalentityeventrecordeddate\",\n",
    "                \"validationdocuments\"\n",
    "            ],\n",
    "            \"gleif_geocoding\": [\n",
    "                \"lei\",\n",
    "                \"relevance\",\n",
    "                \"match_type\",\n",
    "                \"lat\",\n",
    "                \"lng\",\n",
    "                \"geocoding_date\",\n",
    "                \"topleft_latitude\",\n",
    "                \"topleft_longitude\",\n",
    "                \"bottomright_latitude\",\n",
    "                \"bottomright_longitude\",\n",
    "                \"match_level\",\n",
    "                \"mapped_street\",\n",
    "                \"mapped_housenumber\",\n",
    "                \"mapped_postalcode\",\n",
    "                \"mapped_city\",\n",
    "                \"mapped_district\",\n",
    "                \"mapped_state\",\n",
    "                \"mapped_country\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        for table, unique_cols in tables_to_clean.items():\n",
    "            self.remove_duplicates(table, unique_cols)\n",
    "            logging.info(f\"Duplicates cleaned for table '{table}'.\")\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \"\"\"\n",
    "        Stores GLEIF update data in the PostgreSQL database.\n",
    "        \"\"\"\n",
    "        with open(self.str_json_file_path, 'r', encoding='utf-8') as file:\n",
    "            dict_relationships = json.load(file)\n",
    "            records = dict_relationships[\"records\"]\n",
    "            \n",
    "            # Process records in batches\n",
    "            self.process_all_data(list_dict_records = records)\n",
    "        \n",
    "        # Commit the transaction\n",
    "        self.conn.commit()\n",
    "        self.clean_all_duplicates()\n",
    "\n",
    "def bulk_insert_using_copy(self , table_name , columns, data):\n",
    "        \"\"\"Perform a bulk insert using PostgreSQL COPY with an in-memory buffer\n",
    "\n",
    "        Args:\n",
    "            table_name (_type_): Name of the table to insert into\n",
    "            columns (_type_): List of column names for the table\n",
    "            data (_type_): List of tuples with the data to be inserted\n",
    "        \"\"\"\n",
    "        \n",
    "        buffer = io.StringIO()\n",
    "        \n",
    "        #write data to the buffer\n",
    "        \n",
    "        for row in data:\n",
    "            '''row_converted = [\n",
    "            x.replace('\\\\', '\\\\\\\\') if isinstance(x, str) else x \n",
    "            for x in row]'''\n",
    "        # Replace None with \\N for PostgreSQL NULL representation\n",
    "        #row_converted = [str(x) if x is not None else '\\\\N' for x in row_converted]\n",
    "            #buffer.write('\\t'.join(row_converted) + \"\\n\")\n",
    "            #buffer.write('\\t'.join(map(str , row_converted)) + \"\\n\")\n",
    "            #buffer.write('\\t'.join(row_converted) + \"\\n\")\n",
    "            buffer.write('\\t'.join(map(str , row)) + \"\\n\")\n",
    "        buffer.seek(0) #reset buffer position to the beginning\n",
    "        \n",
    "        #Construct the copy query\n",
    "        #copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH DELIMITER '\\t', NULL '\\\\N'\"\n",
    "        \n",
    "        #copy_query = f\"\"\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH (FORMAT text, DELIMITER E'\\\\t', NULL '\\\\N')\"\"\"\n",
    "        copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH DELIMITER '\\t'\"\n",
    "        self.cursor.copy_expert(copy_query , buffer)\n",
    "        self.conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_id_values(table_name):\n",
    "    \"\"\"\n",
    "    Resets the id column values of the specified table to be consecutive,\n",
    "    starting from 1 up to the number of rows, and resets the sequence accordingly.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Name of the table.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Begin a transaction\n",
    "        obj.cursor.execute(\"BEGIN;\")\n",
    "        logging.info(f\"Started transaction for resetting id values in table '{table_name}'.\")\n",
    "        print(f\"Started transaction for resetting id values in table '{table_name}'.\")\n",
    "\n",
    "        # Create a temporary table with new id values\n",
    "        temp_table = f\"temp_{table_name}\"\n",
    "        obj.cursor.execute(sql.SQL(\"\"\"\n",
    "            CREATE TEMP TABLE {temp_table} AS\n",
    "            SELECT\n",
    "                ROW_NUMBER() OVER (ORDER BY id) AS id,\n",
    "                t.*\n",
    "            FROM {table_name} t;\n",
    "        \"\"\").format(\n",
    "            temp_table=sql.Identifier(temp_table),\n",
    "            table_name=sql.Identifier(table_name)\n",
    "        ))\n",
    "        logging.info(f\"Temporary table '{temp_table}' created with new id values.\")\n",
    "        print(f\"Temporary table '{temp_table}' created with new id values.\")\n",
    "\n",
    "        # Truncate the original table\n",
    "        obj.cursor.execute(sql.SQL(\"TRUNCATE TABLE {table_name} RESTART IDENTITY;\").format(\n",
    "            table_name=sql.Identifier(table_name)\n",
    "        ))\n",
    "        logging.info(f\"Table '{table_name}' truncated.\")\n",
    "        print(f\"Table '{table_name}' truncated.\")\n",
    "\n",
    "        # Insert data back into the original table\n",
    "        columns = obj.get_table_columns(table_name)\n",
    "        columns_list = sql.SQL(', ').join([sql.Identifier(col) for col in columns])\n",
    "        obj.cursor.execute(sql.SQL(\"\"\"\n",
    "            INSERT INTO {table_name} ({columns})\n",
    "            SELECT {columns} FROM {temp_table};\n",
    "        \"\"\").format(\n",
    "            table_name=sql.Identifier(table_name),\n",
    "            columns=columns_list,\n",
    "            temp_table=sql.Identifier(temp_table)\n",
    "        ))\n",
    "        logging.info(f\"Data inserted back into table '{table_name}' with new id values.\")\n",
    "        print(f\"Data inserted back into table '{table_name}' with new id values.\")\n",
    "\n",
    "        # Reset the sequence\n",
    "        obj.cursor.execute(sql.SQL(\"\"\"\n",
    "            SELECT setval(pg_get_serial_sequence(%s, 'id'), (SELECT MAX(id) FROM {table_name}), true);\n",
    "        \"\"\").format(\n",
    "            table_name=sql.Identifier(table_name)\n",
    "        ), [table_name])\n",
    "        logging.info(f\"Sequence reset for table '{table_name}'.\")\n",
    "        print(f\"Sequence reset for table '{table_name}'.\")\n",
    "\n",
    "        # Commit the transaction\n",
    "        obj.conn.commit()\n",
    "        logging.info(f\"Transaction committed for table '{table_name}'.\")\n",
    "        print(f\"Transaction committed for table '{table_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        obj.conn.rollback()\n",
    "        logging.error(f\"Error resetting id values for table '{table_name}': {e}\")\n",
    "        print(f\"Error resetting id values for table '{table_name}': {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import json\n",
    "import psycopg2\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "\n",
    "from Production.Update import GLEIF_Update_Helpers\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n",
    "\n",
    "class GLEIFUpdateLevel1:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        self.obj_update_helpers = GLEIF_Update_Helpers.GLEIF_Update_Helpers(bool_Level_1 = True)\n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Update_level_2.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            self.obj_update_helpers.download_on_machine()\n",
    "            self.str_json_file_path = self.obj_update_helpers.unpacking_GLEIF_zip_files()\n",
    "    \n",
    "        self.str_json_file_path = '../file_lib/Level_1_update_unpacked\\\\20241130-0000-gleif-goldencopy-lei2-intra-day.json'\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        #self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def bulk_upsert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"\n",
    "        Perform a bulk upsert using PostgreSQL COPY with a temporary table.\n",
    "        \"\"\"\n",
    "        temp_table = f\"{table_name}_temp\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Create a temporary table\n",
    "            create_temp_table_query = f\"\"\"\n",
    "                CREATE TEMP TABLE {temp_table} (LIKE {table_name} INCLUDING ALL)\n",
    "                ON COMMIT DROP;\n",
    "            \"\"\"\n",
    "            self.cursor.execute(create_temp_table_query)\n",
    "\n",
    "            # Step 2: Copy data into the temporary table\n",
    "            buffer = io.StringIO()\n",
    "            for row in data:\n",
    "                buffer.write('\\t'.join([str(item) if item is not None else '\\\\N' for item in row]) + \"\\n\")\n",
    "            buffer.seek(0)\n",
    "\n",
    "            copy_query = f\"\"\"\n",
    "                COPY {temp_table} ({', '.join(columns)})\n",
    "                FROM STDIN WITH (FORMAT text, DELIMITER '\\t', NULL '\\\\N')\n",
    "            \"\"\"\n",
    "            self.cursor.copy_expert(copy_query, buffer)\n",
    "\n",
    "            # Step 3: Perform upsert from temporary table to target table\n",
    "            update_columns = [col for col in columns if col != \"lei\"]\n",
    "            set_clause = \", \".join([f\"{col}=EXCLUDED.{col}\" for col in update_columns])\n",
    "\n",
    "            upsert_query = f\"\"\"\n",
    "                INSERT INTO {table_name} ({', '.join(columns)})\n",
    "                SELECT {', '.join(columns)} FROM {temp_table}\n",
    "                ON CONFLICT (lei) DO UPDATE SET\n",
    "                    {set_clause};\n",
    "            \"\"\"\n",
    "            self.cursor.execute(upsert_query)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            logging.error(f\"Error during bulk upsert for table {table_name}: {e}\")\n",
    "            raise\n",
    "     \n",
    "     \n",
    "    def drop_table(self , lst_table_names):\n",
    "            \"\"\"\n",
    "            Drops a specific table from the database securely.\n",
    "            \n",
    "            Parameters:\n",
    "                table_name (list of string): The names of the tables to drop.\n",
    "            \"\"\"\n",
    "\n",
    "            for table_name in lst_table_names:\n",
    "                self.cursor.execute(f\"DROP TABLE IF EXISTS {table_name} CASCADE;\")\n",
    "                \n",
    "            self.conn.commit()   \n",
    "        \n",
    "    def process_entity_data(self , list_dict_records):\n",
    "        list_entity_meta_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , subset_string = \"Entity\" , target_keys = [\"LegalName\", \"LegalJurisdiction\", \"EntityCategory\", \"EntitySubCategory\", \"LegalForm_EntityLegalFormCode\", \"LegalForm_OtherLegalForm\", \"EntityStatus\", \"EntityCreationDate\", \"RegistrationAuthority_RegistrationAuthorityID\", \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_entity_meta_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_upsert_using_copy(data = list_entity_meta_data_tuples , table_name = \"GLEIF_entity_data\" , \n",
    "                            columns = \n",
    "                            [\"lei\",\n",
    "                                \"LegalName\",\n",
    "                                \"LegalJurisdiction\",\n",
    "                                \"EntityCategory\",\n",
    "                                \"EntitySubCategory\",\n",
    "                                \"LegalForm_EntityLegalFormCode\",\n",
    "                                \"LegalForm_OtherLegalForm\",\n",
    "                                \"EntityStatus\",\n",
    "                                \"EntityCreationDate\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_entity_meta_data_tuples , table_name = \"GLEIF_entity_data\" , \n",
    "                            columns = \n",
    "                            [\"lei\",\n",
    "                                \"LegalName\",\n",
    "                                \"LegalJurisdiction\",\n",
    "                                \"EntityCategory\",\n",
    "                                \"EntitySubCategory\",\n",
    "                                \"LegalForm_EntityLegalFormCode\",\n",
    "                                \"LegalForm_OtherLegalForm\",\n",
    "                                \"EntityStatus\",\n",
    "                                \"EntityCreationDate\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\"\"\"\n",
    "    \n",
    "    def process_other_legal_names(self , list_dict_records):\n",
    "        list_other_names_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_other_entity_names(data_dict = dict_entity, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup         \n",
    "            list_other_names_tuples.extend(list_output)\n",
    "\n",
    "        #self.bulk_insert_using_copy(data = list_other_names_tuples , table_name = \"GLEIF_other_legal_names\" , columns = [\"lei\", \"OtherEntityNames\", \"Type\"])        \n",
    "        self.bulk_upsert_using_copy(data = list_other_names_tuples , table_name = \"GLEIF_other_legal_names\" , columns = [\"lei\", \"OtherEntityNames\", \"Type\"])        \n",
    "    \n",
    "    def process_legal_address(self , list_dict_records):\n",
    "        list_legal_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , target_keys = [\"Entity_LegalAddress_FirstAddressLine\" , \"Entity_LegalAddress_AdditionalAddressLine_1\" , \"Entity_LegalAddress_AdditionalAddressLine_2\" , \"Entity_LegalAddress_AdditionalAddressLine_3\" , \"Entity_LegalAddress_City\" , \"Entity_LegalAddress_Region\" , \"Entity_LegalAddress_Country\" , \"Entity_LegalAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_legal_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_upsert_using_copy(data = list_legal_address_tuples , table_name = \"GLEIF_LegalAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"LegalAddress_FirstAddressLine\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                                            \"LegalAddress_City\",\n",
    "                                            \"LegalAddress_Region\",\n",
    "                                            \"LegalAddress_Country\",\n",
    "                                            \"LegalAddress_PostalCode\"]) \n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_legal_address_tuples , table_name = \"GLEIF_LegalAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"LegalAddress_FirstAddressLine\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                                            \"LegalAddress_City\",\n",
    "                                            \"LegalAddress_Region\",\n",
    "                                            \"LegalAddress_Country\",\n",
    "                                            \"LegalAddress_PostalCode\"])  \"\"\"\n",
    "    \n",
    "    def process_headquarters_address(self , list_dict_records):\n",
    "        list_headquarters_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity , target_keys = [\"HeadquartersAddress_FirstAddressLine\" , \"HeadquartersAddress_AdditionalAddressLine_1\" , \"HeadquartersAddress_AdditionalAddressLine_2\" , \"HeadquartersAddress_AdditionalAddressLine_3\" , \"HeadquartersAddress_City\" , \"HeadquartersAddress_Region\" , \"HeadquartersAddress_Country\" , \"HeadquartersAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_headquarters_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_upsert_using_copy(data = list_headquarters_address_tuples , table_name = \"GLEIF_HeadquartersAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"HeadquartersAddress_FirstAddressLine\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                                            \"HeadquartersAddress_City\",\n",
    "                                            \"HeadquartersAddress_Region\",\n",
    "                                            \"HeadquartersAddress_Country\",\n",
    "                                            \"HeadquartersAddress_PostalCode\"]) \n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_headquarters_address_tuples , table_name = \"GLEIF_HeadquartersAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"HeadquartersAddress_FirstAddressLine\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                                            \"HeadquartersAddress_City\",\n",
    "                                            \"HeadquartersAddress_Region\",\n",
    "                                            \"HeadquartersAddress_Country\",\n",
    "                                            \"HeadquartersAddress_PostalCode\"]) \"\"\"\n",
    "    \n",
    "    def process_legal_entity_events(self , list_dict_records):\n",
    "        list_legal_entity_events_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_event_data(dict_data = dict_entity , base_keyword=\"LegalEntityEvents\" , target_keys=[\"group_type\", \"event_status\", \"LegalEntityEventType\", \"LegalEntityEventEffectiveDate\", \"LegalEntityEventRecordedDate\", \"ValidationDocuments\"])\n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup \n",
    "            list_legal_entity_events_tuples.extend(list_output)\n",
    "\n",
    "        self.bulk_upsert_using_copy(data = list_legal_entity_events_tuples , table_name = \"GLEIF_LegalEntityEvents\" , \n",
    "                                columns = [\"lei\",\n",
    "                                        \"group_type\",\n",
    "                                        \"event_status\",\n",
    "                                        \"LegalEntityEventType\",\n",
    "                                        \"LegalEntityEventEffectiveDate\",\n",
    "                                        \"LegalEntityEventRecordedDate\",\n",
    "                                        \"ValidationDocuments\"])\n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_legal_entity_events_tuples , table_name = \"GLEIF_LegalEntityEvents\" , \n",
    "                                columns = [\"lei\",\n",
    "                                        \"group_type\",\n",
    "                                        \"event_status\",\n",
    "                                        \"LegalEntityEventType\",\n",
    "                                        \"LegalEntityEventEffectiveDate\",\n",
    "                                        \"LegalEntityEventRecordedDate\",\n",
    "                                        \"ValidationDocuments\"])\"\"\"\n",
    "        \n",
    "    def process_registration_data(self , list_dict_records):\n",
    "        list_registration_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_registration = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Registration\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_registration , target_keys = [\"InitialRegistrationDate\" , \"LastUpdateDate\" , \"RegistrationStatus\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationAuthority\"])    \n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_registration_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_upsert_using_copy(data = list_registration_tuples , table_name = \"GLEIF_registration_data\" , \n",
    "                                columns = [\n",
    "                                            \"lei\",\n",
    "                                            \"InitialRegistrationDate\",\n",
    "                                            \"LastUpdateDate\",\n",
    "                                            \"RegistrationStatus\",\n",
    "                                            \"NextRenewalDate\",\n",
    "                                            \"ManagingLOU\",\n",
    "                                            \"ValidationSources\",\n",
    "                                            \"ValidationAuthority\"]) \n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_registration_tuples , table_name = \"GLEIF_registration_data\" , \n",
    "                                columns = [\n",
    "                                            \"lei\",\n",
    "                                            \"InitialRegistrationDate\",\n",
    "                                            \"LastUpdateDate\",\n",
    "                                            \"RegistrationStatus\",\n",
    "                                            \"NextRenewalDate\",\n",
    "                                            \"ManagingLOU\",\n",
    "                                            \"ValidationSources\",\n",
    "                                            \"ValidationAuthority\"]) \"\"\"\n",
    "    \n",
    "    def process_geoencoding_data(self , list_dict_records):\n",
    "        list_extension_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_extension = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Extension\"]\n",
    "            dict_mega_flat = self.obj_backfill_helpers.further_flatten_geocoding(dict_data = dict_extension)\n",
    "            if any(re.search(r\"_\\d+_\", key) for key in dict_mega_flat.keys()):\n",
    "                list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data = dict_mega_flat)\n",
    "                for dict_extension in list_dicts:\n",
    "                    list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_extension , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                    list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                    list_extension_data_tuples.append(tuple(list_output))\n",
    "            else:\n",
    "                list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_mega_flat , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                list_extension_data_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_upsert_using_copy(data = list_extension_data_tuples , table_name = \"GLEIF_geocoding\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"relevance\",\n",
    "                                            \"match_type\",\n",
    "                                            \"lat\",\n",
    "                                            \"lng\",\n",
    "                                            \"geocoding_date\",\n",
    "                                            \"TopLeft_Latitude\",\n",
    "                                            \"TopLeft_Longitude\",\n",
    "                                            \"BottomRight_Latitude\",\n",
    "                                            \"BottomRight_longitude\",\n",
    "                                            \"match_level\",\n",
    "                                            \"mapped_street\",\n",
    "                                            \"mapped_housenumber\",\n",
    "                                            \"mapped_postalcode\",\n",
    "                                            \"mapped_city\",\n",
    "                                            \"mapped_district\",\n",
    "                                            \"mapped_state\",\n",
    "                                            \"mapped_country\"]) \n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_extension_data_tuples , table_name = \"GLEIF_geocoding\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"relevance\",\n",
    "                                            \"match_type\",\n",
    "                                            \"lat\",\n",
    "                                            \"lng\",\n",
    "                                            \"geocoding_date\",\n",
    "                                            \"TopLeft_Latitude\",\n",
    "                                            \"TopLeft_Longitude\",\n",
    "                                            \"BottomRight_Latitude\",\n",
    "                                            \"BottomRight_longitude\",\n",
    "                                            \"match_level\",\n",
    "                                            \"mapped_street\",\n",
    "                                            \"mapped_housenumber\",\n",
    "                                            \"mapped_postalcode\",\n",
    "                                            \"mapped_city\",\n",
    "                                            \"mapped_district\",\n",
    "                                            \"mapped_state\",\n",
    "                                            \"mapped_country\"]) \"\"\"\n",
    "    \n",
    "    def process_all_data(self , list_dict_records):\n",
    "        self.process_entity_data(list_dict_records = list_dict_records)\n",
    "        self.process_other_legal_names(list_dict_records = list_dict_records)\n",
    "        self.process_legal_address(list_dict_records = list_dict_records)\n",
    "        self.process_headquarters_address(list_dict_records = list_dict_records)\n",
    "        self.process_legal_entity_events(list_dict_records = list_dict_records)\n",
    "        self.process_registration_data(list_dict_records = list_dict_records)\n",
    "        self.process_geoencoding_data(list_dict_records = list_dict_records)\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        with open(self.str_json_file_path, 'r', encoding='utf-8') as file:\n",
    "            dict_relationships = json.load(file)            \n",
    "            self.process_all_data(list_dict_records = dict_relationships[\"records\"])               \n",
    "        self.conn.commit()\n",
    "        \n",
    "        self.conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel1Data:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        \n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Backfill_level_1.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            str_level_1_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "            self.str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link , str_unpacked_zip_file_name = \"Level_1_unpacked\" , str_zip_file_path_name = \"Level_1.zip\")\n",
    "    \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/Level_1_unpacked\")[0]\n",
    "        self.str_json_file_path = rf\"../file_lib/Level_1_unpacked\" + \"//\" + str_unpacked_zip_file_name\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def delete_table(self, table_name):\n",
    "        \"\"\"\n",
    "        Deletes a single table from the PostgreSQL database.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to delete.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Sanitize table name to prevent SQL injection\n",
    "            drop_query = sql.SQL(\"DROP TABLE IF EXISTS {table} CASCADE;\").format(\n",
    "                table=sql.Identifier(table_name)\n",
    "            )\n",
    "\n",
    "            # Execute the DROP TABLE command\n",
    "            self.cursor.execute(drop_query)\n",
    "\n",
    "            # Log and print success message\n",
    "            logging.info(f\"Successfully deleted table: {table_name}\")\n",
    "            print(f\"Successfully deleted table: {table_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log and print error message\n",
    "            logging.error(f\"Error deleting table {table_name}: {e}\")\n",
    "            print(f\"Error deleting table {table_name}: {e}\")\n",
    "\n",
    "    \n",
    "    def create_tables(self):\n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_entity_data (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT UNIQUE,\n",
    "                    LegalName TEXT,\n",
    "                    LegalJurisdiction TEXT,\n",
    "                    EntityCategory TEXT,\n",
    "                    EntitySubCategory TEXT,\n",
    "                    LegalForm_EntityLegalFormCode TEXT,\n",
    "                    LegalForm_OtherLegalForm TEXT,\n",
    "                    EntityStatus TEXT,\n",
    "                    EntityCreationDate TEXT,\n",
    "                    RegistrationAuthority_RegistrationAuthorityID TEXT,\n",
    "                    RegistrationAuthority_RegistrationAuthorityEntityID TEXT\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_other_legal_names (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT,\n",
    "                    OtherEntityNames TEXT,\n",
    "                    Type TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_LegalAddress (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT UNIQUE,\n",
    "                    LegalAddress_FirstAddressLine TEXT,\n",
    "                    LegalAddress_AdditionalAddressLine_1 TEXT,\n",
    "                    LegalAddress_AdditionalAddressLine_2 TEXT,\n",
    "                    LegalAddress_AdditionalAddressLine_3 TEXT,\n",
    "                    LegalAddress_City TEXT,\n",
    "                    LegalAddress_Region TEXT,\n",
    "                    LegalAddress_Country TEXT,\n",
    "                    LegalAddress_PostalCode TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_HeadquartersAddress (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT UNIQUE,\n",
    "                    HeadquartersAddress_FirstAddressLine TEXT,\n",
    "                    HeadquartersAddress_AdditionalAddressLine_1 TEXT,\n",
    "                    HeadquartersAddress_AdditionalAddressLine_2 TEXT,\n",
    "                    HeadquartersAddress_AdditionalAddressLine_3 TEXT,\n",
    "                    HeadquartersAddress_City TEXT,\n",
    "                    HeadquartersAddress_Region TEXT,\n",
    "                    HeadquartersAddress_Country TEXT,\n",
    "                    HeadquartersAddress_PostalCode TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "                        \n",
    "        # LegalEntityEvents\n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_LegalEntityEvents (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT,\n",
    "                    group_type TEXT,\n",
    "                    event_status TEXT,\n",
    "                    LegalEntityEventType TEXT,\n",
    "                    LegalEntityEventEffectiveDate TEXT,\n",
    "                    LegalEntityEventRecordedDate TEXT,\n",
    "                    ValidationDocuments TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        # Registration Data\n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_registration_data (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT UNIQUE,\n",
    "                    InitialRegistrationDate TEXT,\n",
    "                    LastUpdateDate TEXT,\n",
    "                    RegistrationStatus TEXT,\n",
    "                    NextRenewalDate TEXT,\n",
    "                    ManagingLOU TEXT,\n",
    "                    ValidationSources TEXT,\n",
    "                    ValidationAuthorityID TEXT,\n",
    "                    ValidationAuthorityEntityID TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        # Geoencoding\n",
    "        self.cursor.execute(\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS GLEIF_geocoding (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    lei TEXT,\n",
    "                    relevance TEXT,\n",
    "                    match_type TEXT,\n",
    "                    lat TEXT,\n",
    "                    lng TEXT,\n",
    "                    geocoding_date TEXT,\n",
    "                    TopLeft_Latitude TEXT,\n",
    "                    TopLeft_Longitude TEXT,\n",
    "                    BottomRight_Latitude TEXT,\n",
    "                    BottomRight_longitude TEXT,\n",
    "                    match_level TEXT,\n",
    "                    mapped_street TEXT,\n",
    "                    mapped_housenumber TEXT,\n",
    "                    mapped_postalcode TEXT,\n",
    "                    mapped_city TEXT,\n",
    "                    mapped_district TEXT,\n",
    "                    mapped_state TEXT,\n",
    "                    mapped_country TEXT,\n",
    "                    FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "                );\n",
    "            \"\"\")\n",
    "        \n",
    "        self.conn.commit()\n",
    "\n",
    "        \n",
    "    #copy from other code\n",
    "\n",
    "    def load_batches_as_list(self , file_path , batch_size=100000):\n",
    "        \"\"\"Yield records in batches as lists of dictionaries.\"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Create an iterator for the 'records' key\n",
    "            records = ijson.items(file, \"records.item\")\n",
    "            \n",
    "            batch = []\n",
    "            for index, record in enumerate(records, start=1):\n",
    "                batch.append(record)  # Add record to the batch\n",
    "                \n",
    "                if index % batch_size == 0:  # Yield the batch when size is reached\n",
    "                    yield batch  # Yield the batch as a list\n",
    "                    batch = []  # Reset for the next batch\n",
    "            \n",
    "            # Yield any remaining records\n",
    "            if batch:\n",
    "                yield batch\n",
    "    \n",
    "    def bulk_insert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"Perform a bulk insert using PostgreSQL COPY with an in-memory buffer\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to insert into\n",
    "            columns (list): List of column names for the table\n",
    "            data (list): List of tuples with the data to be inserted\n",
    "        \"\"\"\n",
    "\n",
    "        buffer = io.StringIO()\n",
    "\n",
    "        for row in data:\n",
    "            # Escape backslashes and replace None with \\N for NULL\n",
    "            row_converted = []\n",
    "            for x in row:\n",
    "                if x is None:\n",
    "                    row_converted.append('\\\\N')  # NULL representation\n",
    "                elif isinstance(x, str):\n",
    "                    x = x.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "                    row_converted.append(x)\n",
    "                else:\n",
    "                    row_converted.append(str(x))\n",
    "            buffer.write('\\t'.join(row_converted) + '\\n')\n",
    "        \n",
    "        buffer.seek(0)  # Reset buffer position\n",
    "\n",
    "        # Construct the COPY query\n",
    "        copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH (FORMAT text, DELIMITER E'\\t', NULL '\\\\N')\"\n",
    "        self.cursor.copy_expert(copy_query, buffer)\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def process_entity_data(self , list_dict_records):\n",
    "        list_entity_meta_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , subset_string = \"Entity\" , target_keys = [\"LegalName\", \"LegalJurisdiction\", \"EntityCategory\", \"EntitySubCategory\", \"LegalForm_EntityLegalFormCode\", \"LegalForm_OtherLegalForm\", \"EntityStatus\", \"EntityCreationDate\", \"RegistrationAuthority_RegistrationAuthorityID\", \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_entity_meta_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_insert_using_copy(data = list_entity_meta_data_tuples , table_name = \"GLEIF_entity_data\" , \n",
    "                            columns = \n",
    "                            [\"lei\",\n",
    "                                \"LegalName\",\n",
    "                                \"LegalJurisdiction\",\n",
    "                                \"EntityCategory\",\n",
    "                                \"EntitySubCategory\",\n",
    "                                \"LegalForm_EntityLegalFormCode\",\n",
    "                                \"LegalForm_OtherLegalForm\",\n",
    "                                \"EntityStatus\",\n",
    "                                \"EntityCreationDate\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "    \n",
    "    def process_other_legal_names(self , list_dict_records):\n",
    "        list_other_names_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_other_entity_names(data_dict = dict_entity, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup         \n",
    "            list_other_names_tuples.extend(list_output)\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_other_names_tuples , table_name = \"GLEIF_other_legal_names\" , columns = [\"lei\", \"OtherEntityNames\", \"Type\"])        \n",
    "    \n",
    "    def process_legal_address(self , list_dict_records):\n",
    "        list_legal_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , target_keys = [\"Entity_LegalAddress_FirstAddressLine\" , \"Entity_LegalAddress_AdditionalAddressLine_1\" , \"Entity_LegalAddress_AdditionalAddressLine_2\" , \"Entity_LegalAddress_AdditionalAddressLine_3\" , \"Entity_LegalAddress_City\" , \"Entity_LegalAddress_Region\" , \"Entity_LegalAddress_Country\" , \"Entity_LegalAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_legal_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_legal_address_tuples , table_name = \"GLEIF_LegalAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"LegalAddress_FirstAddressLine\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                                            \"LegalAddress_City\",\n",
    "                                            \"LegalAddress_Region\",\n",
    "                                            \"LegalAddress_Country\",\n",
    "                                            \"LegalAddress_PostalCode\"])  \n",
    "    \n",
    "    def process_headquarters_address(self , list_dict_records):\n",
    "        list_headquarters_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity , target_keys = [\"HeadquartersAddress_FirstAddressLine\" , \"HeadquartersAddress_AdditionalAddressLine_1\" , \"HeadquartersAddress_AdditionalAddressLine_2\" , \"HeadquartersAddress_AdditionalAddressLine_3\" , \"HeadquartersAddress_City\" , \"HeadquartersAddress_Region\" , \"HeadquartersAddress_Country\" , \"HeadquartersAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_headquarters_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_headquarters_address_tuples , table_name = \"GLEIF_HeadquartersAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"HeadquartersAddress_FirstAddressLine\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                                            \"HeadquartersAddress_City\",\n",
    "                                            \"HeadquartersAddress_Region\",\n",
    "                                            \"HeadquartersAddress_Country\",\n",
    "                                            \"HeadquartersAddress_PostalCode\"]) \n",
    "    \n",
    "    def process_legal_entity_events(self , list_dict_records):\n",
    "        list_legal_entity_events_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_event_data(dict_data = dict_entity , base_keyword=\"LegalEntityEvents\" , target_keys=[\"group_type\", \"event_status\", \"LegalEntityEventType\", \"LegalEntityEventEffectiveDate\", \"LegalEntityEventRecordedDate\", \"ValidationDocuments\"])\n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup \n",
    "            list_legal_entity_events_tuples.extend(list_output)\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_legal_entity_events_tuples , table_name = \"GLEIF_LegalEntityEvents\" , \n",
    "                                columns = [\"lei\",\n",
    "                                        \"group_type\",\n",
    "                                        \"event_status\",\n",
    "                                        \"LegalEntityEventType\",\n",
    "                                        \"LegalEntityEventEffectiveDate\",\n",
    "                                        \"LegalEntityEventRecordedDate\",\n",
    "                                        \"ValidationDocuments\"])\n",
    "        \n",
    "    def process_registration_data(self , list_dict_records):\n",
    "        list_registration_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_registration = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Registration\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_registration , target_keys = [\"InitialRegistrationDate\" , \"LastUpdateDate\" , \"RegistrationStatus\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationAuthority_ValidationAuthorityID\" , \"ValidationAuthority_ValidationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_registration_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_registration_tuples , table_name = \"GLEIF_registration_data\" , \n",
    "                                columns = [\n",
    "                                            \"lei\",\n",
    "                                            \"InitialRegistrationDate\",\n",
    "                                            \"LastUpdateDate\",\n",
    "                                            \"RegistrationStatus\",\n",
    "                                            \"NextRenewalDate\",\n",
    "                                            \"ManagingLOU\",\n",
    "                                            \"ValidationSources\",\n",
    "                                            \"ValidationAuthorityID\",\n",
    "                                            \"ValidationAuthorityEntityID\"]) \n",
    "    \n",
    "    def process_geoencoding_data(self , list_dict_records):\n",
    "        list_extension_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_extension = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Extension\"]\n",
    "            dict_mega_flat = self.obj_backfill_helpers.further_flatten_geocoding(dict_data = dict_extension)\n",
    "            if any(re.search(r\"_\\d+_\", key) for key in dict_mega_flat.keys()):\n",
    "                list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data = dict_mega_flat)\n",
    "                for dict_extension in list_dicts:\n",
    "                    list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_extension , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                    list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                    list_extension_data_tuples.append(tuple(list_output))\n",
    "            else:\n",
    "                list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_mega_flat , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                list_extension_data_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_extension_data_tuples , table_name = \"GLEIF_geocoding\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"relevance\",\n",
    "                                            \"match_type\",\n",
    "                                            \"lat\",\n",
    "                                            \"lng\",\n",
    "                                            \"geocoding_date\",\n",
    "                                            \"TopLeft_Latitude\",\n",
    "                                            \"TopLeft_Longitude\",\n",
    "                                            \"BottomRight_Latitude\",\n",
    "                                            \"BottomRight_longitude\",\n",
    "                                            \"match_level\",\n",
    "                                            \"mapped_street\",\n",
    "                                            \"mapped_housenumber\",\n",
    "                                            \"mapped_postalcode\",\n",
    "                                            \"mapped_city\",\n",
    "                                            \"mapped_district\",\n",
    "                                            \"mapped_state\",\n",
    "                                            \"mapped_country\"]) \n",
    "    \n",
    "    def process_all_data(self , list_dict_records):\n",
    "        self.process_entity_data(list_dict_records = list_dict_records)\n",
    "        self.process_other_legal_names(list_dict_records = list_dict_records)\n",
    "        self.process_legal_address(list_dict_records = list_dict_records)\n",
    "        self.process_headquarters_address(list_dict_records = list_dict_records)\n",
    "        self.process_legal_entity_events(list_dict_records = list_dict_records)\n",
    "        self.process_registration_data(list_dict_records = list_dict_records)\n",
    "        self.process_geoencoding_data(list_dict_records = list_dict_records)\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        self.create_tables()\n",
    "        \n",
    "        generator_batched_json = self.load_batches_as_list(file_path = self.str_json_file_path , batch_size = 100000)\n",
    "        \n",
    "        for index , list_dict_records in enumerate(generator_batched_json):\n",
    "            self.process_all_data(list_dict_records = list_dict_records)\n",
    "        \n",
    "        self.conn.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
