{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Configure Chrome to download files automatically\n",
    "        download_directory = str_download_path  # Change to your desired directory\n",
    "\n",
    "        options = Options()\n",
    "        prefs = {\n",
    "            \"download.default_directory\": download_directory,  # Set default download directory\n",
    "            \"download.prompt_for_download\": False,  # Disable download prompt\n",
    "            \"download.directory_upgrade\": True,\n",
    "            \"safebrowsing.enabled\": True,  # Disable safety prompts\n",
    "        }\n",
    "        options.add_experimental_option(\"prefs\", prefs)\n",
    "\n",
    "        # Start the driver\n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLEIFLevel1Data:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        \n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Backfill_level_1.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            str_level_1_download_link = self.obj_backfill_helpers.get_level_download_links()\n",
    "            self.str_json_file_path = self.obj_backfill_helpers.unpacking_GLEIF_zip_files(str_download_link = str_level_1_download_link , str_unpacked_zip_file_name = \"Level_1_unpacked\" , str_zip_file_path_name = \"Level_1.zip\")\n",
    "    \n",
    "        str_unpacked_zip_file_name = os.listdir(rf\"../file_lib/Level_1_unpacked\")[0]\n",
    "        self.str_json_file_path = rf\"../file_lib/Level_1_unpacked\" + \"//\" + str_unpacked_zip_file_name\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def delete_table(self, table_name):\n",
    "        \"\"\"\n",
    "        Deletes a single table from the PostgreSQL database.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to delete.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Sanitize table name to prevent SQL injection\n",
    "            drop_query = sql.SQL(\"DROP TABLE IF EXISTS {table} CASCADE;\").format(\n",
    "                table=sql.Identifier(table_name)\n",
    "            )\n",
    "\n",
    "            # Execute the DROP TABLE command\n",
    "            self.cursor.execute(drop_query)\n",
    "\n",
    "            # Log and print success message\n",
    "            logging.info(f\"Successfully deleted table: {table_name}\")\n",
    "            print(f\"Successfully deleted table: {table_name}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # Log and print error message\n",
    "            logging.error(f\"Error deleting table {table_name}: {e}\")\n",
    "            print(f\"Error deleting table {table_name}: {e}\")\n",
    "\n",
    "    \n",
    "    def create_tables(self):\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_entity_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                LegalName TEXT,\n",
    "                LegalJurisdiction TEXT,\n",
    "                EntityCategory TEXT,\n",
    "                EntitySubCategory TEXT,\n",
    "                LegalForm_EntityLegalFormCode TEXT,\n",
    "                LegalForm_OtherLegalForm TEXT,\n",
    "                EntityStatus TEXT,\n",
    "                EntityCreationDate TEXT,\n",
    "                RegistrationAuthority_RegistrationAuthorityID TEXT,\n",
    "                RegistrationAuthority_RegistrationAuthorityEntityID TEXT\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_other_legal_names (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                OtherEntityNames TEXT,\n",
    "                Type TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_LegalAddress (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                LegalAddress_FirstAddressLine TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_1 TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_2 TEXT,\n",
    "                LegalAddress_AdditionalAddressLine_3 TEXT,\n",
    "                LegalAddress_City TEXT,\n",
    "                LegalAddress_Region TEXT,\n",
    "                LegalAddress_Country TEXT,\n",
    "                LegalAddress_PostalCode TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_HeadquartersAddress (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                HeadquartersAddress_FirstAddressLine TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_1 TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_2 TEXT,\n",
    "                HeadquartersAddress_AdditionalAddressLine_3 TEXT,\n",
    "                HeadquartersAddress_City TEXT,\n",
    "                HeadquartersAddress_Region TEXT,\n",
    "                HeadquartersAddress_Country TEXT,\n",
    "                HeadquartersAddress_PostalCode TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "                        \n",
    "        # LegalEntityEvents\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_LegalEntityEvents (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                group_type TEXT,\n",
    "                event_status TEXT,\n",
    "                LegalEntityEventType TEXT,\n",
    "                LegalEntityEventEffectiveDate TEXT,\n",
    "                LegalEntityEventRecordedDate TEXT,\n",
    "                ValidationDocuments TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Registration Data\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_registration_data (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                InitialRegistrationDate TEXT,\n",
    "                LastUpdateDate TEXT,\n",
    "                RegistrationStatus TEXT,\n",
    "                NextRenewalDate TEXT,\n",
    "                ManagingLOU TEXT,\n",
    "                ValidationSources TEXT,\n",
    "                ValidationAuthorityID TEXT,\n",
    "                ValidationAuthorityEntityID TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        # Geoencoding\n",
    "        self.cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS GLEIF_geocoding (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                lei TEXT,\n",
    "                relevance TEXT,\n",
    "                match_type TEXT,\n",
    "                lat TEXT,\n",
    "                lng TEXT,\n",
    "                geocoding_date TEXT,\n",
    "                TopLeft_Latitude TEXT,\n",
    "                TopLeft_Longitude TEXT,\n",
    "                BottomRight_Latitude TEXT,\n",
    "                BottomRight_longitude TEXT,\n",
    "                match_level TEXT,\n",
    "                mapped_street TEXT,\n",
    "                mapped_housenumber TEXT,\n",
    "                mapped_postalcode TEXT,\n",
    "                mapped_city TEXT,\n",
    "                mapped_district TEXT,\n",
    "                mapped_state TEXT,\n",
    "                mapped_country TEXT,\n",
    "                FOREIGN KEY (lei) REFERENCES GLEIF_entity_data(lei)\n",
    "            );\n",
    "        \"\"\")\n",
    "        \n",
    "        self.conn.commit()\n",
    "\n",
    "        \n",
    "    #copy from other code\n",
    "\n",
    "    def load_batches_as_list(self , file_path , batch_size=100000):\n",
    "        \"\"\"Yield records in batches as lists of dictionaries.\"\"\"\n",
    "        with open(file_path, 'rb') as file:\n",
    "            # Create an iterator for the 'records' key\n",
    "            records = ijson.items(file, \"records.item\")\n",
    "            \n",
    "            batch = []\n",
    "            for index, record in enumerate(records, start=1):\n",
    "                batch.append(record)  # Add record to the batch\n",
    "                \n",
    "                if index % batch_size == 0:  # Yield the batch when size is reached\n",
    "                    yield batch  # Yield the batch as a list\n",
    "                    batch = []  # Reset for the next batch\n",
    "            \n",
    "            # Yield any remaining records\n",
    "            if batch:\n",
    "                yield batch\n",
    "    \n",
    "    def bulk_insert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"Perform a bulk insert using PostgreSQL COPY with an in-memory buffer\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to insert into\n",
    "            columns (list): List of column names for the table\n",
    "            data (list): List of tuples with the data to be inserted\n",
    "        \"\"\"\n",
    "\n",
    "        buffer = io.StringIO()\n",
    "\n",
    "        for row in data:\n",
    "            # Escape backslashes and replace None with \\N for NULL\n",
    "            row_converted = []\n",
    "            for x in row:\n",
    "                if x is None:\n",
    "                    row_converted.append('\\\\N')  # NULL representation\n",
    "                elif isinstance(x, str):\n",
    "                    x = x.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "                    row_converted.append(x)\n",
    "                else:\n",
    "                    row_converted.append(str(x))\n",
    "            buffer.write('\\t'.join(row_converted) + '\\n')\n",
    "        \n",
    "        buffer.seek(0)  # Reset buffer position\n",
    "\n",
    "        # Construct the COPY query\n",
    "        copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH (FORMAT text, DELIMITER E'\\t', NULL '\\\\N')\"\n",
    "        self.cursor.copy_expert(copy_query, buffer)\n",
    "        self.conn.commit()\n",
    "        \n",
    "    def process_entity_data(self , list_dict_records):\n",
    "        list_entity_meta_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , subset_string = \"Entity\" , target_keys = [\"LegalName\", \"LegalJurisdiction\", \"EntityCategory\", \"EntitySubCategory\", \"LegalForm_EntityLegalFormCode\", \"LegalForm_OtherLegalForm\", \"EntityStatus\", \"EntityCreationDate\", \"RegistrationAuthority_RegistrationAuthorityID\", \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_entity_meta_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_insert_using_copy(data = list_entity_meta_data_tuples , table_name = \"GLEIF_entity_data\" , \n",
    "                            columns = \n",
    "                            [\"lei\",\n",
    "                                \"LegalName\",\n",
    "                                \"LegalJurisdiction\",\n",
    "                                \"EntityCategory\",\n",
    "                                \"EntitySubCategory\",\n",
    "                                \"LegalForm_EntityLegalFormCode\",\n",
    "                                \"LegalForm_OtherLegalForm\",\n",
    "                                \"EntityStatus\",\n",
    "                                \"EntityCreationDate\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "    \n",
    "    def process_other_legal_names(self , list_dict_records):\n",
    "        list_other_names_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_other_entity_names(data_dict = dict_entity, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup         \n",
    "            list_other_names_tuples.extend(list_output)\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_other_names_tuples , table_name = \"GLEIF_other_legal_names\" , columns = [\"lei\", \"OtherEntityNames\", \"Type\"])        \n",
    "    \n",
    "    def process_legal_address(self , list_dict_records):\n",
    "        list_legal_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , target_keys = [\"Entity_LegalAddress_FirstAddressLine\" , \"Entity_LegalAddress_AdditionalAddressLine_1\" , \"Entity_LegalAddress_AdditionalAddressLine_2\" , \"Entity_LegalAddress_AdditionalAddressLine_3\" , \"Entity_LegalAddress_City\" , \"Entity_LegalAddress_Region\" , \"Entity_LegalAddress_Country\" , \"Entity_LegalAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_legal_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_legal_address_tuples , table_name = \"GLEIF_LegalAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"LegalAddress_FirstAddressLine\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                                            \"LegalAddress_City\",\n",
    "                                            \"LegalAddress_Region\",\n",
    "                                            \"LegalAddress_Country\",\n",
    "                                            \"LegalAddress_PostalCode\"])  \n",
    "    \n",
    "    def process_headquarters_address(self , list_dict_records):\n",
    "        list_headquarters_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity , target_keys = [\"HeadquartersAddress_FirstAddressLine\" , \"HeadquartersAddress_AdditionalAddressLine_1\" , \"HeadquartersAddress_AdditionalAddressLine_2\" , \"HeadquartersAddress_AdditionalAddressLine_3\" , \"HeadquartersAddress_City\" , \"HeadquartersAddress_Region\" , \"HeadquartersAddress_Country\" , \"HeadquartersAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_headquarters_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_headquarters_address_tuples , table_name = \"GLEIF_HeadquartersAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"HeadquartersAddress_FirstAddressLine\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                                            \"HeadquartersAddress_City\",\n",
    "                                            \"HeadquartersAddress_Region\",\n",
    "                                            \"HeadquartersAddress_Country\",\n",
    "                                            \"HeadquartersAddress_PostalCode\"]) \n",
    "    \n",
    "    def process_legal_entity_events(self , list_dict_records):\n",
    "        list_legal_entity_events_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_event_data(dict_data = dict_entity , base_keyword=\"LegalEntityEvents\" , target_keys=[\"group_type\", \"event_status\", \"LegalEntityEventType\", \"LegalEntityEventEffectiveDate\", \"LegalEntityEventRecordedDate\", \"ValidationDocuments\"])\n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup \n",
    "            list_legal_entity_events_tuples.extend(list_output)\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_legal_entity_events_tuples , table_name = \"GLEIF_LegalEntityEvents\" , \n",
    "                                columns = [\"lei\",\n",
    "                                        \"group_type\",\n",
    "                                        \"event_status\",\n",
    "                                        \"LegalEntityEventType\",\n",
    "                                        \"LegalEntityEventEffectiveDate\",\n",
    "                                        \"LegalEntityEventRecordedDate\",\n",
    "                                        \"ValidationDocuments\"])\n",
    "        \n",
    "    def process_registration_data(self , list_dict_records):\n",
    "        list_registration_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_registration = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Registration\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_registration , target_keys = [\"InitialRegistrationDate\" , \"LastUpdateDate\" , \"RegistrationStatus\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationAuthority_ValidationAuthorityID\" , \"ValidationAuthority_ValidationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_registration_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_registration_tuples , table_name = \"GLEIF_registration_data\" , \n",
    "                                columns = [\n",
    "                                            \"lei\",\n",
    "                                            \"InitialRegistrationDate\",\n",
    "                                            \"LastUpdateDate\",\n",
    "                                            \"RegistrationStatus\",\n",
    "                                            \"NextRenewalDate\",\n",
    "                                            \"ManagingLOU\",\n",
    "                                            \"ValidationSources\",\n",
    "                                            \"ValidationAuthorityID\",\n",
    "                                            \"ValidationAuthorityEntityID\"]) \n",
    "    \n",
    "    def process_geoencoding_data(self , list_dict_records):\n",
    "        list_extension_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_extension = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Extension\"]\n",
    "            dict_mega_flat = self.obj_backfill_helpers.further_flatten_geocoding(dict_data = dict_extension)\n",
    "            if any(re.search(r\"_\\d+_\", key) for key in dict_mega_flat.keys()):\n",
    "                list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data = dict_mega_flat)\n",
    "                for dict_extension in list_dicts:\n",
    "                    list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_extension , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                    list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                    list_extension_data_tuples.append(tuple(list_output))\n",
    "            else:\n",
    "                list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_mega_flat , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                list_extension_data_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_insert_using_copy(data = list_extension_data_tuples , table_name = \"GLEIF_geocoding\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"relevance\",\n",
    "                                            \"match_type\",\n",
    "                                            \"lat\",\n",
    "                                            \"lng\",\n",
    "                                            \"geocoding_date\",\n",
    "                                            \"TopLeft_Latitude\",\n",
    "                                            \"TopLeft_Longitude\",\n",
    "                                            \"BottomRight_Latitude\",\n",
    "                                            \"BottomRight_longitude\",\n",
    "                                            \"match_level\",\n",
    "                                            \"mapped_street\",\n",
    "                                            \"mapped_housenumber\",\n",
    "                                            \"mapped_postalcode\",\n",
    "                                            \"mapped_city\",\n",
    "                                            \"mapped_district\",\n",
    "                                            \"mapped_state\",\n",
    "                                            \"mapped_country\"]) \n",
    "    \n",
    "    def process_all_data(self , list_dict_records):\n",
    "        self.process_entity_data(list_dict_records = list_dict_records)\n",
    "        self.process_other_legal_names(list_dict_records = list_dict_records)\n",
    "        self.process_legal_address(list_dict_records = list_dict_records)\n",
    "        self.process_headquarters_address(list_dict_records = list_dict_records)\n",
    "        self.process_legal_entity_events(list_dict_records = list_dict_records)\n",
    "        self.process_registration_data(list_dict_records = list_dict_records)\n",
    "        self.process_geoencoding_data(list_dict_records = list_dict_records)\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        self.create_tables()\n",
    "        \n",
    "        generator_batched_json = self.load_batches_as_list(file_path = self.str_json_file_path , batch_size = 100000)\n",
    "        \n",
    "        for index , list_dict_records in enumerate(generator_batched_json):\n",
    "            self.process_all_data(list_dict_records = list_dict_records)\n",
    "        \n",
    "        self.conn.close()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
