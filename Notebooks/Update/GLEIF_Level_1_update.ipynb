{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLEIF update rewritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import json\n",
    "import psycopg2\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "\n",
    "from Production.Update import GLEIF_Update_Helpers\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n",
    "\n",
    "class GLEIFUpdateLevel1:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        self.obj_update_helpers = GLEIF_Update_Helpers.GLEIF_Update_Helpers(bool_Level_1 = True)\n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Update_level_2.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            self.obj_update_helpers.download_on_machine()\n",
    "            self.str_json_file_path = self.obj_update_helpers.unpacking_GLEIF_zip_files()\n",
    "    \n",
    "        self.str_json_file_path = '../file_lib/Level_1_update_unpacked\\\\20241130-0000-gleif-goldencopy-lei2-intra-day.json'\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        #self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def bulk_upsert_using_copy(self, table_name, columns, data, conflict_columns, set_clause=None, do_nothing=False, compare_columns=None):\n",
    "        \"\"\"\n",
    "        Perform a bulk upsert using PostgreSQL COPY with a temporary table.\n",
    "        Supports both:\n",
    "        - Upsert with conditional update (skip exact duplicates).\n",
    "        - Insert with conflict ignoring.\n",
    "        \n",
    "        Args:\n",
    "            table_name (str): Name of the target table.\n",
    "            columns (list): List of column names.\n",
    "            data (list of tuples): Data to be inserted.\n",
    "            conflict_columns (list): Columns that define uniqueness.\n",
    "            set_clause (str, optional): SET clause for DO UPDATE.\n",
    "            do_nothing (bool, optional): If True, perform DO NOTHING on conflict.\n",
    "            compare_columns (list, optional): Columns to compare for skipping updates.\n",
    "        \"\"\"\n",
    "        temp_table = f\"{table_name}_temp\"\n",
    "\n",
    "        # Step 1: Create a temporary table\n",
    "        create_temp_table_query = f\"\"\"\n",
    "            CREATE TEMP TABLE {temp_table} (LIKE {table_name} INCLUDING ALL)\n",
    "            ON COMMIT DROP;\n",
    "        \"\"\"\n",
    "        self.cursor.execute(create_temp_table_query)\n",
    "\n",
    "        # Step 2: Copy data into the temporary table\n",
    "        buffer = io.StringIO()\n",
    "        for row in data:\n",
    "            # Escape backslashes, tabs, and newlines\n",
    "            escaped_row = []\n",
    "            for item in row:\n",
    "                if item is None:\n",
    "                    escaped_row.append('\\\\N')  # NULL representation\n",
    "                elif isinstance(item, str):\n",
    "                    item = item.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "                    escaped_row.append(item)\n",
    "                else:\n",
    "                    escaped_row.append(str(item))\n",
    "            buffer.write('\\t'.join(escaped_row) + \"\\n\")\n",
    "        buffer.seek(0)\n",
    "\n",
    "        copy_query = f\"\"\"\n",
    "            COPY {temp_table} ({', '.join(columns)})\n",
    "            FROM STDIN WITH (FORMAT text, DELIMITER '\\t', NULL '\\\\N')\n",
    "        \"\"\"\n",
    "        self.cursor.copy_expert(copy_query, buffer)\n",
    "\n",
    "        # Step 3: Construct the upsert query based on `do_nothing`\n",
    "        conflict_clause = ', '.join(conflict_columns)\n",
    "\n",
    "        if do_nothing:\n",
    "            # Perform INSERT ... ON CONFLICT DO NOTHING\n",
    "            upsert_query = f\"\"\"\n",
    "                INSERT INTO {table_name} ({', '.join(columns)})\n",
    "                SELECT {', '.join(columns)} FROM {temp_table}\n",
    "                ON CONFLICT ({conflict_clause}) DO NOTHING;\n",
    "            \"\"\"\n",
    "        else:\n",
    "            # Perform INSERT ... ON CONFLICT DO UPDATE SET ... WHERE ...\n",
    "            if not set_clause or not compare_columns:\n",
    "                raise ValueError(\"For upsert operations, `set_clause` and `compare_columns` must be provided when `do_nothing=False`.\")\n",
    "\n",
    "            # Build the WHERE clause to skip exact duplicates\n",
    "            where_conditions = \" OR \".join(\n",
    "                f\"{table_name}.{col} IS DISTINCT FROM EXCLUDED.{col}\"\n",
    "                for col in compare_columns\n",
    "            )\n",
    "            where_clause = f\"WHERE ({where_conditions})\"\n",
    "\n",
    "            upsert_query = f\"\"\"\n",
    "                INSERT INTO {table_name} ({', '.join(columns)})\n",
    "                SELECT {', '.join(columns)} FROM {temp_table}\n",
    "                ON CONFLICT ({conflict_clause}) DO UPDATE SET\n",
    "                    {set_clause}\n",
    "                {where_clause};\n",
    "            \"\"\"\n",
    "\n",
    "        # Execute the upsert query\n",
    "        self.cursor.execute(upsert_query)\n",
    "\n",
    "    def remove_duplicates_keep_order(self , input_list):\n",
    "        seen = set()\n",
    "        output_list = []\n",
    "        for item in input_list:\n",
    "            if item not in seen:\n",
    "                output_list.append(item)\n",
    "                seen.add(item)\n",
    "        return output_list\n",
    "\n",
    "    def process_entity_data(self , list_dict_records):\n",
    "        list_entity_meta_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , subset_string = \"Entity\" , target_keys = [\"LegalName\", \"LegalJurisdiction\", \"EntityCategory\", \"EntitySubCategory\", \"LegalForm_EntityLegalFormCode\", \"LegalForm_OtherLegalForm\", \"EntityStatus\", \"EntityCreationDate\", \"RegistrationAuthority_RegistrationAuthorityID\", \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_entity_meta_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_entity_meta_data_tuples,\n",
    "            table_name=\"GLEIF_entity_data\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"LegalName\",\n",
    "                \"LegalJurisdiction\",\n",
    "                \"EntityCategory\",\n",
    "                \"EntitySubCategory\",\n",
    "                \"LegalForm_EntityLegalFormCode\",\n",
    "                \"LegalForm_OtherLegalForm\",\n",
    "                \"EntityStatus\",\n",
    "                \"EntityCreationDate\",\n",
    "                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                \"RegistrationAuthority_RegistrationAuthorityEntityID\"\n",
    "            ],\n",
    "            conflict_columns=[\"lei\"],  # Unique constraint on 'lei'\n",
    "            compare_columns=[\n",
    "                \"LegalName\",\n",
    "                \"LegalJurisdiction\",\n",
    "                \"EntityCategory\",\n",
    "                \"EntitySubCategory\",\n",
    "                \"LegalForm_EntityLegalFormCode\",\n",
    "                \"LegalForm_OtherLegalForm\",\n",
    "                \"EntityStatus\",\n",
    "                \"EntityCreationDate\",\n",
    "                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                \"RegistrationAuthority_RegistrationAuthorityEntityID\"\n",
    "            ],\n",
    "            set_clause=\"\"\"\n",
    "                LegalName = EXCLUDED.LegalName,\n",
    "                LegalJurisdiction = EXCLUDED.LegalJurisdiction,\n",
    "                EntityCategory = EXCLUDED.EntityCategory,\n",
    "                EntitySubCategory = EXCLUDED.EntitySubCategory,\n",
    "                LegalForm_EntityLegalFormCode = EXCLUDED.LegalForm_EntityLegalFormCode,\n",
    "                LegalForm_OtherLegalForm = EXCLUDED.LegalForm_OtherLegalForm,\n",
    "                EntityStatus = EXCLUDED.EntityStatus,\n",
    "                EntityCreationDate = EXCLUDED.EntityCreationDate,\n",
    "                RegistrationAuthority_RegistrationAuthorityID = EXCLUDED.RegistrationAuthority_RegistrationAuthorityID,\n",
    "                RegistrationAuthority_RegistrationAuthorityEntityID = EXCLUDED.RegistrationAuthority_RegistrationAuthorityEntityID\n",
    "            \"\"\",\n",
    "            do_nothing=False  # Perform upsert with possible updates\n",
    "        )\n",
    "                            \n",
    "    \n",
    "    def process_other_legal_names(self , list_dict_records):\n",
    "        list_other_names_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_other_entity_names(data_dict = dict_entity, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup         \n",
    "            list_other_names_tuples.extend(list_output)\n",
    "\n",
    "        list_clean_other_names_tuples = self.remove_duplicates_keep_order(list_other_names_tuples)\n",
    "\n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_clean_other_names_tuples,\n",
    "            table_name=\"GLEIF_other_legal_names\",\n",
    "            columns=[\"lei\", \"OtherEntityNames\", \"Type\"],\n",
    "            conflict_columns=[\"lei\", \"OtherEntityNames\", \"Type\"],  # Composite unique constraint\n",
    "            do_nothing=True  # Perform INSERT ... ON CONFLICT DO NOTHING\n",
    "        )\n",
    "        \n",
    "    def process_legal_address(self , list_dict_records):\n",
    "        list_legal_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , target_keys = [\"Entity_LegalAddress_FirstAddressLine\" , \"Entity_LegalAddress_AdditionalAddressLine_1\" , \"Entity_LegalAddress_AdditionalAddressLine_2\" , \"Entity_LegalAddress_AdditionalAddressLine_3\" , \"Entity_LegalAddress_City\" , \"Entity_LegalAddress_Region\" , \"Entity_LegalAddress_Country\" , \"Entity_LegalAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_legal_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_legal_address_tuples,\n",
    "            table_name=\"GLEIF_LegalAddress\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"LegalAddress_FirstAddressLine\",\n",
    "                \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                \"LegalAddress_City\",\n",
    "                \"LegalAddress_Region\",\n",
    "                \"LegalAddress_Country\",\n",
    "                \"LegalAddress_PostalCode\"\n",
    "            ],\n",
    "            conflict_columns=[\"lei\"],  # Unique constraint on 'lei'\n",
    "            compare_columns=[\n",
    "                \"LegalAddress_FirstAddressLine\",\n",
    "                \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                \"LegalAddress_City\",\n",
    "                \"LegalAddress_Region\",\n",
    "                \"LegalAddress_Country\",\n",
    "                \"LegalAddress_PostalCode\"\n",
    "            ],\n",
    "            set_clause=\"\"\"\n",
    "                LegalAddress_FirstAddressLine = EXCLUDED.LegalAddress_FirstAddressLine,\n",
    "                LegalAddress_AdditionalAddressLine_1 = EXCLUDED.LegalAddress_AdditionalAddressLine_1,\n",
    "                LegalAddress_AdditionalAddressLine_2 = EXCLUDED.LegalAddress_AdditionalAddressLine_2,\n",
    "                LegalAddress_AdditionalAddressLine_3 = EXCLUDED.LegalAddress_AdditionalAddressLine_3,\n",
    "                LegalAddress_City = EXCLUDED.LegalAddress_City,\n",
    "                LegalAddress_Region = EXCLUDED.LegalAddress_Region,\n",
    "                LegalAddress_Country = EXCLUDED.LegalAddress_Country,\n",
    "                LegalAddress_PostalCode = EXCLUDED.LegalAddress_PostalCode\n",
    "            \"\"\",\n",
    "            do_nothing=False  # Perform upsert with possible updates\n",
    "        )\n",
    "    \n",
    "    def process_headquarters_address(self , list_dict_records):\n",
    "        list_headquarters_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity , target_keys = [\"HeadquartersAddress_FirstAddressLine\" , \"HeadquartersAddress_AdditionalAddressLine_1\" , \"HeadquartersAddress_AdditionalAddressLine_2\" , \"HeadquartersAddress_AdditionalAddressLine_3\" , \"HeadquartersAddress_City\" , \"HeadquartersAddress_Region\" , \"HeadquartersAddress_Country\" , \"HeadquartersAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_headquarters_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_headquarters_address_tuples,\n",
    "            table_name=\"GLEIF_HeadquartersAddress\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"HeadquartersAddress_FirstAddressLine\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                \"HeadquartersAddress_City\",\n",
    "                \"HeadquartersAddress_Region\",\n",
    "                \"HeadquartersAddress_Country\",\n",
    "                \"HeadquartersAddress_PostalCode\"\n",
    "            ],\n",
    "            conflict_columns=[\"lei\"],  # Unique constraint on 'lei'\n",
    "            compare_columns=[\n",
    "                \"HeadquartersAddress_FirstAddressLine\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                \"HeadquartersAddress_City\",\n",
    "                \"HeadquartersAddress_Region\",\n",
    "                \"HeadquartersAddress_Country\",\n",
    "                \"HeadquartersAddress_PostalCode\"\n",
    "            ],\n",
    "            set_clause=\"\"\"\n",
    "                HeadquartersAddress_FirstAddressLine = EXCLUDED.HeadquartersAddress_FirstAddressLine,\n",
    "                HeadquartersAddress_AdditionalAddressLine_1 = EXCLUDED.HeadquartersAddress_AdditionalAddressLine_1,\n",
    "                HeadquartersAddress_AdditionalAddressLine_2 = EXCLUDED.HeadquartersAddress_AdditionalAddressLine_2,\n",
    "                HeadquartersAddress_AdditionalAddressLine_3 = EXCLUDED.HeadquartersAddress_AdditionalAddressLine_3,\n",
    "                HeadquartersAddress_City = EXCLUDED.HeadquartersAddress_City,\n",
    "                HeadquartersAddress_Region = EXCLUDED.HeadquartersAddress_Region,\n",
    "                HeadquartersAddress_Country = EXCLUDED.HeadquartersAddress_Country,\n",
    "                HeadquartersAddress_PostalCode = EXCLUDED.HeadquartersAddress_PostalCode\n",
    "            \"\"\",\n",
    "            do_nothing=False  # Perform upsert with possible updates\n",
    "        )\n",
    "                                \n",
    "    def process_legal_entity_events(self , list_dict_records):\n",
    "        list_legal_entity_events_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_event_data(dict_data = dict_entity , base_keyword=\"LegalEntityEvents\" , target_keys=[\"group_type\", \"event_status\", \"LegalEntityEventType\", \"LegalEntityEventEffectiveDate\", \"LegalEntityEventRecordedDate\", \"ValidationDocuments\"])\n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup \n",
    "            list_legal_entity_events_tuples.extend(list_output)\n",
    "\n",
    "        list_clean_legal_entity_events_tuples = self.remove_duplicates_keep_order(list_legal_entity_events_tuples)\n",
    "\n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_clean_legal_entity_events_tuples,\n",
    "            table_name=\"GLEIF_LegalEntityEvents\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"group_type\",\n",
    "                \"event_status\",\n",
    "                \"LegalEntityEventType\",\n",
    "                \"LegalEntityEventEffectiveDate\",\n",
    "                \"LegalEntityEventRecordedDate\",\n",
    "                \"ValidationDocuments\"\n",
    "            ],\n",
    "            conflict_columns=[\n",
    "                \"lei\",\n",
    "                \"group_type\",\n",
    "                \"event_status\",\n",
    "                \"LegalEntityEventType\", \n",
    "                \"LegalEntityEventEffectiveDate\", \n",
    "                \"LegalEntityEventRecordedDate\",\n",
    "                \"ValidationDocuments\"\n",
    "            ],  # Composite unique constraint\n",
    "            do_nothing=True  # Perform INSERT ... ON CONFLICT DO NOTHING\n",
    "        )\n",
    "        \n",
    "    def process_registration_data(self , list_dict_records):\n",
    "        list_registration_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_registration = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Registration\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_registration , target_keys = [\"InitialRegistrationDate\" , \"LastUpdateDate\" , \"RegistrationStatus\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationAuthorityID\" , \"ValidationAuthorityEntityID\"])    \n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_registration_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_registration_tuples,\n",
    "            table_name=\"GLEIF_registration_data\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"InitialRegistrationDate\",\n",
    "                \"LastUpdateDate\",\n",
    "                \"RegistrationStatus\",\n",
    "                \"NextRenewalDate\",\n",
    "                \"ManagingLOU\",\n",
    "                \"ValidationSources\",\n",
    "                \"ValidationAuthorityID\",\n",
    "                \"ValidationAuthorityEntityID\"\n",
    "            ],\n",
    "            conflict_columns=[\"lei\"],  # Unique constraint on 'lei'\n",
    "            compare_columns=[\n",
    "                \"InitialRegistrationDate\",\n",
    "                \"LastUpdateDate\",\n",
    "                \"RegistrationStatus\",\n",
    "                \"NextRenewalDate\",\n",
    "                \"ManagingLOU\",\n",
    "                \"ValidationSources\",\n",
    "                \"ValidationAuthorityID\",\n",
    "                \"ValidationAuthorityEntityID\"\n",
    "            ],\n",
    "            set_clause=\"\"\"\n",
    "                InitialRegistrationDate = EXCLUDED.InitialRegistrationDate,\n",
    "                LastUpdateDate = EXCLUDED.LastUpdateDate,\n",
    "                RegistrationStatus = EXCLUDED.RegistrationStatus,\n",
    "                NextRenewalDate = EXCLUDED.NextRenewalDate,\n",
    "                ManagingLOU = EXCLUDED.ManagingLOU,\n",
    "                ValidationSources = EXCLUDED.ValidationSources,\n",
    "                ValidationAuthorityID = EXCLUDED.ValidationAuthorityID,\n",
    "                ValidationAuthorityEntityID = EXCLUDED.ValidationAuthorityEntityID\n",
    "            \"\"\",\n",
    "            do_nothing=False  # Perform upsert with possible updates\n",
    "        )\n",
    "    \n",
    "    def process_geoencoding_data(self , list_dict_records):\n",
    "        list_extension_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_extension = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Extension\"]\n",
    "            dict_mega_flat = self.obj_backfill_helpers.further_flatten_geocoding(dict_data = dict_extension)\n",
    "            if any(re.search(r\"_\\d+_\", key) for key in dict_mega_flat.keys()):\n",
    "                list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data = dict_mega_flat)\n",
    "                for dict_extension in list_dicts:\n",
    "                    list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_extension , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                    list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                    list_extension_data_tuples.append(tuple(list_output))\n",
    "            else:\n",
    "                list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_mega_flat , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                list_extension_data_tuples.append(tuple(list_output))\n",
    "\n",
    "        list_clean_extension_data_tuples = self.remove_duplicates_keep_order(list_extension_data_tuples)\n",
    "\n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_clean_extension_data_tuples,\n",
    "            table_name=\"GLEIF_geocoding\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"relevance\",\n",
    "                \"match_type\",\n",
    "                \"lat\",\n",
    "                \"lng\",\n",
    "                \"geocoding_date\",\n",
    "                \"TopLeft_Latitude\",\n",
    "                \"TopLeft_Longitude\",\n",
    "                \"BottomRight_Latitude\",\n",
    "                \"BottomRight_longitude\",\n",
    "                \"match_level\",\n",
    "                \"mapped_street\",\n",
    "                \"mapped_housenumber\",\n",
    "                \"mapped_postalcode\",\n",
    "                \"mapped_city\",\n",
    "                \"mapped_district\",\n",
    "                \"mapped_state\",\n",
    "                \"mapped_country\"\n",
    "            ],\n",
    "            conflict_columns=[\"lei\", \"relevance\", \"match_type\", \"lat\", \"geocoding_date\", \"TopLeft_Latitude\", \"TopLeft_Longitude\", \"BottomRight_Latitude\", \"BottomRight_longitude\", \"match_level\", \"mapped_street\", \"mapped_housenumber\", \"mapped_postalcode\", \"mapped_city\", \"mapped_district\", \"mapped_state\", \"mapped_country\"],  # Composite unique constraint\n",
    "            do_nothing=True  # Perform INSERT ... ON CONFLICT DO NOTHING\n",
    "        )\n",
    "    \n",
    "    def process_all_data(self , list_dict_records):\n",
    "        self.process_entity_data(list_dict_records = list_dict_records)\n",
    "        self.process_other_legal_names(list_dict_records = list_dict_records)\n",
    "        self.process_legal_address(list_dict_records = list_dict_records)\n",
    "        self.process_headquarters_address(list_dict_records = list_dict_records)\n",
    "        self.process_legal_entity_events(list_dict_records = list_dict_records)\n",
    "        self.process_registration_data(list_dict_records = list_dict_records)\n",
    "        self.process_geoencoding_data(list_dict_records = list_dict_records)\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        with open(self.str_json_file_path, 'r', encoding='utf-8') as file:\n",
    "            dict_relationships = json.load(file)            \n",
    "            self.process_all_data(list_dict_records = dict_relationships[\"records\"])               \n",
    "        self.conn.commit()\n",
    "        \n",
    "        self.conn.close()\n",
    "        \n",
    "        self.obj_backfill_helpers.file_tracker(file_path = self.str_json_file_path , str_db_name = \"GLEIF_test_db\" , str_data_title = \"Level_1_meta_data\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../file_lib/Level_1_update_unpacked\\\\20241130-0000-gleif-goldencopy-lei2-intra-day.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj = GLEIFUpdateLevel1(bool_log = True)\n",
    "obj.str_json_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obj.storing_GLEIF_data_in_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_backfill_helpers.file_tracker(file_path= obj.str_json_file_path , str_db_name = \"GLEIF_test_db\" , str_data_title = \"Level_1_meta_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
