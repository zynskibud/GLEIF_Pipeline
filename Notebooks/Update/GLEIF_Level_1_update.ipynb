{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import json\n",
    "import psycopg2\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "\n",
    "from Production.Update import GLEIF_Update_Helpers\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n",
    "\n",
    "class GLEIFUpdateLevel1:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        self.obj_update_helpers = GLEIF_Update_Helpers.GLEIF_Update_Helpers(bool_Level_1 = True)\n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Update_level_2.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            self.obj_update_helpers.download_on_machine()\n",
    "            self.str_json_file_path = self.obj_update_helpers.unpacking_GLEIF_zip_files()\n",
    "    \n",
    "        self.str_json_file_path = '../file_lib/Level_1_update_unpacked\\\\20241130-0000-gleif-goldencopy-lei2-intra-day.json'\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        #self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def bulk_upsert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"\n",
    "        Perform a bulk upsert using PostgreSQL COPY with a temporary table.\n",
    "        \"\"\"\n",
    "        temp_table = f\"{table_name}_temp\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Create a temporary table\n",
    "            create_temp_table_query = f\"\"\"\n",
    "                CREATE TEMP TABLE {temp_table} (LIKE {table_name} INCLUDING ALL)\n",
    "                ON COMMIT DROP;\n",
    "            \"\"\"\n",
    "            self.cursor.execute(create_temp_table_query)\n",
    "\n",
    "            # Step 2: Copy data into the temporary table\n",
    "            buffer = io.StringIO()\n",
    "            for row in data:\n",
    "                buffer.write('\\t'.join([str(item) if item is not None else '\\\\N' for item in row]) + \"\\n\")\n",
    "            buffer.seek(0)\n",
    "\n",
    "            copy_query = f\"\"\"\n",
    "                COPY {temp_table} ({', '.join(columns)})\n",
    "                FROM STDIN WITH (FORMAT text, DELIMITER '\\t', NULL '\\\\N')\n",
    "            \"\"\"\n",
    "            self.cursor.copy_expert(copy_query, buffer)\n",
    "\n",
    "            # Step 3: Perform upsert from temporary table to target table\n",
    "            update_columns = [col for col in columns if col != \"lei\"]\n",
    "            set_clause = \", \".join([f\"{col}=EXCLUDED.{col}\" for col in update_columns])\n",
    "\n",
    "            upsert_query = f\"\"\"\n",
    "                INSERT INTO {table_name} ({', '.join(columns)})\n",
    "                SELECT {', '.join(columns)} FROM {temp_table}\n",
    "                ON CONFLICT (lei) DO UPDATE SET\n",
    "                    {set_clause};\n",
    "            \"\"\"\n",
    "            self.cursor.execute(upsert_query)\n",
    "\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            logging.error(f\"Error during bulk upsert for table {table_name}: {e}\")\n",
    "            raise\n",
    "        \n",
    "        \n",
    "    def process_entity_data(self , list_dict_records):\n",
    "        list_entity_meta_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , subset_string = \"Entity\" , target_keys = [\"LegalName\", \"LegalJurisdiction\", \"EntityCategory\", \"EntitySubCategory\", \"LegalForm_EntityLegalFormCode\", \"LegalForm_OtherLegalForm\", \"EntityStatus\", \"EntityCreationDate\", \"RegistrationAuthority_RegistrationAuthorityID\", \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_entity_meta_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_upsert_using_copy(data = list_entity_meta_data_tuples , table_name = \"GLEIF_entity_data\" , \n",
    "                            columns = \n",
    "                            [\"lei\",\n",
    "                                \"LegalName\",\n",
    "                                \"LegalJurisdiction\",\n",
    "                                \"EntityCategory\",\n",
    "                                \"EntitySubCategory\",\n",
    "                                \"LegalForm_EntityLegalFormCode\",\n",
    "                                \"LegalForm_OtherLegalForm\",\n",
    "                                \"EntityStatus\",\n",
    "                                \"EntityCreationDate\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_entity_meta_data_tuples , table_name = \"GLEIF_entity_data\" , \n",
    "                            columns = \n",
    "                            [\"lei\",\n",
    "                                \"LegalName\",\n",
    "                                \"LegalJurisdiction\",\n",
    "                                \"EntityCategory\",\n",
    "                                \"EntitySubCategory\",\n",
    "                                \"LegalForm_EntityLegalFormCode\",\n",
    "                                \"LegalForm_OtherLegalForm\",\n",
    "                                \"EntityStatus\",\n",
    "                                \"EntityCreationDate\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                                \"RegistrationAuthority_RegistrationAuthorityEntityID\"])\"\"\"\n",
    "    \n",
    "    def process_other_legal_names(self , list_dict_records):\n",
    "        list_other_names_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_other_entity_names(data_dict = dict_entity, base_keyword=\"OtherEntityNames\", exclude_keywords=[\"TranslatedOtherEntityNames\"]) \n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup         \n",
    "            list_other_names_tuples.extend(list_output)\n",
    "\n",
    "        #self.bulk_insert_using_copy(data = list_other_names_tuples , table_name = \"GLEIF_other_legal_names\" , columns = [\"lei\", \"OtherEntityNames\", \"Type\"])        \n",
    "        self.bulk_upsert_using_copy(data = list_other_names_tuples , table_name = \"GLEIF_other_legal_names\" , columns = [\"lei\", \"OtherEntityNames\", \"Type\"])        \n",
    "\n",
    "        \n",
    "    def process_legal_address(self , list_dict_records):\n",
    "        list_legal_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_clean , target_keys = [\"Entity_LegalAddress_FirstAddressLine\" , \"Entity_LegalAddress_AdditionalAddressLine_1\" , \"Entity_LegalAddress_AdditionalAddressLine_2\" , \"Entity_LegalAddress_AdditionalAddressLine_3\" , \"Entity_LegalAddress_City\" , \"Entity_LegalAddress_Region\" , \"Entity_LegalAddress_Country\" , \"Entity_LegalAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_legal_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_upsert_using_copy(data = list_legal_address_tuples , table_name = \"GLEIF_LegalAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"LegalAddress_FirstAddressLine\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                                            \"LegalAddress_City\",\n",
    "                                            \"LegalAddress_Region\",\n",
    "                                            \"LegalAddress_Country\",\n",
    "                                            \"LegalAddress_PostalCode\"]) \n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_legal_address_tuples , table_name = \"GLEIF_LegalAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"LegalAddress_FirstAddressLine\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                                            \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                                            \"LegalAddress_City\",\n",
    "                                            \"LegalAddress_Region\",\n",
    "                                            \"LegalAddress_Country\",\n",
    "                                            \"LegalAddress_PostalCode\"])  \"\"\"\n",
    "    \n",
    "    def process_headquarters_address(self , list_dict_records):\n",
    "        list_headquarters_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_entity , target_keys = [\"HeadquartersAddress_FirstAddressLine\" , \"HeadquartersAddress_AdditionalAddressLine_1\" , \"HeadquartersAddress_AdditionalAddressLine_2\" , \"HeadquartersAddress_AdditionalAddressLine_3\" , \"HeadquartersAddress_City\" , \"HeadquartersAddress_Region\" , \"HeadquartersAddress_Country\" , \"HeadquartersAddress_PostalCode\"])\n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_headquarters_address_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_upsert_using_copy(data = list_headquarters_address_tuples , table_name = \"GLEIF_HeadquartersAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"HeadquartersAddress_FirstAddressLine\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                                            \"HeadquartersAddress_City\",\n",
    "                                            \"HeadquartersAddress_Region\",\n",
    "                                            \"HeadquartersAddress_Country\",\n",
    "                                            \"HeadquartersAddress_PostalCode\"]) \n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_headquarters_address_tuples , table_name = \"GLEIF_HeadquartersAddress\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"HeadquartersAddress_FirstAddressLine\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                                            \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                                            \"HeadquartersAddress_City\",\n",
    "                                            \"HeadquartersAddress_Region\",\n",
    "                                            \"HeadquartersAddress_Country\",\n",
    "                                            \"HeadquartersAddress_PostalCode\"]) \"\"\"\n",
    "    \n",
    "    def process_legal_entity_events(self , list_dict_records):\n",
    "        list_legal_entity_events_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_entity = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Entity\"]\n",
    "            list_output = self.obj_backfill_helpers.extract_event_data(dict_data = dict_entity , base_keyword=\"LegalEntityEvents\" , target_keys=[\"group_type\", \"event_status\", \"LegalEntityEventType\", \"LegalEntityEventEffectiveDate\", \"LegalEntityEventRecordedDate\", \"ValidationDocuments\"])\n",
    "            for index, tup in enumerate(list_output):\n",
    "                list_output[index] = (dict_clean[\"LEI\"],) + tup \n",
    "            list_legal_entity_events_tuples.extend(list_output)\n",
    "\n",
    "        self.bulk_upsert_using_copy(data = list_legal_entity_events_tuples , table_name = \"GLEIF_LegalEntityEvents\" , \n",
    "                                columns = [\"lei\",\n",
    "                                        \"group_type\",\n",
    "                                        \"event_status\",\n",
    "                                        \"LegalEntityEventType\",\n",
    "                                        \"LegalEntityEventEffectiveDate\",\n",
    "                                        \"LegalEntityEventRecordedDate\",\n",
    "                                        \"ValidationDocuments\"])\n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_legal_entity_events_tuples , table_name = \"GLEIF_LegalEntityEvents\" , \n",
    "                                columns = [\"lei\",\n",
    "                                        \"group_type\",\n",
    "                                        \"event_status\",\n",
    "                                        \"LegalEntityEventType\",\n",
    "                                        \"LegalEntityEventEffectiveDate\",\n",
    "                                        \"LegalEntityEventRecordedDate\",\n",
    "                                        \"ValidationDocuments\"])\"\"\"\n",
    "        \n",
    "    def process_registration_data(self , list_dict_records):\n",
    "        list_registration_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_registration = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Registration\"]\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_registration , target_keys = [\"InitialRegistrationDate\" , \"LastUpdateDate\" , \"RegistrationStatus\" , \"NextRenewalDate\" , \"ManagingLOU\" , \"ValidationSources\" , \"ValidationAuthority\"])    \n",
    "            list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "            list_registration_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_upsert_using_copy(data = list_registration_tuples , table_name = \"GLEIF_registration_data\" , \n",
    "                                columns = [\n",
    "                                            \"lei\",\n",
    "                                            \"InitialRegistrationDate\",\n",
    "                                            \"LastUpdateDate\",\n",
    "                                            \"RegistrationStatus\",\n",
    "                                            \"NextRenewalDate\",\n",
    "                                            \"ManagingLOU\",\n",
    "                                            \"ValidationSources\",\n",
    "                                            \"ValidationAuthority\"]) \n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_registration_tuples , table_name = \"GLEIF_registration_data\" , \n",
    "                                columns = [\n",
    "                                            \"lei\",\n",
    "                                            \"InitialRegistrationDate\",\n",
    "                                            \"LastUpdateDate\",\n",
    "                                            \"RegistrationStatus\",\n",
    "                                            \"NextRenewalDate\",\n",
    "                                            \"ManagingLOU\",\n",
    "                                            \"ValidationSources\",\n",
    "                                            \"ValidationAuthority\"]) \"\"\"\n",
    "    \n",
    "    def process_geoencoding_data(self , list_dict_records):\n",
    "        list_extension_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict = dict_flat)\n",
    "            dict_extension = (self.obj_backfill_helpers.organize_by_prefix(dict_clean))[\"Extension\"]\n",
    "            dict_mega_flat = self.obj_backfill_helpers.further_flatten_geocoding(dict_data = dict_extension)\n",
    "            if any(re.search(r\"_\\d+_\", key) for key in dict_mega_flat.keys()):\n",
    "                list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data = dict_mega_flat)\n",
    "                for dict_extension in list_dicts:\n",
    "                    list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_extension , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                    list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                    list_extension_data_tuples.append(tuple(list_output))\n",
    "            else:\n",
    "                list_output = self.obj_backfill_helpers.get_target_values(dict_data = dict_mega_flat , subset_string = True, target_keys = [\"relevance\" , \"match_type\" , \"lat\" , \"lng\" , \"geocoding_date\" , \"TopLeft.Latitude\" , \"TopLeft.Longitude\" , \"BottomRight.Latitude\" , \"BottomRight.Longitude\" , \"match_level\" , \"mapped_street\" , \"mapped_housenumber\" , \"mapped_postalcode\" , \"mapped_city\" , \"mapped_district\" , \"mapped_state\" , \"mapped_country\"])\n",
    "                list_output.insert(0 , dict_clean[\"LEI\"])\n",
    "                list_extension_data_tuples.append(tuple(list_output))\n",
    "\n",
    "        self.bulk_upsert_using_copy(data = list_extension_data_tuples , table_name = \"GLEIF_geocoding\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"relevance\",\n",
    "                                            \"match_type\",\n",
    "                                            \"lat\",\n",
    "                                            \"lng\",\n",
    "                                            \"geocoding_date\",\n",
    "                                            \"TopLeft_Latitude\",\n",
    "                                            \"TopLeft_Longitude\",\n",
    "                                            \"BottomRight_Latitude\",\n",
    "                                            \"BottomRight_longitude\",\n",
    "                                            \"match_level\",\n",
    "                                            \"mapped_street\",\n",
    "                                            \"mapped_housenumber\",\n",
    "                                            \"mapped_postalcode\",\n",
    "                                            \"mapped_city\",\n",
    "                                            \"mapped_district\",\n",
    "                                            \"mapped_state\",\n",
    "                                            \"mapped_country\"]) \n",
    "        \n",
    "        \"\"\"self.bulk_insert_using_copy(data = list_extension_data_tuples , table_name = \"GLEIF_geocoding\" , \n",
    "                                columns = [\"lei\",\n",
    "                                            \"relevance\",\n",
    "                                            \"match_type\",\n",
    "                                            \"lat\",\n",
    "                                            \"lng\",\n",
    "                                            \"geocoding_date\",\n",
    "                                            \"TopLeft_Latitude\",\n",
    "                                            \"TopLeft_Longitude\",\n",
    "                                            \"BottomRight_Latitude\",\n",
    "                                            \"BottomRight_longitude\",\n",
    "                                            \"match_level\",\n",
    "                                            \"mapped_street\",\n",
    "                                            \"mapped_housenumber\",\n",
    "                                            \"mapped_postalcode\",\n",
    "                                            \"mapped_city\",\n",
    "                                            \"mapped_district\",\n",
    "                                            \"mapped_state\",\n",
    "                                            \"mapped_country\"]) \"\"\"\n",
    "    \n",
    "    def process_all_data(self , list_dict_records):\n",
    "        self.process_entity_data(list_dict_records = list_dict_records)\n",
    "        self.process_other_legal_names(list_dict_records = list_dict_records)\n",
    "        self.process_legal_address(list_dict_records = list_dict_records)\n",
    "        self.process_headquarters_address(list_dict_records = list_dict_records)\n",
    "        self.process_legal_entity_events(list_dict_records = list_dict_records)\n",
    "        self.process_registration_data(list_dict_records = list_dict_records)\n",
    "        self.process_geoencoding_data(list_dict_records = list_dict_records)\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \n",
    "        with open(self.str_json_file_path, 'r', encoding='utf-8') as file:\n",
    "            dict_relationships = json.load(file)            \n",
    "            self.process_all_data(list_dict_records = dict_relationships[\"records\"])               \n",
    "        self.conn.commit()\n",
    "        \n",
    "        self.conn.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidColumnReference",
     "evalue": "there is no unique or exclusion constraint matching the ON CONFLICT specification\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidColumnReference\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m obj \u001b[38;5;241m=\u001b[39m GLEIFUpdateLevel1(bool_log \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstoring_GLEIF_data_in_database\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 341\u001b[0m, in \u001b[0;36mGLEIFUpdateLevel1.storing_GLEIF_data_in_database\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstr_json_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m    340\u001b[0m     dict_relationships \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(file)            \n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_all_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdict_relationships\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecords\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m               \n\u001b[0;32m    342\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[0;32m    344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mclose()\n",
      "Cell \u001b[1;32mIn[1], line 330\u001b[0m, in \u001b[0;36mGLEIFUpdateLevel1.process_all_data\u001b[1;34m(self, list_dict_records)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_all_data\u001b[39m(\u001b[38;5;28mself\u001b[39m , list_dict_records):\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_entity_data(list_dict_records \u001b[38;5;241m=\u001b[39m list_dict_records)\n\u001b[1;32m--> 330\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_other_legal_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlist_dict_records\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_legal_address(list_dict_records \u001b[38;5;241m=\u001b[39m list_dict_records)\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_headquarters_address(list_dict_records \u001b[38;5;241m=\u001b[39m list_dict_records)\n",
      "Cell \u001b[1;32mIn[1], line 138\u001b[0m, in \u001b[0;36mGLEIFUpdateLevel1.process_other_legal_names\u001b[1;34m(self, list_dict_records)\u001b[0m\n\u001b[0;32m    135\u001b[0m     list_other_names_tuples\u001b[38;5;241m.\u001b[39mextend(list_output)\n\u001b[0;32m    137\u001b[0m \u001b[38;5;66;03m#self.bulk_insert_using_copy(data = list_other_names_tuples , table_name = \"GLEIF_other_legal_names\" , columns = [\"lei\", \"OtherEntityNames\", \"Type\"])        \u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbulk_upsert_using_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlist_other_names_tuples\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGLEIF_other_legal_names\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlei\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOtherEntityNames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mType\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 79\u001b[0m, in \u001b[0;36mGLEIFUpdateLevel1.bulk_upsert_using_copy\u001b[1;34m(self, table_name, columns, data)\u001b[0m\n\u001b[0;32m     71\u001b[0m     set_clause \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=EXCLUDED.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m update_columns])\n\u001b[0;32m     73\u001b[0m     upsert_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;124m        INSERT INTO \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124m        SELECT \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(columns)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m FROM \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_table\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;124m        ON CONFLICT (lei) DO UPDATE SET\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mset_clause\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m;\u001b[39m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m---> 79\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mupsert_query\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconn\u001b[38;5;241m.\u001b[39mrollback()\n",
      "\u001b[1;31mInvalidColumnReference\u001b[0m: there is no unique or exclusion constraint matching the ON CONFLICT specification\n"
     ]
    }
   ],
   "source": [
    "obj = GLEIFUpdateLevel1(bool_log = True)\n",
    "obj.storing_GLEIF_data_in_database()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help from chatgpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import json\n",
    "import psycopg2\n",
    "import sys\n",
    "import io\n",
    "import re\n",
    "from psycopg2 import sql\n",
    "current_directory = os.getcwd()\n",
    "target_directory = os.path.abspath(os.path.join(current_directory, \"..\", \"..\"))\n",
    "sys.path.append(target_directory)\n",
    "\n",
    "from Production.Update import GLEIF_Update_Helpers\n",
    "from Production.Backfill import GLEIF_Backfill_Helpers\n",
    "\n",
    "class GLEIFUpdateLevel1:\n",
    "    def __init__(self , bool_log = True , str_db_name = \"GLEIF_test_db\" , bool_downloaded = True):\n",
    "        self.obj_update_helpers = GLEIF_Update_Helpers.GLEIF_Update_Helpers(bool_Level_1 = True)\n",
    "        self.obj_backfill_helpers = GLEIF_Backfill_Helpers.GLEIF_Backill_Helpers(bool_Level_1 = True)\n",
    "        if bool_log:\n",
    "            logging_folder = \"../logging\"  # Adjust the folder path as necessary\n",
    "    \n",
    "            if os.path.exists(logging_folder):\n",
    "                if not os.path.isdir(logging_folder):\n",
    "                    raise FileExistsError(f\"'{logging_folder}' exists but is not a directory. Please remove or rename the file.\")\n",
    "            else:\n",
    "                os.makedirs(logging_folder)\n",
    "    \n",
    "            logging.basicConfig(filename=f\"{logging_folder}/GLEIF_Update_level_2.log\", level=logging.DEBUG, format='%(levelname)s: %(message)s', filemode=\"w\")\n",
    "\n",
    "        if not bool_downloaded:\n",
    "            if not os.path.exists(\"../file_lib\"):\n",
    "                os.makedirs(\"../file_lib\")\n",
    "                \n",
    "            self.obj_update_helpers.download_on_machine()\n",
    "            self.str_json_file_path = self.obj_update_helpers.unpacking_GLEIF_zip_files()\n",
    "    \n",
    "        self.str_json_file_path = '../file_lib/Level_1_update_unpacked\\\\20241130-0000-gleif-goldencopy-lei2-intra-day.json'\n",
    "        self.conn = psycopg2.connect(dbname = str_db_name, user=\"Matthew_Pisinski\", password=\"matt1\", host=\"localhost\", port=\"5432\")    \n",
    "        #self.conn.autocommit = True\n",
    "        self.cursor = self.conn.cursor()\n",
    "\n",
    "    def bulk_upsert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"\n",
    "        Perform a bulk upsert using PostgreSQL COPY with a temporary table.\n",
    "        \n",
    "        Args:\n",
    "            table_name (str): Name of the target table.\n",
    "            columns (list): List of column names.\n",
    "            data (list): List of tuples containing the data to upsert.\n",
    "        \"\"\"\n",
    "        temp_table = f\"{table_name}_temp\"\n",
    "\n",
    "        try:\n",
    "            # Step 1: Create a temporary table\n",
    "            create_temp_table_query = f\"\"\"\n",
    "                CREATE TEMP TABLE {temp_table} (LIKE {table_name} INCLUDING ALL)\n",
    "                ON COMMIT DROP;\n",
    "            \"\"\"\n",
    "            self.cursor.execute(create_temp_table_query)\n",
    "            logging.info(f\"Temporary table {temp_table} created.\")\n",
    "\n",
    "            # Step 2: Copy data into the temporary table\n",
    "            buffer = io.StringIO()\n",
    "            for row in data:\n",
    "                # Escape backslashes, tabs, and newlines; replace None with \\N\n",
    "                row_converted = []\n",
    "                for item in row:\n",
    "                    if item is None:\n",
    "                        row_converted.append('\\\\N')\n",
    "                    elif isinstance(item, str):\n",
    "                        item = item.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "                        row_converted.append(item)\n",
    "                    else:\n",
    "                        row_converted.append(str(item))\n",
    "                buffer.write('\\t'.join(row_converted) + '\\n')\n",
    "            buffer.seek(0)\n",
    "\n",
    "            copy_query = f\"\"\"\n",
    "                COPY {temp_table} ({', '.join(columns)})\n",
    "                FROM STDIN WITH (FORMAT text, DELIMITER E'\\\\t', NULL '\\\\N')\n",
    "            \"\"\"\n",
    "            self.cursor.copy_expert(copy_query, buffer)\n",
    "            logging.info(f\"Data copied to temporary table {temp_table}.\")\n",
    "\n",
    "            # Step 3: Perform upsert from temporary table to target table\n",
    "            update_columns = [col for col in columns if col != \"lei\"]\n",
    "            set_clause = \", \".join([f\"{col}=EXCLUDED.{col}\" for col in update_columns])\n",
    "\n",
    "            upsert_query = f\"\"\"\n",
    "                INSERT INTO {table_name} ({', '.join(columns)})\n",
    "                SELECT {', '.join(columns)} FROM {temp_table}\n",
    "                ON CONFLICT (lei) DO UPDATE SET\n",
    "                    {set_clause};\n",
    "            \"\"\"\n",
    "            self.cursor.execute(upsert_query)\n",
    "            logging.info(f\"Upsert operation completed for table {table_name}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            logging.error(f\"Error during bulk upsert for table {table_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def bulk_insert_using_copy(self, table_name, columns, data):\n",
    "        \"\"\"\n",
    "        Perform a bulk insert using PostgreSQL COPY.\n",
    "        \n",
    "        Args:\n",
    "            table_name (str): Name of the target table.\n",
    "            columns (list): List of column names.\n",
    "            data (list): List of tuples containing the data to insert.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            buffer = io.StringIO()\n",
    "            for row in data:\n",
    "                # Escape backslashes, tabs, and newlines; replace None with \\N\n",
    "                row_converted = []\n",
    "                for item in row:\n",
    "                    if item is None:\n",
    "                        row_converted.append('\\\\N')\n",
    "                    elif isinstance(item, str):\n",
    "                        item = item.replace('\\\\', '\\\\\\\\').replace('\\t', '\\\\t').replace('\\n', '\\\\n')\n",
    "                        row_converted.append(item)\n",
    "                    else:\n",
    "                        row_converted.append(str(item))\n",
    "                buffer.write('\\t'.join(row_converted) + '\\n')\n",
    "            buffer.seek(0)\n",
    "\n",
    "            copy_query = f\"\"\"\n",
    "                COPY {table_name} ({', '.join(columns)})\n",
    "                FROM STDIN WITH (FORMAT text, DELIMITER E'\\\\t', NULL '\\\\N')\n",
    "            \"\"\"\n",
    "            self.cursor.copy_expert(copy_query, buffer)\n",
    "            logging.info(f\"Bulk insert completed for table {table_name}.\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            logging.error(f\"Error during bulk insert for table {table_name}: {e}\")\n",
    "            raise\n",
    "\n",
    "    def process_entity_data(self, list_dict_records):\n",
    "        list_entity_meta_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                dict_data=dict_clean,\n",
    "                subset_string=\"Entity\",\n",
    "                target_keys=[\n",
    "                    \"LegalName\",\n",
    "                    \"LegalJurisdiction\",\n",
    "                    \"EntityCategory\",\n",
    "                    \"EntitySubCategory\",\n",
    "                    \"LegalForm_EntityLegalFormCode\",\n",
    "                    \"LegalForm_OtherLegalForm\",\n",
    "                    \"EntityStatus\",\n",
    "                    \"EntityCreationDate\",\n",
    "                    \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                    \"RegistrationAuthority_RegistrationAuthorityEntityID\"\n",
    "                ]\n",
    "            )\n",
    "            list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "            list_entity_meta_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_entity_meta_data_tuples,\n",
    "            table_name=\"GLEIF_entity_data\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"LegalName\",\n",
    "                \"LegalJurisdiction\",\n",
    "                \"EntityCategory\",\n",
    "                \"EntitySubCategory\",\n",
    "                \"LegalForm_EntityLegalFormCode\",\n",
    "                \"LegalForm_OtherLegalForm\",\n",
    "                \"EntityStatus\",\n",
    "                \"EntityCreationDate\",\n",
    "                \"RegistrationAuthority_RegistrationAuthorityID\",\n",
    "                \"RegistrationAuthority_RegistrationAuthorityEntityID\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_other_legal_names(self, list_dict_records):\n",
    "        list_other_names_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            dict_entity = self.obj_backfill_helpers.organize_by_prefix(dict_clean).get(\"Entity\", {})\n",
    "            list_output = self.obj_backfill_helpers.extract_other_entity_names(\n",
    "                data_dict=dict_entity,\n",
    "                base_keyword=\"OtherEntityNames\",\n",
    "                exclude_keywords=[\"TranslatedOtherEntityNames\"]\n",
    "            )\n",
    "            for tup in list_output:\n",
    "                list_other_names_tuples.append((dict_clean.get(\"LEI\"),) + tup)\n",
    "        \n",
    "        self.bulk_insert_using_copy(\n",
    "            data=list_other_names_tuples,\n",
    "            table_name=\"GLEIF_other_legal_names\",\n",
    "            columns=[\"lei\", \"OtherEntityNames\", \"Type\"]\n",
    "        )\n",
    "\n",
    "    def process_legal_address(self, list_dict_records):\n",
    "        list_legal_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                dict_data=dict_clean,\n",
    "                target_keys=[\n",
    "                    \"Entity_LegalAddress_FirstAddressLine\",\n",
    "                    \"Entity_LegalAddress_AdditionalAddressLine_1\",\n",
    "                    \"Entity_LegalAddress_AdditionalAddressLine_2\",\n",
    "                    \"Entity_LegalAddress_AdditionalAddressLine_3\",\n",
    "                    \"Entity_LegalAddress_City\",\n",
    "                    \"Entity_LegalAddress_Region\",\n",
    "                    \"Entity_LegalAddress_Country\",\n",
    "                    \"Entity_LegalAddress_PostalCode\"\n",
    "                ]\n",
    "            )\n",
    "            list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "            list_legal_address_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_legal_address_tuples,\n",
    "            table_name=\"GLEIF_LegalAddress\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"LegalAddress_FirstAddressLine\",\n",
    "                \"LegalAddress_AdditionalAddressLine_1\",\n",
    "                \"LegalAddress_AdditionalAddressLine_2\",\n",
    "                \"LegalAddress_AdditionalAddressLine_3\",\n",
    "                \"LegalAddress_City\",\n",
    "                \"LegalAddress_Region\",\n",
    "                \"LegalAddress_Country\",\n",
    "                \"LegalAddress_PostalCode\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_headquarters_address(self, list_dict_records):\n",
    "        list_headquarters_address_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            dict_entity = self.obj_backfill_helpers.organize_by_prefix(dict_clean).get(\"Entity\", {})\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                dict_data=dict_entity,\n",
    "                target_keys=[\n",
    "                    \"HeadquartersAddress_FirstAddressLine\",\n",
    "                    \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                    \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                    \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                    \"HeadquartersAddress_City\",\n",
    "                    \"HeadquartersAddress_Region\",\n",
    "                    \"HeadquartersAddress_Country\",\n",
    "                    \"HeadquartersAddress_PostalCode\"\n",
    "                ]\n",
    "            )\n",
    "            list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "            list_headquarters_address_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_headquarters_address_tuples,\n",
    "            table_name=\"GLEIF_HeadquartersAddress\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"HeadquartersAddress_FirstAddressLine\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_1\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_2\",\n",
    "                \"HeadquartersAddress_AdditionalAddressLine_3\",\n",
    "                \"HeadquartersAddress_City\",\n",
    "                \"HeadquartersAddress_Region\",\n",
    "                \"HeadquartersAddress_Country\",\n",
    "                \"HeadquartersAddress_PostalCode\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_legal_entity_events(self, list_dict_records):\n",
    "        list_legal_entity_events_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            dict_entity = self.obj_backfill_helpers.organize_by_prefix(dict_clean).get(\"Entity\", {})\n",
    "            list_output = self.obj_backfill_helpers.extract_event_data(\n",
    "                dict_data=dict_entity,\n",
    "                base_keyword=\"LegalEntityEvents\",\n",
    "                target_keys=[\n",
    "                    \"group_type\",\n",
    "                    \"event_status\",\n",
    "                    \"LegalEntityEventType\",\n",
    "                    \"LegalEntityEventEffectiveDate\",\n",
    "                    \"LegalEntityEventRecordedDate\",\n",
    "                    \"ValidationDocuments\"\n",
    "                ]\n",
    "            )\n",
    "            for tup in list_output:\n",
    "                list_legal_entity_events_tuples.append((dict_clean.get(\"LEI\"),) + tup)\n",
    "        \n",
    "        self.bulk_insert_using_copy(\n",
    "            data=list_legal_entity_events_tuples,\n",
    "            table_name=\"GLEIF_LegalEntityEvents\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"group_type\",\n",
    "                \"event_status\",\n",
    "                \"LegalEntityEventType\",\n",
    "                \"LegalEntityEventEffectiveDate\",\n",
    "                \"LegalEntityEventRecordedDate\",\n",
    "                \"ValidationDocuments\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_registration_data(self, list_dict_records):\n",
    "        list_registration_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            dict_registration = self.obj_backfill_helpers.organize_by_prefix(dict_clean).get(\"Registration\", {})\n",
    "            list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                dict_data=dict_registration,\n",
    "                target_keys=[\n",
    "                    \"InitialRegistrationDate\",\n",
    "                    \"LastUpdateDate\",\n",
    "                    \"RegistrationStatus\",\n",
    "                    \"NextRenewalDate\",\n",
    "                    \"ManagingLOU\",\n",
    "                    \"ValidationSources\",\n",
    "                    \"ValidationAuthority_ValidationAuthorityID\",\n",
    "                    \"ValidationAuthority_ValidationAuthorityEntityID\"\n",
    "                ]\n",
    "            )\n",
    "            list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "            list_registration_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_upsert_using_copy(\n",
    "            data=list_registration_tuples,\n",
    "            table_name=\"GLEIF_registration_data\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"InitialRegistrationDate\",\n",
    "                \"LastUpdateDate\",\n",
    "                \"RegistrationStatus\",\n",
    "                \"NextRenewalDate\",\n",
    "                \"ManagingLOU\",\n",
    "                \"ValidationSources\",\n",
    "                \"ValidationAuthorityID\",\n",
    "                \"ValidationAuthorityEntityID\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_geoencoding_data(self, list_dict_records):\n",
    "        list_extension_data_tuples = []\n",
    "        \n",
    "        for dict_record in list_dict_records:\n",
    "            dict_flat = self.obj_backfill_helpers.flatten_dict(dict_record)\n",
    "            dict_clean = self.obj_backfill_helpers.clean_keys(input_dict=dict_flat)\n",
    "            dict_extension = self.obj_backfill_helpers.organize_by_prefix(dict_clean).get(\"Extension\", {})\n",
    "            dict_mega_flat = self.obj_backfill_helpers.further_flatten_geocoding(dict_data=dict_extension)\n",
    "            if any(re.search(r\"_\\d+_\", key) for key in dict_mega_flat.keys()):\n",
    "                list_dicts = self.obj_backfill_helpers.split_into_list_of_dictionaries(dict_data=dict_mega_flat)\n",
    "                for dict_ext in list_dicts:\n",
    "                    list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                        dict_data=dict_ext,\n",
    "                        subset_string=True,\n",
    "                        target_keys=[\n",
    "                            \"relevance\",\n",
    "                            \"match_type\",\n",
    "                            \"lat\",\n",
    "                            \"lng\",\n",
    "                            \"geocoding_date\",\n",
    "                            \"TopLeft.Latitude\",\n",
    "                            \"TopLeft.Longitude\",\n",
    "                            \"BottomRight.Latitude\",\n",
    "                            \"BottomRight.Longitude\",\n",
    "                            \"match_level\",\n",
    "                            \"mapped_street\",\n",
    "                            \"mapped_housenumber\",\n",
    "                            \"mapped_postalcode\",\n",
    "                            \"mapped_city\",\n",
    "                            \"mapped_district\",\n",
    "                            \"mapped_state\",\n",
    "                            \"mapped_country\"\n",
    "                        ]\n",
    "                    )\n",
    "                    list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "                    list_extension_data_tuples.append(tuple(list_output))\n",
    "            else:\n",
    "                list_output = self.obj_backfill_helpers.get_target_values(\n",
    "                    dict_data=dict_mega_flat,\n",
    "                    subset_string=True,\n",
    "                    target_keys=[\n",
    "                        \"relevance\",\n",
    "                        \"match_type\",\n",
    "                        \"lat\",\n",
    "                        \"lng\",\n",
    "                        \"geocoding_date\",\n",
    "                        \"TopLeft.Latitude\",\n",
    "                        \"TopLeft.Longitude\",\n",
    "                        \"BottomRight.Latitude\",\n",
    "                        \"BottomRight.Longitude\",\n",
    "                        \"match_level\",\n",
    "                        \"mapped_street\",\n",
    "                        \"mapped_housenumber\",\n",
    "                        \"mapped_postalcode\",\n",
    "                        \"mapped_city\",\n",
    "                        \"mapped_district\",\n",
    "                        \"mapped_state\",\n",
    "                        \"mapped_country\"\n",
    "                    ]\n",
    "                )\n",
    "                list_output.insert(0, dict_clean.get(\"LEI\"))\n",
    "                list_extension_data_tuples.append(tuple(list_output))\n",
    "        \n",
    "        self.bulk_insert_using_copy(\n",
    "            data=list_extension_data_tuples,\n",
    "            table_name=\"GLEIF_geocoding\",\n",
    "            columns=[\n",
    "                \"lei\",\n",
    "                \"relevance\",\n",
    "                \"match_type\",\n",
    "                \"lat\",\n",
    "                \"lng\",\n",
    "                \"geocoding_date\",\n",
    "                \"TopLeft_Latitude\",\n",
    "                \"TopLeft_Longitude\",\n",
    "                \"BottomRight_Latitude\",\n",
    "                \"BottomRight_longitude\",\n",
    "                \"match_level\",\n",
    "                \"mapped_street\",\n",
    "                \"mapped_housenumber\",\n",
    "                \"mapped_postalcode\",\n",
    "                \"mapped_city\",\n",
    "                \"mapped_district\",\n",
    "                \"mapped_state\",\n",
    "                \"mapped_country\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def process_all_data(self, list_dict_records):\n",
    "        \"\"\"\n",
    "        Processes all types of data and performs upserts/inserts accordingly.\n",
    "        \n",
    "        Args:\n",
    "            list_dict_records (list): List of record dictionaries.\n",
    "        \"\"\"\n",
    "        self.process_entity_data(list_dict_records=list_dict_records)\n",
    "        self.process_other_legal_names(list_dict_records=list_dict_records)\n",
    "        self.process_legal_address(list_dict_records=list_dict_records)\n",
    "        self.process_headquarters_address(list_dict_records=list_dict_records)\n",
    "        self.process_legal_entity_events(list_dict_records=list_dict_records)\n",
    "        self.process_registration_data(list_dict_records=list_dict_records)\n",
    "        self.process_geoencoding_data(list_dict_records=list_dict_records)\n",
    "\n",
    "    def remove_duplicates(self, table_name, unique_columns):\n",
    "        \"\"\"\n",
    "        Removes duplicate rows from a PostgreSQL table based on specified unique columns.\n",
    "        Keeps the row with the smallest id and deletes others.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to clean.\n",
    "            unique_columns (list of str): Columns that define a unique record.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Starting duplicate removal for table '{table_name}' based on columns {unique_columns}.\")\n",
    "            \n",
    "            # Construct the PARTITION BY clause\n",
    "            partition_by = sql.SQL(', ').join([sql.Identifier(col) for col in unique_columns])\n",
    "            \n",
    "            # Construct the DELETE query using ROW_NUMBER()\n",
    "            delete_query = sql.SQL(\"\"\"\n",
    "                DELETE FROM {table} a\n",
    "                USING (\n",
    "                    SELECT id, ROW_NUMBER() OVER (\n",
    "                        PARTITION BY {partition_by}\n",
    "                        ORDER BY id\n",
    "                    ) AS rnum\n",
    "                    FROM {table}\n",
    "                ) b\n",
    "                WHERE a.id = b.id AND b.rnum > 1;\n",
    "            \"\"\").format(\n",
    "                table=sql.Identifier(table_name),\n",
    "                partition_by=partition_by\n",
    "            )\n",
    "            \n",
    "            # Execute the DELETE query\n",
    "            self.cursor.execute(delete_query)\n",
    "            logging.info(f\"Duplicate removal completed for table '{table_name}'.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error removing duplicates from table '{table_name}': {e}\")\n",
    "            raise\n",
    "\n",
    "    def remove_duplicates(self, table_name, unique_columns):\n",
    "        \"\"\"\n",
    "        Removes duplicate rows from a PostgreSQL table based on specified unique columns.\n",
    "        Keeps the row with the smallest id and deletes others.\n",
    "\n",
    "        Args:\n",
    "            table_name (str): Name of the table to clean.\n",
    "            unique_columns (list of str): Columns that define a unique record.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logging.info(f\"Starting duplicate removal for table '{table_name}' based on columns {unique_columns}.\")\n",
    "\n",
    "            # Construct the PARTITION BY clause\n",
    "            partition_by = sql.SQL(', ').join([sql.Identifier(col) for col in unique_columns])\n",
    "\n",
    "            # Construct the DELETE query using ROW_NUMBER()\n",
    "            delete_query = sql.SQL(\"\"\"\n",
    "                DELETE FROM {table} a\n",
    "                USING (\n",
    "                    SELECT id, ROW_NUMBER() OVER (\n",
    "                        PARTITION BY {partition_by}\n",
    "                        ORDER BY id\n",
    "                    ) AS rnum\n",
    "                    FROM {table}\n",
    "                ) b\n",
    "                WHERE a.id = b.id AND b.rnum > 1;\n",
    "            \"\"\").format(\n",
    "                table=sql.Identifier(table_name),\n",
    "                partition_by=partition_by\n",
    "            )\n",
    "\n",
    "            # Execute the DELETE query\n",
    "            self.cursor.execute(delete_query)\n",
    "            logging.info(f\"Duplicate removal completed for table '{table_name}'.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error removing duplicates from table '{table_name}': {e}\")\n",
    "            raise\n",
    "\n",
    "    def clean_all_duplicates(self):\n",
    "        \"\"\"\n",
    "        Cleans duplicates from all specified tables.\n",
    "        Defines the tables and their unique columns.\n",
    "        \"\"\"\n",
    "        tables_to_clean = {\n",
    "            \"gleif_other_legal_names\": [\"lei\", \"otherentitynames\", \"type\"],\n",
    "            \"gleif_legalentityevents\": [\n",
    "                \"lei\",\n",
    "                \"group_type\",\n",
    "                \"event_status\",\n",
    "                \"legalentityeventtype\",\n",
    "                \"legalentityeventeffectivedate\",\n",
    "                \"legalentityeventrecordeddate\",\n",
    "                \"validationdocuments\"\n",
    "            ],\n",
    "            \"gleif_geocoding\": [\n",
    "                \"lei\",\n",
    "                \"relevance\",\n",
    "                \"match_type\",\n",
    "                \"lat\",\n",
    "                \"lng\",\n",
    "                \"geocoding_date\",\n",
    "                \"topleft_latitude\",\n",
    "                \"topleft_longitude\",\n",
    "                \"bottomright_latitude\",\n",
    "                \"bottomright_longitude\",\n",
    "                \"match_level\",\n",
    "                \"mapped_street\",\n",
    "                \"mapped_housenumber\",\n",
    "                \"mapped_postalcode\",\n",
    "                \"mapped_city\",\n",
    "                \"mapped_district\",\n",
    "                \"mapped_state\",\n",
    "                \"mapped_country\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        for table, unique_cols in tables_to_clean.items():\n",
    "            self.remove_duplicates(table, unique_cols)\n",
    "            logging.info(f\"Duplicates cleaned for table '{table}'.\")\n",
    "    \n",
    "    def storing_GLEIF_data_in_database(self):\n",
    "        \"\"\"\n",
    "        Stores GLEIF update data in the PostgreSQL database.\n",
    "        \"\"\"\n",
    "        with open(self.str_json_file_path, 'r', encoding='utf-8') as file:\n",
    "            dict_relationships = json.load(file)\n",
    "            records = dict_relationships[\"records\"]\n",
    "            \n",
    "            # Process records in batches\n",
    "            self.process_all_data(list_dict_records = records)\n",
    "        \n",
    "        # Commit the transaction\n",
    "        self.conn.commit()\n",
    "        self.clean_all_duplicates()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = GLEIFUpdateLevel1(bool_log = True)\n",
    "obj.storing_GLEIF_data_in_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_id_values(self, table_name):\n",
    "    \"\"\"\n",
    "    Resets the id column values of the specified table to be consecutive,\n",
    "    starting from 1 up to the number of rows, and resets the sequence accordingly.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Name of the table.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Begin a transaction\n",
    "        self.cursor.execute(\"BEGIN;\")\n",
    "        logging.info(f\"Started transaction for resetting id values in table '{table_name}'.\")\n",
    "        print(f\"Started transaction for resetting id values in table '{table_name}'.\")\n",
    "\n",
    "        # Create a temporary table with new id values\n",
    "        temp_table = f\"temp_{table_name}\"\n",
    "        self.cursor.execute(sql.SQL(\"\"\"\n",
    "            CREATE TEMP TABLE {temp_table} AS\n",
    "            SELECT\n",
    "                ROW_NUMBER() OVER (ORDER BY id) AS id,\n",
    "                t.*\n",
    "            FROM {table_name} t;\n",
    "        \"\"\").format(\n",
    "            temp_table=sql.Identifier(temp_table),\n",
    "            table_name=sql.Identifier(table_name)\n",
    "        ))\n",
    "        logging.info(f\"Temporary table '{temp_table}' created with new id values.\")\n",
    "        print(f\"Temporary table '{temp_table}' created with new id values.\")\n",
    "\n",
    "        # Truncate the original table\n",
    "        self.cursor.execute(sql.SQL(\"TRUNCATE TABLE {table_name} RESTART IDENTITY;\").format(\n",
    "            table_name=sql.Identifier(table_name)\n",
    "        ))\n",
    "        logging.info(f\"Table '{table_name}' truncated.\")\n",
    "        print(f\"Table '{table_name}' truncated.\")\n",
    "\n",
    "        # Insert data back into the original table\n",
    "        columns = self.get_table_columns(table_name)\n",
    "        columns_list = sql.SQL(', ').join([sql.Identifier(col) for col in columns])\n",
    "        self.cursor.execute(sql.SQL(\"\"\"\n",
    "            INSERT INTO {table_name} ({columns})\n",
    "            SELECT {columns} FROM {temp_table};\n",
    "        \"\"\").format(\n",
    "            table_name=sql.Identifier(table_name),\n",
    "            columns=columns_list,\n",
    "            temp_table=sql.Identifier(temp_table)\n",
    "        ))\n",
    "        logging.info(f\"Data inserted back into table '{table_name}' with new id values.\")\n",
    "        print(f\"Data inserted back into table '{table_name}' with new id values.\")\n",
    "\n",
    "        # Reset the sequence\n",
    "        self.cursor.execute(sql.SQL(\"\"\"\n",
    "            SELECT setval(pg_get_serial_sequence(%s, 'id'), (SELECT MAX(id) FROM {table_name}), true);\n",
    "        \"\"\").format(\n",
    "            table_name=sql.Identifier(table_name)\n",
    "        ), [table_name])\n",
    "        logging.info(f\"Sequence reset for table '{table_name}'.\")\n",
    "        print(f\"Sequence reset for table '{table_name}'.\")\n",
    "\n",
    "        # Commit the transaction\n",
    "        self.conn.commit()\n",
    "        logging.info(f\"Transaction committed for table '{table_name}'.\")\n",
    "        print(f\"Transaction committed for table '{table_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        self.conn.rollback()\n",
    "        logging.error(f\"Error resetting id values for table '{table_name}': {e}\")\n",
    "        print(f\"Error resetting id values for table '{table_name}': {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_id_values(table_name):\n",
    "    \"\"\"\n",
    "    Resets the id column values of the specified table to be consecutive,\n",
    "    starting from 1 up to the number of rows, and resets the sequence accordingly.\n",
    "\n",
    "    Args:\n",
    "        table_name (str): Name of the table.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Begin a transaction\n",
    "        obj.cursor.execute(\"BEGIN;\")\n",
    "        logging.info(f\"Started transaction for resetting id values in table '{table_name}'.\")\n",
    "        print(f\"Started transaction for resetting id values in table '{table_name}'.\")\n",
    "\n",
    "        # Create a temporary table with new id values\n",
    "        temp_table = f\"temp_{table_name}\"\n",
    "        obj.cursor.execute(sql.SQL(\"\"\"\n",
    "            CREATE TEMP TABLE {temp_table} AS\n",
    "            SELECT\n",
    "                ROW_NUMBER() OVER (ORDER BY id) AS id,\n",
    "                t.*\n",
    "            FROM {table_name} t;\n",
    "        \"\"\").format(\n",
    "            temp_table=sql.Identifier(temp_table),\n",
    "            table_name=sql.Identifier(table_name)\n",
    "        ))\n",
    "        logging.info(f\"Temporary table '{temp_table}' created with new id values.\")\n",
    "        print(f\"Temporary table '{temp_table}' created with new id values.\")\n",
    "\n",
    "        # Truncate the original table\n",
    "        obj.cursor.execute(sql.SQL(\"TRUNCATE TABLE {table_name} RESTART IDENTITY;\").format(\n",
    "            table_name=sql.Identifier(table_name)\n",
    "        ))\n",
    "        logging.info(f\"Table '{table_name}' truncated.\")\n",
    "        print(f\"Table '{table_name}' truncated.\")\n",
    "\n",
    "        # Insert data back into the original table\n",
    "        columns = obj.get_table_columns(table_name)\n",
    "        columns_list = sql.SQL(', ').join([sql.Identifier(col) for col in columns])\n",
    "        obj.cursor.execute(sql.SQL(\"\"\"\n",
    "            INSERT INTO {table_name} ({columns})\n",
    "            SELECT {columns} FROM {temp_table};\n",
    "        \"\"\").format(\n",
    "            table_name=sql.Identifier(table_name),\n",
    "            columns=columns_list,\n",
    "            temp_table=sql.Identifier(temp_table)\n",
    "        ))\n",
    "        logging.info(f\"Data inserted back into table '{table_name}' with new id values.\")\n",
    "        print(f\"Data inserted back into table '{table_name}' with new id values.\")\n",
    "\n",
    "        # Reset the sequence\n",
    "        obj.cursor.execute(sql.SQL(\"\"\"\n",
    "            SELECT setval(pg_get_serial_sequence(%s, 'id'), (SELECT MAX(id) FROM {table_name}), true);\n",
    "        \"\"\").format(\n",
    "            table_name=sql.Identifier(table_name)\n",
    "        ), [table_name])\n",
    "        logging.info(f\"Sequence reset for table '{table_name}'.\")\n",
    "        print(f\"Sequence reset for table '{table_name}'.\")\n",
    "\n",
    "        # Commit the transaction\n",
    "        obj.conn.commit()\n",
    "        logging.info(f\"Transaction committed for table '{table_name}'.\")\n",
    "        print(f\"Transaction committed for table '{table_name}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        obj.conn.rollback()\n",
    "        logging.error(f\"Error resetting id values for table '{table_name}': {e}\")\n",
    "        print(f\"Error resetting id values for table '{table_name}': {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence for 'gleif_entity_data.id' reset to 906289.\n"
     ]
    }
   ],
   "source": [
    "reset_id_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulk_insert_using_copy(self , table_name , columns, data):\n",
    "        \"\"\"Perform a bulk insert using PostgreSQL COPY with an in-memory buffer\n",
    "\n",
    "        Args:\n",
    "            table_name (_type_): Name of the table to insert into\n",
    "            columns (_type_): List of column names for the table\n",
    "            data (_type_): List of tuples with the data to be inserted\n",
    "        \"\"\"\n",
    "        \n",
    "        buffer = io.StringIO()\n",
    "        \n",
    "        #write data to the buffer\n",
    "        \n",
    "        for row in data:\n",
    "            '''row_converted = [\n",
    "            x.replace('\\\\', '\\\\\\\\') if isinstance(x, str) else x \n",
    "            for x in row]'''\n",
    "        # Replace None with \\N for PostgreSQL NULL representation\n",
    "        #row_converted = [str(x) if x is not None else '\\\\N' for x in row_converted]\n",
    "            #buffer.write('\\t'.join(row_converted) + \"\\n\")\n",
    "            #buffer.write('\\t'.join(map(str , row_converted)) + \"\\n\")\n",
    "            #buffer.write('\\t'.join(row_converted) + \"\\n\")\n",
    "            buffer.write('\\t'.join(map(str , row)) + \"\\n\")\n",
    "        buffer.seek(0) #reset buffer position to the beginning\n",
    "        \n",
    "        #Construct the copy query\n",
    "        #copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH DELIMITER '\\t', NULL '\\\\N'\"\n",
    "        \n",
    "        #copy_query = f\"\"\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH (FORMAT text, DELIMITER E'\\\\t', NULL '\\\\N')\"\"\"\n",
    "        copy_query = f\"COPY {table_name} ({', '.join(columns)}) FROM STDIN WITH DELIMITER '\\t'\"\n",
    "        self.cursor.copy_expert(copy_query , buffer)\n",
    "        self.conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(obj.str_json_file_path, 'r', encoding='utf-8') as file:\n",
    "        dict_relationships = json.load(file)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_dicts = dict_relationships[\"records\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LEI': {'$': '0VNOQKIRP7AG4HT7MV54'},\n",
       "  'Entity': {'LegalName': {'@xml:lang': 'en', '$': 'ENABLE PRODUCTS, LLC'},\n",
       "   'LegalAddress': {'@xml:lang': 'en',\n",
       "    'FirstAddressLine': {'$': 'C/O CORPORATION SERVICE COMPANY'},\n",
       "    'City': {'$': 'OKLAHOMA CITY'},\n",
       "    'Region': {'$': 'US-OK'},\n",
       "    'Country': {'$': 'US'},\n",
       "    'PostalCode': {'$': '73159'}},\n",
       "   'HeadquartersAddress': {'@xml:lang': 'en',\n",
       "    'FirstAddressLine': {'$': 'Bok Park Plaza'},\n",
       "    'City': {'$': 'Oklahoma City'},\n",
       "    'Region': {'$': 'US-OK'},\n",
       "    'Country': {'$': 'US'},\n",
       "    'PostalCode': {'$': '73102'}},\n",
       "   'RegistrationAuthority': {'RegistrationAuthorityID': {'$': 'RA000630'},\n",
       "    'RegistrationAuthorityEntityID': {'$': '3512174322'}},\n",
       "   'LegalJurisdiction': {'$': 'US-OK'},\n",
       "   'EntityCategory': {'$': 'GENERAL'},\n",
       "   'LegalForm': {'EntityLegalFormCode': {'$': 'B8XC'}},\n",
       "   'EntityStatus': {'$': 'INACTIVE'},\n",
       "   'EntityCreationDate': {'$': '2008-03-31T00:00:00.000Z'},\n",
       "   'LegalEntityEvents': {'LegalEntityEvent': [{'@group_type': 'STANDALONE',\n",
       "      '@event_status': 'COMPLETED',\n",
       "      'LegalEntityEventType': {'$': 'CHANGE_LEGAL_ADDRESS'},\n",
       "      'LegalEntityEventEffectiveDate': {'$': '2021-12-27T00:00:00.000Z'},\n",
       "      'LegalEntityEventRecordedDate': {'$': '2022-04-11T07:23:54.000Z'},\n",
       "      'ValidationDocuments': {'$': 'REGULATORY_FILING'}},\n",
       "     {'@group_type': 'STANDALONE',\n",
       "      '@event_status': 'COMPLETED',\n",
       "      'LegalEntityEventType': {'$': 'DISSOLUTION'},\n",
       "      'LegalEntityEventEffectiveDate': {'$': '2024-11-29T00:00:00.000Z'},\n",
       "      'LegalEntityEventRecordedDate': {'$': '2024-11-29T00:00:00.000Z'},\n",
       "      'ValidationDocuments': {'$': 'SUPPORTING_DOCUMENTS'}}]}},\n",
       "  'Registration': {'InitialRegistrationDate': {'$': '2012-10-23T18:52:00.000Z'},\n",
       "   'LastUpdateDate': {'$': '2024-11-29T13:25:30.954Z'},\n",
       "   'RegistrationStatus': {'$': 'RETIRED'},\n",
       "   'NextRenewalDate': {'$': '2023-04-07T15:49:00.000Z'},\n",
       "   'ManagingLOU': {'$': '5493001KJTIIGC8Y1R12'},\n",
       "   'ValidationSources': {'$': 'FULLY_CORROBORATED'},\n",
       "   'ValidationAuthority': {'ValidationAuthorityID': {'$': 'RA000630'},\n",
       "    'ValidationAuthorityEntityID': {'$': '3512174322'}}},\n",
       "  'Extension': {'gleif:Geocoding': [{'gleif:original_address': {'$': '73159, OKLAHOMA CITY, US-OK, US'},\n",
       "     'gleif:relevance': {'$': '0.9'},\n",
       "     'gleif:lat': {'$': '35.38591'},\n",
       "     'gleif:lng': {'$': '-97.54787'},\n",
       "     'gleif:geocoding_date': {'$': '2023-04-09T02:46:28'},\n",
       "     'gleif:bounding_box': {'$': 'TopLeft.Latitude: 35.41245, TopLeft.Longitude: -97.61847, BottomRight.Latitude: 35.35671, BottomRight.Longitude: -97.53469'},\n",
       "     'gleif:match_level': {'$': 'postalCode'},\n",
       "     'gleif:formatted_address': {'$': '73159, Oklahoma City, OK, United States'},\n",
       "     'gleif:mapped_location_id': {'$': 'NT_zJKf6xhfmEfrGC9Mk9Od1C'},\n",
       "     'gleif:mapped_postalcode': {'$': '73159'},\n",
       "     'gleif:mapped_city': {'$': 'Oklahoma City'},\n",
       "     'gleif:mapped_state': {'$': 'OK'},\n",
       "     'gleif:mapped_country': {'$': 'USA'}},\n",
       "    {'gleif:original_address': {'$': 'Bok Park Plaza, 73102, Oklahoma City, US-OK, US'},\n",
       "     'gleif:geocoding_failed': {'$': 'Ambigous geocoding results'},\n",
       "     'gleif:geocoding_date': {'$': '2023-04-09T02:46:28'}}],\n",
       "   'gleif:conformity': {'gleif:conformityflag': {'$': 'NON_CONFORMING'}}}},\n",
       " {'LEI': {'$': '1595864AH8IQOAO8MN68'},\n",
       "  'Entity': {'LegalName': {'@xml:lang': 'da',\n",
       "    '$': 'Tommy Schmidt Hansen Holding ApS'},\n",
       "   'LegalAddress': {'@xml:lang': 'da',\n",
       "    'FirstAddressLine': {'$': 'Stenkærgård 5'},\n",
       "    'City': {'$': 'Rask Mølle'},\n",
       "    'Country': {'$': 'DK'},\n",
       "    'PostalCode': {'$': '8763'}},\n",
       "   'HeadquartersAddress': {'@xml:lang': 'da',\n",
       "    'FirstAddressLine': {'$': 'Stenkærgård 5'},\n",
       "    'City': {'$': 'Rask Mølle'},\n",
       "    'Country': {'$': 'DK'},\n",
       "    'PostalCode': {'$': '8763'}},\n",
       "   'RegistrationAuthority': {'RegistrationAuthorityID': {'$': 'RA000170'},\n",
       "    'RegistrationAuthorityEntityID': {'$': '45103803'}},\n",
       "   'LegalJurisdiction': {'$': 'DK'},\n",
       "   'EntityCategory': {'$': 'GENERAL'},\n",
       "   'LegalForm': {'EntityLegalFormCode': {'$': 'H8VP'}},\n",
       "   'EntityStatus': {'$': 'ACTIVE'},\n",
       "   'EntityCreationDate': {'$': '2024-09-26T00:00:00+00:00'}},\n",
       "  'Registration': {'InitialRegistrationDate': {'$': '2024-11-29T13:15:29+01:00'},\n",
       "   'LastUpdateDate': {'$': '2024-11-29T13:15:29+01:00'},\n",
       "   'RegistrationStatus': {'$': 'ISSUED'},\n",
       "   'NextRenewalDate': {'$': '2025-11-29T13:15:29+01:00'},\n",
       "   'ManagingLOU': {'$': '98450045AN5EB5FDC780'},\n",
       "   'ValidationSources': {'$': 'FULLY_CORROBORATED'},\n",
       "   'ValidationAuthority': {'ValidationAuthorityID': {'$': 'RA000170'},\n",
       "    'ValidationAuthorityEntityID': {'$': '45103803'}}},\n",
       "  'Extension': {'gleif:Geocoding': {'gleif:original_address': {'$': 'Stenkærgård 5, 8763, Rask Mølle, DK'},\n",
       "    'gleif:relevance': {'$': '1.0'},\n",
       "    'gleif:match_type': {'$': 'pointAddress'},\n",
       "    'gleif:lat': {'$': '55.86593'},\n",
       "    'gleif:lng': {'$': '9.60827'},\n",
       "    'gleif:geocoding_date': {'$': '2024-11-29T17:58:34'},\n",
       "    'gleif:bounding_box': {'$': 'TopLeft.Latitude: 55.8670542, TopLeft.Longitude: 9.6062666, BottomRight.Latitude: 55.8648058, BottomRight.Longitude: 9.6102734'},\n",
       "    'gleif:match_level': {'$': 'houseNumber'},\n",
       "    'gleif:formatted_address': {'$': 'Stenkærgård 5, 8763 Rask Mølle, Danmark'},\n",
       "    'gleif:mapped_location_id': {'$': 'NT_UfG3HF-Y-biWRvoQFeaymC_1A'},\n",
       "    'gleif:mapped_street': {'$': 'Stenkærgård'},\n",
       "    'gleif:mapped_housenumber': {'$': '5'},\n",
       "    'gleif:mapped_postalcode': {'$': '8763'},\n",
       "    'gleif:mapped_city': {'$': 'Rask Mølle'},\n",
       "    'gleif:mapped_state': {'$': 'Midtjylland'},\n",
       "    'gleif:mapped_country': {'$': 'DNK'}}}},\n",
       " {'LEI': {'$': '1595B3V94KSXX9W1SL31'},\n",
       "  'Entity': {'LegalName': {'@xml:lang': 'sv', '$': 'Wilhelm von Essen AB'},\n",
       "   'LegalAddress': {'@xml:lang': 'sv',\n",
       "    'FirstAddressLine': {'$': 'Salsta Trädgård'},\n",
       "    'City': {'$': 'VATTHOLMA'},\n",
       "    'Country': {'$': 'SE'},\n",
       "    'PostalCode': {'$': '743 92'}},\n",
       "   'HeadquartersAddress': {'@xml:lang': 'sv',\n",
       "    'FirstAddressLine': {'$': 'Salsta Trädgård'},\n",
       "    'City': {'$': 'VATTHOLMA'},\n",
       "    'Country': {'$': 'SE'},\n",
       "    'PostalCode': {'$': '743 92'}},\n",
       "   'RegistrationAuthority': {'RegistrationAuthorityID': {'$': 'RA000544'},\n",
       "    'RegistrationAuthorityEntityID': {'$': '5566988829'}},\n",
       "   'LegalJurisdiction': {'$': 'SE'},\n",
       "   'EntityCategory': {'$': 'GENERAL'},\n",
       "   'LegalForm': {'EntityLegalFormCode': {'$': 'XJHM'}},\n",
       "   'EntityStatus': {'$': 'ACTIVE'},\n",
       "   'EntityCreationDate': {'$': '2006-03-01T00:00:00+00:00'}},\n",
       "  'Registration': {'InitialRegistrationDate': {'$': '2023-06-01T15:08:53+02:00'},\n",
       "   'LastUpdateDate': {'$': '2024-11-29T12:30:51+01:00'},\n",
       "   'RegistrationStatus': {'$': 'ISSUED'},\n",
       "   'NextRenewalDate': {'$': '2025-11-29T12:30:51+01:00'},\n",
       "   'ManagingLOU': {'$': '98450045AN5EB5FDC780'},\n",
       "   'ValidationSources': {'$': 'FULLY_CORROBORATED'},\n",
       "   'ValidationAuthority': {'ValidationAuthorityID': {'$': 'RA000544'},\n",
       "    'ValidationAuthorityEntityID': {'$': '5566988829'}}},\n",
       "  'Extension': {'gleif:Geocoding': {'gleif:original_address': {'$': 'Salsta Trädgård, 743 92, VATTHOLMA, SE'},\n",
       "    'gleif:relevance': {'$': '0.73'},\n",
       "    'gleif:lat': {'$': '60.0469'},\n",
       "    'gleif:lng': {'$': '17.74569'},\n",
       "    'gleif:geocoding_date': {'$': '2023-06-01T18:20:53'},\n",
       "    'gleif:bounding_box': {'$': 'TopLeft.Latitude: 60.06039, TopLeft.Longitude: 17.71867, BottomRight.Latitude: 60.03341, BottomRight.Longitude: 17.77271'},\n",
       "    'gleif:match_level': {'$': 'district'},\n",
       "    'gleif:formatted_address': {'$': 'Lena-Salsta, Vattholma, Uppsala län, Sverige'},\n",
       "    'gleif:mapped_location_id': {'$': 'NT_aG5939U.4MpXoCAGHC0RBA'},\n",
       "    'gleif:mapped_postalcode': {'$': '743 92'},\n",
       "    'gleif:mapped_city': {'$': 'Vattholma'},\n",
       "    'gleif:mapped_district': {'$': 'Lena-Salsta'},\n",
       "    'gleif:mapped_state': {'$': 'Uppsala län'},\n",
       "    'gleif:mapped_country': {'$': 'SWE'}},\n",
       "   'gleif:conformity': {'gleif:conformityflag': {'$': 'NON_CONFORMING'}}}},\n",
       " {'LEI': {'$': '1595CJDQMRPFQK0O1227'},\n",
       "  'Entity': {'LegalName': {'@xml:lang': 'da', '$': 'LKAN ApS'},\n",
       "   'LegalAddress': {'@xml:lang': 'da',\n",
       "    'FirstAddressLine': {'$': 'Tjelevej 13, 7. th'},\n",
       "    'MailRouting': {'$': 'C/O Per nørbo'},\n",
       "    'City': {'$': 'Risskov'},\n",
       "    'Country': {'$': 'DK'},\n",
       "    'PostalCode': {'$': '8240'}},\n",
       "   'HeadquartersAddress': {'@xml:lang': 'da',\n",
       "    'FirstAddressLine': {'$': 'Tjelevej 13, 7. th'},\n",
       "    'MailRouting': {'$': 'C/O Per nørbo'},\n",
       "    'City': {'$': 'Risskov'},\n",
       "    'Country': {'$': 'DK'},\n",
       "    'PostalCode': {'$': '8240'}},\n",
       "   'RegistrationAuthority': {'RegistrationAuthorityID': {'$': 'RA000170'},\n",
       "    'RegistrationAuthorityEntityID': {'$': '26833442'}},\n",
       "   'LegalJurisdiction': {'$': 'DK'},\n",
       "   'EntityCategory': {'$': 'GENERAL'},\n",
       "   'LegalForm': {'EntityLegalFormCode': {'$': 'H8VP'}},\n",
       "   'EntityStatus': {'$': 'ACTIVE'},\n",
       "   'EntityCreationDate': {'$': '2002-10-18T00:00:00+00:00'}},\n",
       "  'Registration': {'InitialRegistrationDate': {'$': '2023-11-29T13:41:35+01:00'},\n",
       "   'LastUpdateDate': {'$': '2024-11-29T17:00:08+01:00'},\n",
       "   'RegistrationStatus': {'$': 'LAPSED'},\n",
       "   'NextRenewalDate': {'$': '2024-11-29T13:41:35+01:00'},\n",
       "   'ManagingLOU': {'$': '98450045AN5EB5FDC780'},\n",
       "   'ValidationSources': {'$': 'FULLY_CORROBORATED'},\n",
       "   'ValidationAuthority': {'ValidationAuthorityID': {'$': 'RA000170'},\n",
       "    'ValidationAuthorityEntityID': {'$': '26833442'}}},\n",
       "  'Extension': {'gleif:Geocoding': {'gleif:original_address': {'$': 'Tjelevej 13, 7. th, 8240, Risskov, DK'},\n",
       "    'gleif:relevance': {'$': '0.84'},\n",
       "    'gleif:match_type': {'$': 'pointAddress'},\n",
       "    'gleif:lat': {'$': '56.19095'},\n",
       "    'gleif:lng': {'$': '10.21794'},\n",
       "    'gleif:geocoding_date': {'$': '2024-01-23T10:26:11'},\n",
       "    'gleif:bounding_box': {'$': 'TopLeft.Latitude: 56.1920742, TopLeft.Longitude: 10.2159197, BottomRight.Latitude: 56.1898258, BottomRight.Longitude: 10.2199603'},\n",
       "    'gleif:match_level': {'$': 'houseNumber'},\n",
       "    'gleif:formatted_address': {'$': 'Tjelevej 13, 8240 Risskov, Danmark'},\n",
       "    'gleif:mapped_location_id': {'$': 'NT_G-NwSp9mcDj.Jjshev.O8A_xMD'},\n",
       "    'gleif:mapped_street': {'$': 'Tjelevej'},\n",
       "    'gleif:mapped_housenumber': {'$': '13'},\n",
       "    'gleif:mapped_postalcode': {'$': '8240'},\n",
       "    'gleif:mapped_city': {'$': 'Risskov'},\n",
       "    'gleif:mapped_district': {'$': 'Vejlby'},\n",
       "    'gleif:mapped_state': {'$': 'Midtjylland'},\n",
       "    'gleif:mapped_country': {'$': 'DNK'}},\n",
       "   'gleif:conformity': {'gleif:conformityflag': {'$': 'CONFORMING'}}}},\n",
       " {'LEI': {'$': '1595HMSJARF7E3QO8229'},\n",
       "  'Entity': {'LegalName': {'@xml:lang': 'no', '$': 'MYSPOT LIVING 2 AS'},\n",
       "   'LegalAddress': {'@xml:lang': 'no',\n",
       "    'FirstAddressLine': {'$': 'Prinsessevegen 6A'},\n",
       "    'City': {'$': 'RANHEIM'},\n",
       "    'Country': {'$': 'NO'},\n",
       "    'PostalCode': {'$': '7056'}},\n",
       "   'HeadquartersAddress': {'@xml:lang': 'no',\n",
       "    'FirstAddressLine': {'$': 'Prinsessevegen 6A'},\n",
       "    'City': {'$': 'RANHEIM'},\n",
       "    'Country': {'$': 'NO'},\n",
       "    'PostalCode': {'$': '7056'}},\n",
       "   'RegistrationAuthority': {'RegistrationAuthorityID': {'$': 'RA000472'},\n",
       "    'RegistrationAuthorityEntityID': {'$': '929092104'}},\n",
       "   'LegalJurisdiction': {'$': 'NO'},\n",
       "   'EntityCategory': {'$': 'GENERAL'},\n",
       "   'LegalForm': {'EntityLegalFormCode': {'$': 'YI42'}},\n",
       "   'EntityStatus': {'$': 'ACTIVE'},\n",
       "   'EntityCreationDate': {'$': '2022-03-11T00:00:00+00:00'}},\n",
       "  'Registration': {'InitialRegistrationDate': {'$': '2024-01-16T10:21:33+01:00'},\n",
       "   'LastUpdateDate': {'$': '2024-11-29T10:32:17+01:00'},\n",
       "   'RegistrationStatus': {'$': 'ISSUED'},\n",
       "   'NextRenewalDate': {'$': '2026-01-16T10:21:33+01:00'},\n",
       "   'ManagingLOU': {'$': '98450045AN5EB5FDC780'},\n",
       "   'ValidationSources': {'$': 'FULLY_CORROBORATED'},\n",
       "   'ValidationAuthority': {'ValidationAuthorityID': {'$': 'RA000472'},\n",
       "    'ValidationAuthorityEntityID': {'$': '929092104'}}},\n",
       "  'Extension': {'gleif:Geocoding': {'gleif:original_address': {'$': 'Prinsessevegen 6A, 7056, RANHEIM, NO'},\n",
       "    'gleif:relevance': {'$': '1.0'},\n",
       "    'gleif:match_type': {'$': 'pointAddress'},\n",
       "    'gleif:lat': {'$': '63.42972'},\n",
       "    'gleif:lng': {'$': '10.51039'},\n",
       "    'gleif:geocoding_date': {'$': '2022-05-13T17:47:57'},\n",
       "    'gleif:bounding_box': {'$': 'TopLeft.Latitude: 63.4308442, TopLeft.Longitude: 10.5078768, BottomRight.Latitude: 63.4285958, BottomRight.Longitude: 10.5129032'},\n",
       "    'gleif:match_level': {'$': 'houseNumber'},\n",
       "    'gleif:formatted_address': {'$': 'Prinsessevegen 6A, 7056 Ranheim, Norge'},\n",
       "    'gleif:mapped_location_id': {'$': 'NT_FFo-CuUP97uyHe3gP.UPGD_2EE'},\n",
       "    'gleif:mapped_street': {'$': 'Prinsessevegen'},\n",
       "    'gleif:mapped_housenumber': {'$': '6A'},\n",
       "    'gleif:mapped_postalcode': {'$': '7056'},\n",
       "    'gleif:mapped_city': {'$': 'Ranheim'},\n",
       "    'gleif:mapped_state': {'$': 'Trøndelag'},\n",
       "    'gleif:mapped_country': {'$': 'NOR'}},\n",
       "   'gleif:conformity': {'gleif:conformityflag': {'$': 'NON_CONFORMING'}}}},\n",
       " {'LEI': {'$': '1595JGGE7U5KF8NES079'},\n",
       "  'Entity': {'LegalName': {'@xml:lang': 'da', '$': 'KORE HOLDING ApS'},\n",
       "   'LegalAddress': {'@xml:lang': 'da',\n",
       "    'FirstAddressLine': {'$': 'Hasselvej 18'},\n",
       "    'MailRouting': {'$': 'C/O Kim Olvar Ebdrup'},\n",
       "    'City': {'$': 'Skovlunde'},\n",
       "    'Country': {'$': 'DK'},\n",
       "    'PostalCode': {'$': '2740'}},\n",
       "   'HeadquartersAddress': {'@xml:lang': 'da',\n",
       "    'FirstAddressLine': {'$': 'Hasselvej 18'},\n",
       "    'MailRouting': {'$': 'C/O Kim Olvar Ebdrup'},\n",
       "    'City': {'$': 'Skovlunde'},\n",
       "    'Country': {'$': 'DK'},\n",
       "    'PostalCode': {'$': '2740'}},\n",
       "   'RegistrationAuthority': {'RegistrationAuthorityID': {'$': 'RA000170'},\n",
       "    'RegistrationAuthorityEntityID': {'$': '35873627'}},\n",
       "   'LegalJurisdiction': {'$': 'DK'},\n",
       "   'EntityCategory': {'$': 'GENERAL'},\n",
       "   'LegalForm': {'EntityLegalFormCode': {'$': 'H8VP'}},\n",
       "   'EntityStatus': {'$': 'ACTIVE'},\n",
       "   'EntityCreationDate': {'$': '2014-12-19T00:00:00+00:00'}},\n",
       "  'Registration': {'InitialRegistrationDate': {'$': '2022-10-18T17:24:10+02:00'},\n",
       "   'LastUpdateDate': {'$': '2024-11-29T13:14:49+01:00'},\n",
       "   'RegistrationStatus': {'$': 'ISSUED'},\n",
       "   'NextRenewalDate': {'$': '2025-11-29T13:14:49+01:00'},\n",
       "   'ManagingLOU': {'$': '98450045AN5EB5FDC780'},\n",
       "   'ValidationSources': {'$': 'FULLY_CORROBORATED'},\n",
       "   'ValidationAuthority': {'ValidationAuthorityID': {'$': 'RA000170'},\n",
       "    'ValidationAuthorityEntityID': {'$': '35873627'}}},\n",
       "  'Extension': {'gleif:Geocoding': {'gleif:original_address': {'$': 'Hasselvej 18, 2740, Skovlunde, DK'},\n",
       "    'gleif:relevance': {'$': '1.0'},\n",
       "    'gleif:match_type': {'$': 'pointAddress'},\n",
       "    'gleif:lat': {'$': '55.71339'},\n",
       "    'gleif:lng': {'$': '12.39678'},\n",
       "    'gleif:geocoding_date': {'$': '2023-02-23T06:26:05'},\n",
       "    'gleif:bounding_box': {'$': 'TopLeft.Latitude: 55.7145142, TopLeft.Longitude: 12.3947845, BottomRight.Latitude: 55.7122658, BottomRight.Longitude: 12.3987755'},\n",
       "    'gleif:match_level': {'$': 'houseNumber'},\n",
       "    'gleif:formatted_address': {'$': 'Hasselvej 18, 2740 Skovlunde, Danmark'},\n",
       "    'gleif:mapped_location_id': {'$': 'NT_LwYUs3N3pio64yDI3ZfBdC_xgD'},\n",
       "    'gleif:mapped_street': {'$': 'Hasselvej'},\n",
       "    'gleif:mapped_housenumber': {'$': '18'},\n",
       "    'gleif:mapped_postalcode': {'$': '2740'},\n",
       "    'gleif:mapped_city': {'$': 'Skovlunde'},\n",
       "    'gleif:mapped_state': {'$': 'Hovedstaden'},\n",
       "    'gleif:mapped_country': {'$': 'DNK'}},\n",
       "   'gleif:conformity': {'gleif:conformityflag': {'$': 'NON_CONFORMING'}}}},\n",
       " {'LEI': {'$': '1595LKL6FISVG5YI8240'},\n",
       "  'Entity': {'LegalName': {'@xml:lang': 'da', '$': 'A/B Havkik'},\n",
       "   'LegalAddress': {'@xml:lang': 'da',\n",
       "    'FirstAddressLine': {'$': 'Søstien 3A'},\n",
       "    'MailRouting': {'$': 'C/O Kredsadministration F.M.B.A.'},\n",
       "    'City': {'$': 'Birkerød'},\n",
       "    'Country': {'$': 'DK'},\n",
       "    'PostalCode': {'$': '3460'}},\n",
       "   'HeadquartersAddress': {'@xml:lang': 'da',\n",
       "    'FirstAddressLine': {'$': 'Søstien 3A'},\n",
       "    'MailRouting': {'$': 'C/O Kredsadministration F.M.B.A.'},\n",
       "    'City': {'$': 'Birkerød'},\n",
       "    'Country': {'$': 'DK'},\n",
       "    'PostalCode': {'$': '3460'}},\n",
       "   'RegistrationAuthority': {'RegistrationAuthorityID': {'$': 'RA000170'},\n",
       "    'RegistrationAuthorityEntityID': {'$': '30851749'}},\n",
       "   'LegalJurisdiction': {'$': 'DK'},\n",
       "   'EntityCategory': {'$': 'GENERAL'},\n",
       "   'LegalForm': {'EntityLegalFormCode': {'$': '40R4'}},\n",
       "   'EntityStatus': {'$': 'ACTIVE'},\n",
       "   'EntityCreationDate': {'$': '2007-09-26T00:00:00+00:00'}},\n",
       "  'Registration': {'InitialRegistrationDate': {'$': '2023-11-29T09:25:43+01:00'},\n",
       "   'LastUpdateDate': {'$': '2024-11-29T17:00:08+01:00'},\n",
       "   'RegistrationStatus': {'$': 'LAPSED'},\n",
       "   'NextRenewalDate': {'$': '2024-11-29T09:25:43+01:00'},\n",
       "   'ManagingLOU': {'$': '98450045AN5EB5FDC780'},\n",
       "   'ValidationSources': {'$': 'FULLY_CORROBORATED'},\n",
       "   'ValidationAuthority': {'ValidationAuthorityID': {'$': 'RA000170'},\n",
       "    'ValidationAuthorityEntityID': {'$': '30851749'}}},\n",
       "  'Extension': {'gleif:Geocoding': {'gleif:original_address': {'$': 'Søstien 3A, 3460, Birkerød, DK'},\n",
       "    'gleif:relevance': {'$': '1.0'},\n",
       "    'gleif:match_type': {'$': 'pointAddress'},\n",
       "    'gleif:lat': {'$': '55.83807'},\n",
       "    'gleif:lng': {'$': '12.4242'},\n",
       "    'gleif:geocoding_date': {'$': '2024-01-23T10:32:17'},\n",
       "    'gleif:bounding_box': {'$': 'TopLeft.Latitude: 55.8391942, TopLeft.Longitude: 12.4221981, BottomRight.Latitude: 55.8369458, BottomRight.Longitude: 12.4262019'},\n",
       "    'gleif:match_level': {'$': 'houseNumber'},\n",
       "    'gleif:formatted_address': {'$': 'Søstien 3A, 3460 Birkerød, Danmark'},\n",
       "    'gleif:mapped_location_id': {'$': 'NT_1aYznFWOIcRosPezAx.SsB_zEE'},\n",
       "    'gleif:mapped_street': {'$': 'Søstien'},\n",
       "    'gleif:mapped_housenumber': {'$': '3A'},\n",
       "    'gleif:mapped_postalcode': {'$': '3460'},\n",
       "    'gleif:mapped_city': {'$': 'Birkerød'},\n",
       "    'gleif:mapped_state': {'$': 'Hovedstaden'},\n",
       "    'gleif:mapped_country': {'$': 'DNK'}},\n",
       "   'gleif:conformity': {'gleif:conformityflag': {'$': 'CONFORMING'}}}},\n",
       " {'LEI': {'$': '15LP308NQGOU658AIG33'},\n",
       "  'Entity': {'LegalName': {'@xml:lang': 'fr',\n",
       "    '$': 'GOLDMAN SACHS US FIXED INCOME PORTFOLIO'},\n",
       "   'LegalAddress': {'@xml:lang': 'fr',\n",
       "    'FirstAddressLine': {'$': '49 Avenue J. F. Kennedy'},\n",
       "    'City': {'$': 'Luxembourg'},\n",
       "    'Region': {'$': 'LU-LU'},\n",
       "    'Country': {'$': 'LU'},\n",
       "    'PostalCode': {'$': '1855'}},\n",
       "   'HeadquartersAddress': {'@xml:lang': 'nl',\n",
       "    'FirstAddressLine': {'$': 'c/o Goldman Sachs Asset Management B.V. Prinses Beatrixlaan 35'},\n",
       "    'MailRouting': {'$': 'c/o Goldman Sachs Asset Management B.V.'},\n",
       "    'City': {'$': \"'s-Gravenhage\"},\n",
       "    'Region': {'$': 'NL-ZH'},\n",
       "    'Country': {'$': 'NL'},\n",
       "    'PostalCode': {'$': '2595 AK'}},\n",
       "   'RegistrationAuthority': {'RegistrationAuthorityID': {'$': 'RA000433'},\n",
       "    'RegistrationAuthorityEntityID': {'$': 'O00001204_00000016'}},\n",
       "   'LegalJurisdiction': {'$': 'LU'},\n",
       "   'EntityCategory': {'$': 'FUND'},\n",
       "   'LegalForm': {'EntityLegalFormCode': {'$': 'UDY2'}},\n",
       "   'EntityStatus': {'$': 'ACTIVE'},\n",
       "   'EntityCreationDate': {'$': '1998-07-14T00:00:00+00:00'},\n",
       "   'LegalEntityEvents': {'LegalEntityEvent': [{'@group_type': 'STANDALONE',\n",
       "      '@event_status': 'COMPLETED',\n",
       "      'LegalEntityEventType': {'$': 'CHANGE_HQ_ADDRESS'},\n",
       "      'LegalEntityEventEffectiveDate': {'$': '2023-11-01T00:00:00+00:00'},\n",
       "      'LegalEntityEventRecordedDate': {'$': '2024-11-29T18:04:17+00:00'},\n",
       "      'ValidationDocuments': {'$': 'SUPPORTING_DOCUMENTS'}}]}},\n",
       "  'Registration': {'InitialRegistrationDate': {'$': '2012-08-28T16:00:00+00:00'},\n",
       "   'LastUpdateDate': {'$': '2024-11-29T18:04:17+00:00'},\n",
       "   'RegistrationStatus': {'$': 'ISSUED'},\n",
       "   'NextRenewalDate': {'$': '2025-10-05T12:52:13+00:00'},\n",
       "   'ManagingLOU': {'$': '529900T8BM49AURSDO55'},\n",
       "   'ValidationSources': {'$': 'PARTIALLY_CORROBORATED'},\n",
       "   'ValidationAuthority': {'ValidationAuthorityID': {'$': 'RA000433'},\n",
       "    'ValidationAuthorityEntityID': {'$': 'O00001204_00000016'}}},\n",
       "  'Extension': {'gleif:Geocoding': [{'gleif:original_address': {'$': '49 Avenue J. F. Kennedy, 1855, Luxembourg, LU-LU, LU'},\n",
       "     'gleif:relevance': {'$': '0.83'},\n",
       "     'gleif:match_type': {'$': 'pointAddress'},\n",
       "     'gleif:lat': {'$': '49.63476'},\n",
       "     'gleif:lng': {'$': '6.1745'},\n",
       "     'gleif:geocoding_date': {'$': '2024-02-09T04:43:59'},\n",
       "     'gleif:bounding_box': {'$': 'TopLeft.Latitude: 49.6358842, TopLeft.Longitude: 6.1727643, BottomRight.Latitude: 49.6336358, BottomRight.Longitude: 6.1762357'},\n",
       "     'gleif:match_level': {'$': 'houseNumber'},\n",
       "     'gleif:formatted_address': {'$': '49 Avenue John F. Kennedy, L-1855 Luxembourg, Luxembourg'},\n",
       "     'gleif:mapped_location_id': {'$': 'NT_ODAqlmED2Sa45eHcC1bLwC_0kD'},\n",
       "     'gleif:mapped_street': {'$': 'Avenue John F. Kennedy'},\n",
       "     'gleif:mapped_housenumber': {'$': '49'},\n",
       "     'gleif:mapped_postalcode': {'$': '1855'},\n",
       "     'gleif:mapped_city': {'$': 'Luxembourg'},\n",
       "     'gleif:mapped_district': {'$': 'Kirchberg'},\n",
       "     'gleif:mapped_state': {'$': 'Luxembourg'},\n",
       "     'gleif:mapped_country': {'$': 'LUX'}},\n",
       "    {'gleif:original_address': {'$': \"2595 AK, 's-Gravenhage, NL-ZH, NL\"},\n",
       "     'gleif:relevance': {'$': '0.8'},\n",
       "     'gleif:lat': {'$': '52.07782'},\n",
       "     'gleif:lng': {'$': '4.33731'},\n",
       "     'gleif:geocoding_date': {'$': '2024-02-11T10:30:29'},\n",
       "     'gleif:bounding_box': {'$': 'TopLeft.Latitude: 52.0819, TopLeft.Longitude: 4.33266, BottomRight.Latitude: 52.07674, BottomRight.Longitude: 4.33885'},\n",
       "     'gleif:match_level': {'$': 'postalCode'},\n",
       "     'gleif:formatted_address': {'$': '2595 AK, Den Haag, Zuid-Holland, Nederland'},\n",
       "     'gleif:mapped_location_id': {'$': 'NT_dsMLVO5VQuBvPs3bVGhCQD'},\n",
       "     'gleif:mapped_postalcode': {'$': '2595 AK'},\n",
       "     'gleif:mapped_city': {'$': 'Den Haag'},\n",
       "     'gleif:mapped_state': {'$': 'Zuid-Holland'},\n",
       "     'gleif:mapped_country': {'$': 'NLD'}}],\n",
       "   'gleif:conformity': {'gleif:conformityflag': {'$': 'CONFORMING'}}}},\n",
       " {'LEI': {'$': '21380011NNJIUAE7PU43'},\n",
       "  'Entity': {'LegalName': {'@xml:lang': 'en', '$': 'SCAN-TERIEUR TRUST'},\n",
       "   'LegalAddress': {'@xml:lang': 'en',\n",
       "    'FirstAddressLine': {'$': 'C/O BARNETT WADDINGHAM LLP'},\n",
       "    'AdditionalAddressLine': [{'$': 'PINNACLE, 67 ALBION STREET'}],\n",
       "    'City': {'$': 'LEEDS'},\n",
       "    'Region': {'$': 'GB-LDS'},\n",
       "    'Country': {'$': 'GB'},\n",
       "    'PostalCode': {'$': 'LS1 5AA'}},\n",
       "   'HeadquartersAddress': {'@xml:lang': 'en',\n",
       "    'FirstAddressLine': {'$': 'C/O SCAN-TERIEUR LTD'},\n",
       "    'AdditionalAddressLine': [{'$': '30 THE METRO CENTRE, TOLPITS LANE'}],\n",
       "    'City': {'$': 'WATFORD'},\n",
       "    'Region': {'$': 'GB-HRT'},\n",
       "    'Country': {'$': 'GB'},\n",
       "    'PostalCode': {'$': 'WD18 9XG'}},\n",
       "   'RegistrationAuthority': {'RegistrationAuthorityID': {'$': 'RA000591'},\n",
       "    'RegistrationAuthorityEntityID': {'$': '10175048'}},\n",
       "   'LegalJurisdiction': {'$': 'GB'},\n",
       "   'EntityCategory': {'$': 'GENERAL'},\n",
       "   'LegalForm': {'EntityLegalFormCode': {'$': '8888'},\n",
       "    'OtherLegalForm': {'$': 'PENSION SCHEME'}},\n",
       "   'EntityStatus': {'$': 'ACTIVE'},\n",
       "   'EntityCreationDate': {'$': '2016-07-22T00:00:00Z'}},\n",
       "  'Registration': {'InitialRegistrationDate': {'$': '2017-12-22T00:00:00Z'},\n",
       "   'LastUpdateDate': {'$': '2024-11-29T07:29:23.043Z'},\n",
       "   'RegistrationStatus': {'$': 'ISSUED'},\n",
       "   'NextRenewalDate': {'$': '2026-01-15T00:00:00Z'},\n",
       "   'ManagingLOU': {'$': '213800WAVVOPS85N2205'},\n",
       "   'ValidationSources': {'$': 'PARTIALLY_CORROBORATED'},\n",
       "   'ValidationAuthority': {'ValidationAuthorityID': {'$': 'RA000591'},\n",
       "    'ValidationAuthorityEntityID': {'$': '10175048'}}},\n",
       "  'Extension': {'gleif:Geocoding': {'gleif:original_address': {'$': '30 THE METRO CENTRE, TOLPITS LANE, WD18 9XG, WATFORD, GB-HRT, GB'},\n",
       "    'gleif:relevance': {'$': '0.62'},\n",
       "    'gleif:match_type': {'$': 'pointAddress'},\n",
       "    'gleif:lat': {'$': '51.63958'},\n",
       "    'gleif:lng': {'$': '-0.43182'},\n",
       "    'gleif:geocoding_date': {'$': '2020-06-16T02:09:27'},\n",
       "    'gleif:bounding_box': {'$': 'TopLeft.Latitude: 51.6407042, TopLeft.Longitude: -0.4336314, BottomRight.Latitude: 51.6384558, BottomRight.Longitude: -0.4300086'},\n",
       "    'gleif:match_level': {'$': 'houseNumber'},\n",
       "    'gleif:formatted_address': {'$': '28 Tolpits Lane, Watford, WD18 9, United Kingdom'},\n",
       "    'gleif:mapped_location_id': {'$': 'NT_uYIdOjE5hh1hnozhx-iSGC_ygD'},\n",
       "    'gleif:mapped_street': {'$': 'Tolpits Lane'},\n",
       "    'gleif:mapped_housenumber': {'$': '28'},\n",
       "    'gleif:mapped_postalcode': {'$': 'WD18 9'},\n",
       "    'gleif:mapped_city': {'$': 'Watford'},\n",
       "    'gleif:mapped_district': {'$': 'Watford'},\n",
       "    'gleif:mapped_state': {'$': 'England'},\n",
       "    'gleif:mapped_country': {'$': 'GBR'}},\n",
       "   'gleif:conformity': {'gleif:conformityflag': {'$': 'CONFORMING'}}}},\n",
       " {'LEI': {'$': '21380012YR78Z3SGMG97'},\n",
       "  'Entity': {'LegalName': {'@xml:lang': 'en',\n",
       "    '$': 'NENE WAREHOUSE SOLUTIONS LTD'},\n",
       "   'OtherEntityNames': {'OtherEntityName': [{'@xml:lang': 'en',\n",
       "      '@type': 'PREVIOUS_LEGAL_NAME',\n",
       "      '$': 'NENE MECHANICAL HANDLING LIMITED'},\n",
       "     {'@xml:lang': 'en',\n",
       "      '@type': 'PREVIOUS_LEGAL_NAME',\n",
       "      '$': 'NENE STORAGE EQUIPMENT LIMITED'}]},\n",
       "   'LegalAddress': {'@xml:lang': 'en',\n",
       "    'FirstAddressLine': {'$': 'NENE HOUSE STATION ROAD'},\n",
       "    'AdditionalAddressLine': [{'$': 'WATFORD VILLAGE'}],\n",
       "    'City': {'$': 'NORTHAMPTON'},\n",
       "    'Region': {'$': 'GB-ENG'},\n",
       "    'Country': {'$': 'GB'},\n",
       "    'PostalCode': {'$': 'NN6 7XN'}},\n",
       "   'HeadquartersAddress': {'@xml:lang': 'en',\n",
       "    'FirstAddressLine': {'$': 'NENE HOUSE STATION ROAD'},\n",
       "    'AdditionalAddressLine': [{'$': 'WATFORD VILLAGE'}],\n",
       "    'City': {'$': 'NORTHAMPTON'},\n",
       "    'Region': {'$': 'GB-ENG'},\n",
       "    'Country': {'$': 'GB'},\n",
       "    'PostalCode': {'$': 'NN6 7XN'}},\n",
       "   'RegistrationAuthority': {'RegistrationAuthorityID': {'$': 'RA000585'},\n",
       "    'RegistrationAuthorityEntityID': {'$': '01162468'}},\n",
       "   'LegalJurisdiction': {'$': 'GB'},\n",
       "   'EntityCategory': {'$': 'GENERAL'},\n",
       "   'LegalForm': {'EntityLegalFormCode': {'$': 'H0PO'}},\n",
       "   'EntityStatus': {'$': 'ACTIVE'},\n",
       "   'EntityCreationDate': {'$': '1974-03-08T00:00:00Z'}},\n",
       "  'Registration': {'InitialRegistrationDate': {'$': '2016-03-17T00:00:00Z'},\n",
       "   'LastUpdateDate': {'$': '2024-11-29T11:27:22.850Z'},\n",
       "   'RegistrationStatus': {'$': 'ISSUED'},\n",
       "   'NextRenewalDate': {'$': '2025-12-08T00:00:00Z'},\n",
       "   'ManagingLOU': {'$': '213800WAVVOPS85N2205'},\n",
       "   'ValidationSources': {'$': 'FULLY_CORROBORATED'},\n",
       "   'ValidationAuthority': {'ValidationAuthorityID': {'$': 'RA000585'},\n",
       "    'ValidationAuthorityEntityID': {'$': '01162468'}}},\n",
       "  'Extension': {'gleif:Geocoding': {'gleif:original_address': {'$': 'NENE HOUSE STATION ROAD, WATFORD VILLAGE, NN6 7XN, NORTHAMPTON, GB-ENG, GB'},\n",
       "    'gleif:geocoding_failed': {'$': 'Ambigous geocoding results'},\n",
       "    'gleif:geocoding_date': {'$': '2018-12-21T03:02:28'}},\n",
       "   'gleif:conformity': {'gleif:conformityflag': {'$': 'CONFORMING'}}}}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display((list_dicts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
